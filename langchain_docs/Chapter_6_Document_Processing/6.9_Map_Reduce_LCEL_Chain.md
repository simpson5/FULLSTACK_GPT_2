# ğŸ“– Section 6.9: Map Reduce LCEL Chain - ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ì˜ í•´ë‹µ

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- âœ… Map-Reduce íŒ¨í„´ì„ LCELë¡œ ìˆ˜ë™ êµ¬í˜„í•˜ì—¬ ë™ì‘ ì›ë¦¬ ì™„ì „ ì´í•´
- âœ… RunnableLambdaë¥¼ í™œìš©í•œ ì»¤ìŠ¤í…€ í•¨ìˆ˜ì˜ ì²´ì¸ í†µí•© ë°©ë²• ìŠµë“
- âœ… ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ìµœì í™” ì „ëµ í•™ìŠµ
- âœ… ë¹„ìš©ê³¼ ì„±ëŠ¥ì„ ê³ ë ¤í•œ Map-Reduce ì „ëµì˜ ì‹¤ì „ í™œìš©ë²• ì´í•´

## ğŸ§  í•µì‹¬ ê°œë…

### Map-Reduce íŒ¨í„´ì´ë€?
**Map-Reduce**ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë¶„ì‚° ì»´í“¨íŒ… íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ, LangChainì—ì„œëŠ” ë§ì€ ë¬¸ì„œë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•œ í›„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.

```mermaid
graph TD
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B[Retriever<br/>ë¬¸ì„œ ê²€ìƒ‰]
    B --> C[ë¬¸ì„œ 1, 2, 3, ..., N]
    
    C --> D[MAP ë‹¨ê³„]
    
    D --> E[LLM 1<br/>ë¬¸ì„œ1 ë¶„ì„]
    D --> F[LLM 2<br/>ë¬¸ì„œ2 ë¶„ì„]  
    D --> G[LLM N<br/>ë¬¸ì„œN ë¶„ì„]
    
    E --> H[ê´€ë ¨ ì •ë³´ 1]
    F --> I[ê´€ë ¨ ì •ë³´ 2]
    G --> J[ê´€ë ¨ ì •ë³´ N]
    
    H --> K[REDUCE ë‹¨ê³„<br/>ì •ë³´ í†µí•©]
    I --> K
    J --> K
    
    K --> L[Final LLM<br/>ìµœì¢… ë‹µë³€ ìƒì„±]
    L --> M[ì‚¬ìš©ìì—ê²Œ ë‹µë³€]
    
    style A fill:#E6F3FF
    style D fill:#FFE6CC
    style K fill:#FFE6CC
    style M fill:#E6FFE6
```

### Stuff vs Map-Reduce ë¹„êµ

| íŠ¹ì„± | Stuff ì „ëµ | Map-Reduce ì „ëµ |
|------|------------|-----------------|
| **ë¬¸ì„œ ìˆ˜ ì œí•œ** | í† í° ì œí•œì— ì˜ì¡´ | ê±°ì˜ ì œí•œ ì—†ìŒ |
| **LLM í˜¸ì¶œ ìˆ˜** | 1íšŒ | N+1íšŒ (N=ë¬¸ì„œ ìˆ˜) |
| **ì²˜ë¦¬ ì†ë„** | ë¹ ë¦„ | ëŠë¦¼ (ìˆœì°¨ ì²˜ë¦¬) |
| **ë¹„ìš©** | ë‚®ìŒ | ë†’ìŒ (í˜¸ì¶œ ìˆ˜ì— ë¹„ë¡€) |
| **ì •ë³´ ë³´ì¡´** | ì œí•œì  (í† í° ì œí•œ) | ìš°ìˆ˜ (ë‹¨ê³„ë³„ ì¶”ì¶œ) |
| **ì í•© ìƒí™©** | ì†Œìˆ˜ ë¬¸ì„œ | ëŒ€ëŸ‰ ë¬¸ì„œ |

### LCEL Map-Reduce ì•„í‚¤í…ì²˜
```python
# ì „ì²´ ì²´ì¸ êµ¬ì¡° ê°œìš”
final_chain = (
    {
        "context": map_chain,           # MAP: ë¬¸ì„œë³„ ì •ë³´ ì¶”ì¶œ í›„ í†µí•©
        "question": RunnablePassthrough() # ì›ë³¸ ì§ˆë¬¸ ì „ë‹¬
    }
    | final_prompt                      # REDUCE: í†µí•©ëœ ì •ë³´ë¡œ ìµœì¢… ë‹µë³€
    | llm
)

# map_chain ë‚´ë¶€ êµ¬ì¡°
map_chain = (
    {
        "documents": retriever,         # ê´€ë ¨ ë¬¸ì„œë“¤ ê²€ìƒ‰
        "question": RunnablePassthrough() # ì§ˆë¬¸ ì „ë‹¬
    }
    | RunnableLambda(map_documents)     # ì»¤ìŠ¤í…€ ë§µí•‘ í•¨ìˆ˜
)
```

## ğŸ“‹ ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤

### RunnableLambda
```python
from langchain.schema.runnable import RunnableLambda

class RunnableLambda:
    def __init__(self, func: Callable):
        """
        ğŸ“‹ ê¸°ëŠ¥: ì„ì˜ì˜ Python í•¨ìˆ˜ë¥¼ Runnableë¡œ ë³€í™˜
        ğŸ“¥ ì…ë ¥: í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜
        ğŸ“¤ ì¶œë ¥: LCEL ì²´ì¸ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Runnable
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ì»¤ìŠ¤í…€ ë¡œì§ì„ ì²´ì¸ì— í†µí•©í•  ë•Œ
        
        ì˜ˆì‹œ:
        def custom_function(input_data):
            return process(input_data)
        
        runnable = RunnableLambda(custom_function)
        """

# Map-Reduce ì „ìš© í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜
def map_documents(inputs: Dict[str, Any]) -> str:
    """
    ğŸ“‹ ê¸°ëŠ¥: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°œë³„ ì²˜ë¦¬í•˜ì—¬ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ í›„ í†µí•©
    ğŸ“¥ ì…ë ¥: {"documents": List[Document], "question": str}
    ğŸ“¤ ì¶œë ¥: í†µí•©ëœ ê´€ë ¨ ì •ë³´ ë¬¸ìì—´
    """
```

### Map-Reduce ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
```python
# MAP ë‹¨ê³„ìš© í”„ë¡¬í”„íŠ¸
map_doc_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¤ìŒ ë¬¸ì„œì˜ ì¼ë¶€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° 
    ê´€ë ¨ì´ ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
    
    ê´€ë ¨ëœ í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš” (ì¶•ì–´ì ìœ¼ë¡œ).
    ê´€ë ¨ëœ ì •ë³´ê°€ ì—†ë‹¤ë©´ 'ê´€ë ¨ ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”.
    
    ë¬¸ì„œ ë‚´ìš©:
    {context}"""),
    ("human", "{question}")
])

# REDUCE ë‹¨ê³„ìš© í”„ë¡¬í”„íŠ¸  
final_prompt = ChatPromptTemplate.from_messages([
    ("system", """ê¸´ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ë‹¤ìŒ ì •ë³´ë“¤ê³¼ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ 
    ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
    
    ë‹µì„ ëª¨ë¥´ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
    
    ì¶”ì¶œëœ ì •ë³´:
    {context}"""),
    ("human", "{question}")
])
```

## ğŸ”§ ë™ì‘ ê³¼ì • ìƒì„¸

### 1ë‹¨ê³„: ê¸°ë³¸ Map-Reduce LCEL Chain êµ¬í˜„
```python
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from typing import Dict, List, Any

# === ê¸°ë³¸ Map-Reduce LCEL Chain êµ¬í˜„ ===
# ğŸ§  ê°œë…: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¶„ì‚° ì²˜ë¦¬ íŒ¨í„´

print("ğŸ—ºï¸ Map-Reduce LCEL Chain êµ¬ì¶•:")
print("=" * 50)

# ğŸ”§ 1ë‹¨ê³„: ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ ì„¤ì •
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.1
)

# ë²¡í„° ì €ì¥ì†Œ ë° retriever ì„¤ì •
embeddings = OpenAIEmbeddings()
vector_store = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
retriever = vector_store.as_retriever(search_kwargs={"k": 5})  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰

print("âœ… ê¸°ë³¸ êµ¬ì„± ìš”ì†Œ ì¤€ë¹„ ì™„ë£Œ")

# ğŸ”§ 2ë‹¨ê³„: MAP ë‹¨ê³„ìš© í”„ë¡¬í”„íŠ¸ì™€ ì²´ì¸
map_doc_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¤ìŒ ë¬¸ì„œ ë¶€ë¶„ì„ ê²€í† í•˜ì—¬ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° 
    ë„ì›€ì´ ë˜ëŠ” ì •ë³´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
    
    ê´€ë ¨ ìˆëŠ” í…ìŠ¤íŠ¸ê°€ ìˆë‹¤ë©´ ì •í™•íˆ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    ê´€ë ¨ ì—†ë‹¤ë©´ "ê´€ë ¨ ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ì‘ë‹µí•˜ì„¸ìš”.
    
    ë¬¸ì„œ ë‚´ìš©:
    {context}"""),
    ("human", "ì§ˆë¬¸: {question}")
])

# ê°œë³„ ë¬¸ì„œ ì²˜ë¦¬ìš© ì²´ì¸
map_doc_chain = map_doc_prompt | llm

print("âœ… MAP ë‹¨ê³„ ì²´ì¸ ì¤€ë¹„ ì™„ë£Œ")

# ğŸ”§ 3ë‹¨ê³„: MAP í•¨ìˆ˜ êµ¬í˜„ (í•µì‹¬ ë¡œì§)
def map_documents(inputs: Dict[str, Any]) -> str:
    """
    ğŸ“‹ ê¸°ëŠ¥: ê° ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ í†µí•©
    ğŸ”„ ê³¼ì •: ë¬¸ì„œ ê°œìˆ˜ë§Œí¼ LLMì„ í˜¸ì¶œí•˜ì—¬ ì •ë³´ ì¶”ì¶œ
    """
    
    documents = inputs["documents"]
    question = inputs["question"]
    
    print(f"   ğŸ” MAP ë‹¨ê³„: {len(documents)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘...")
    
    # ê° ë¬¸ì„œë³„ë¡œ ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
    extracted_info = []
    
    for i, document in enumerate(documents, 1):
        print(f"   ğŸ“„ ë¬¸ì„œ {i}/{len(documents)} ì²˜ë¦¬ ì¤‘...")
        
        try:
            # ê°œë³„ ë¬¸ì„œë¥¼ map_doc_chainìœ¼ë¡œ ì²˜ë¦¬
            response = map_doc_chain.invoke({
                "context": document.page_content,
                "question": question
            })
            
            # AI ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            extracted_text = response.content.strip()
            
            # "ê´€ë ¨ ì •ë³´ ì—†ìŒ" ë“±ì˜ ë¶ˆí•„ìš”í•œ ì‘ë‹µ í•„í„°ë§
            if (extracted_text and 
                "ê´€ë ¨ ì •ë³´ ì—†ìŒ" not in extracted_text and 
                "ê´€ë ¨ì´ ì—†" not in extracted_text.lower() and
                len(extracted_text) > 10):  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ì‘ë‹µë§Œ
                
                extracted_info.append(f"[ë¬¸ì„œ {i}ì—ì„œ ì¶”ì¶œ]\n{extracted_text}")
                print(f"   âœ… ê´€ë ¨ ì •ë³´ ë°œê²¬: {extracted_text[:50]}...")
            else:
                print(f"   âš ï¸ ê´€ë ¨ ì •ë³´ ì—†ìŒ")
                
        except Exception as e:
            print(f"   âŒ ë¬¸ì„œ {i} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue
    
    # ì¶”ì¶œëœ ì •ë³´ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ í†µí•©
    if extracted_info:
        combined_context = "\n\n".join(extracted_info)
        print(f"   ğŸ“Š MAP ì™„ë£Œ: {len(extracted_info)}ê°œ ë¬¸ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ")
        return combined_context
    else:
        print(f"   âš ï¸ ì–´ë–¤ ë¬¸ì„œì—ì„œë„ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
        return "ì œê³µëœ ë¬¸ì„œë“¤ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

print("âœ… MAP í•¨ìˆ˜ êµ¬í˜„ ì™„ë£Œ")

# ğŸ”§ 4ë‹¨ê³„: REDUCE ë‹¨ê³„ìš© í”„ë¡¬í”„íŠ¸
final_prompt = ChatPromptTemplate.from_messages([
    ("system", """ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì¶”ì¶œëœ ë‹¤ìŒ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ 
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ì „í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    
    ì§€ì¹¨:
    1. ì¶”ì¶œëœ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
    2. ì—¬ëŸ¬ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì¢…í•©í•˜ì„¸ìš”
    3. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
    4. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
    
    ì¶”ì¶œëœ ì •ë³´:
    {context}"""),
    ("human", "{question}")
])

print("âœ… REDUCE ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ")

# ğŸ”§ 5ë‹¨ê³„: ì „ì²´ Map-Reduce ì²´ì¸ êµ¬ì„±
map_reduce_chain = (
    {
        "context": (
            {
                "documents": retriever,
                "question": RunnablePassthrough()
            }
            | RunnableLambda(map_documents)
        ),
        "question": RunnablePassthrough()
    }
    | final_prompt
    | llm
)

print("âœ… Map-Reduce LCEL Chain êµ¬ì„± ì™„ë£Œ")

# ğŸ”§ 6ë‹¨ê³„: ì²´ì¸ í…ŒìŠ¤íŠ¸
test_questions = [
    "Winston SmithëŠ” ì–´ë””ì—ì„œ ì¼í•˜ë‚˜ìš”?",
    "Victory Mansionsì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "Ministry of Truthì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?"
]

for i, question in enumerate(test_questions, 1):
    print(f"\n{'='*20} Map-Reduce í…ŒìŠ¤íŠ¸ {i} {'='*20}")
    print(f"ğŸ“‹ ì§ˆë¬¸: {question}")
    
    try:
        import time
        start_time = time.time()
        
        # Map-Reduce ì²´ì¸ ì‹¤í–‰
        response = map_reduce_chain.invoke(question)
        
        execution_time = time.time() - start_time
        
        print(f"ğŸ¤– ë‹µë³€: {response.content}")
        print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

print("\nâœ… ê¸°ë³¸ Map-Reduce LCEL Chain í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
```

### 2ë‹¨ê³„: ìµœì í™”ëœ Map-Reduce Chain (ë³‘ë ¬ ì²˜ë¦¬)
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from typing import List, Dict, Any

# === ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”ëœ Map-Reduce Chain ===
# ğŸ§  ê°œë…: ë¬¸ì„œë³„ ì²˜ë¦¬ë¥¼ ë³‘ë ¬í™”í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ

class OptimizedMapReduceChain:
    """
    ğŸ¯ ìµœì í™”ëœ Map-Reduce ì²´ì¸
    
    íŠ¹ì§•:
    - ë³‘ë ¬ ë¬¸ì„œ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
    - ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œë„ ë¡œì§
    - ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
    - ë¹„ìš© ë° ì‹œê°„ ì¶”ì 
    """
    
    def __init__(self, 
                 llm,
                 retriever,
                 max_workers: int = 3,
                 timeout: float = 30.0):
        
        self.llm = llm
        self.retriever = retriever
        self.max_workers = max_workers
        self.timeout = timeout
        
        # í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™”
        self._initialize_prompts()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = {
            "total_queries": 0,
            "total_documents_processed": 0,
            "avg_map_time": 0.0,
            "avg_reduce_time": 0.0
        }
    
    def _initialize_prompts(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”"""
        
        self.map_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì´ ë¬¸ì„œ ë¶€ë¶„ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”:

            ê·œì¹™:
            1. ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”
            2. ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ë˜, í•µì‹¬ ë¶€ë¶„ë§Œ ì„ ë³„í•˜ì„¸ìš”  
            3. ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”
            4. ì¶”ì¸¡ì´ë‚˜ í•´ì„ì€ í•˜ì§€ ë§ˆì„¸ìš”
            
            ë¬¸ì„œ:
            {context}"""),
            ("human", "ì§ˆë¬¸: {question}")
        ])
        
        self.reduce_prompt = ChatPromptTemplate.from_messages([
            ("system", """ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:
            
            ì§€ì¹¨:
            1. ëª¨ë“  ê´€ë ¨ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”
            2. ì¼ê´€ì„± ìˆê²Œ ì •ë³´ë¥¼ ì •ë¦¬í•˜ì„¸ìš”
            3. ì¤‘ë³µëœ ì •ë³´ëŠ” í•œ ë²ˆë§Œ ì–¸ê¸‰í•˜ì„¸ìš”
            4. ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…ì‹œí•˜ì„¸ìš”
            
            ìˆ˜ì§‘ëœ ì •ë³´:
            {context}"""),
            ("human", "{question}")
        ])
    
    def _process_single_document(self, doc_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        
        document = doc_data["document"]
        question = doc_data["question"]
        doc_index = doc_data["index"]
        
        try:
            start_time = time.time()
            
            # MAP ì²´ì¸ìœ¼ë¡œ ë¬¸ì„œ ì²˜ë¦¬
            chain = self.map_prompt | self.llm
            response = chain.invoke({
                "context": document.page_content,
                "question": question
            })
            
            processing_time = time.time() - start_time
            
            # ê²°ê³¼ í‰ê°€
            extracted_text = response.content.strip()
            
            is_relevant = (
                len(extracted_text) > 10 and
                "ì •ë³´ ì—†ìŒ" not in extracted_text and
                "ê´€ë ¨ ì—†" not in extracted_text.lower() and
                "ì—†ìŠµë‹ˆë‹¤" not in extracted_text
            )
            
            return {
                "index": doc_index,
                "content": extracted_text if is_relevant else "",
                "is_relevant": is_relevant,
                "processing_time": processing_time,
                "success": True
            }
            
        except Exception as e:
            return {
                "index": doc_index,
                "content": "",
                "is_relevant": False,
                "processing_time": 0,
                "error": str(e),
                "success": False
            }
    
    def _parallel_map_phase(self, documents: List, question: str) -> List[str]:
        """ë³‘ë ¬ MAP ë‹¨ê³„ ì‹¤í–‰"""
        
        print(f"   ğŸ”„ ë³‘ë ¬ MAP ì‹œì‘: {len(documents)}ê°œ ë¬¸ì„œ, {self.max_workers}ê°œ ì›Œì»¤")
        
        # ë¬¸ì„œë³„ ì‘ì—… ë°ì´í„° ì¤€ë¹„
        doc_tasks = [
            {
                "document": doc,
                "question": question,
                "index": i
            }
            for i, doc in enumerate(documents)
        ]
        
        relevant_extracts = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ëª¨ë“  ì‘ì—… ì œì¶œ
            future_to_doc = {
                executor.submit(self._process_single_document, doc_data): doc_data["index"]
                for doc_data in doc_tasks
            }
            
            # ì™„ë£Œëœ ì‘ì—…ë“¤ ìˆ˜ì§‘
            completed = 0
            for future in as_completed(future_to_doc, timeout=self.timeout):
                doc_index = future_to_doc[future]
                completed += 1
                
                try:
                    result = future.result()
                    
                    if result["success"] and result["is_relevant"]:
                        relevant_extracts.append(
                            f"[ë¬¸ì„œ {doc_index + 1}]\n{result['content']}"
                        )
                        print(f"   âœ… ë¬¸ì„œ {doc_index + 1}: ê´€ë ¨ ì •ë³´ ì¶”ì¶œ ({result['processing_time']:.2f}ì´ˆ)")
                    else:
                        print(f"   âš ï¸ ë¬¸ì„œ {doc_index + 1}: ê´€ë ¨ ì •ë³´ ì—†ìŒ")
                        
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    if completed % max(1, len(documents) // 4) == 0:
                        progress = (completed / len(documents)) * 100
                        print(f"   ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({completed}/{len(documents)})")
                        
                except Exception as e:
                    print(f"   âŒ ë¬¸ì„œ {doc_index + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        print(f"   ğŸ“Š MAP ì™„ë£Œ: {len(relevant_extracts)}ê°œ ë¬¸ì„œì—ì„œ ì •ë³´ ì¶”ì¶œ")
        return relevant_extracts
    
    def process_query(self, question: str) -> Dict[str, Any]:
        """ì „ì²´ Map-Reduce ì²˜ë¦¬"""
        
        total_start = time.time()
        
        print(f"ğŸš€ Map-Reduce ì²˜ë¦¬ ì‹œì‘: '{question[:50]}...'")
        
        try:
            # 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰
            print("ğŸ“– ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            documents = self.retriever.get_relevant_documents(question)
            print(f"   ğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
            
            if not documents:
                return {
                    "answer": "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "success": False,
                    "total_time": time.time() - total_start
                }
            
            # 2ë‹¨ê³„: ë³‘ë ¬ MAP ë‹¨ê³„
            map_start = time.time()
            relevant_extracts = self._parallel_map_phase(documents, question)
            map_time = time.time() - map_start
            
            if not relevant_extracts:
                return {
                    "answer": "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "success": False,
                    "map_time": map_time,
                    "total_time": time.time() - total_start
                }
            
            # 3ë‹¨ê³„: REDUCE ë‹¨ê³„
            print("ğŸ”— REDUCE ë‹¨ê³„: ì •ë³´ í†µí•© ë° ìµœì¢… ë‹µë³€ ìƒì„±")
            reduce_start = time.time()
            
            # ì¶”ì¶œëœ ì •ë³´ë“¤ì„ í•˜ë‚˜ë¡œ í†µí•©
            combined_context = "\n\n".join(relevant_extracts)
            
            # ìµœì¢… ë‹µë³€ ìƒì„±
            final_chain = self.reduce_prompt | self.llm
            final_response = final_chain.invoke({
                "context": combined_context,
                "question": question
            })
            
            reduce_time = time.time() - reduce_start
            total_time = time.time() - total_start
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.metrics["total_queries"] += 1
            self.metrics["total_documents_processed"] += len(documents)
            self.metrics["avg_map_time"] = (
                (self.metrics["avg_map_time"] * (self.metrics["total_queries"] - 1) + map_time) 
                / self.metrics["total_queries"]
            )
            self.metrics["avg_reduce_time"] = (
                (self.metrics["avg_reduce_time"] * (self.metrics["total_queries"] - 1) + reduce_time)
                / self.metrics["total_queries"]
            )
            
            return {
                "answer": final_response.content,
                "success": True,
                "documents_found": len(documents),
                "relevant_documents": len(relevant_extracts),
                "map_time": map_time,
                "reduce_time": reduce_time,
                "total_time": total_time
            }
            
        except Exception as e:
            return {
                "answer": f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "success": False,
                "total_time": time.time() - total_start,
                "error": str(e)
            }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        
        return {
            "ì´_ì§ˆì˜_ìˆ˜": self.metrics["total_queries"],
            "ì´_ì²˜ë¦¬_ë¬¸ì„œ_ìˆ˜": self.metrics["total_documents_processed"],
            "í‰ê· _MAP_ì‹œê°„": f"{self.metrics['avg_map_time']:.2f}ì´ˆ",
            "í‰ê· _REDUCE_ì‹œê°„": f"{self.metrics['avg_reduce_time']:.2f}ì´ˆ",
            "ë¬¸ì„œë‹¹_í‰ê· _ì²˜ë¦¬ì‹œê°„": f"{self.metrics['avg_map_time'] / max(1, self.metrics['total_documents_processed'] / max(1, self.metrics['total_queries'])):.3f}ì´ˆ"
        }

# === ìµœì í™”ëœ Map-Reduce ì²´ì¸ ì‚¬ìš© ì˜ˆì‹œ ===
print("\nğŸš€ ìµœì í™”ëœ Map-Reduce ì²´ì¸ í…ŒìŠ¤íŠ¸:")
print("=" * 60)

# ìµœì í™”ëœ ì²´ì¸ ì´ˆê¸°í™”
optimized_chain = OptimizedMapReduceChain(
    llm=llm,
    retriever=retriever,
    max_workers=3,  # ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜
    timeout=30.0
)

# ë³µì¡í•œ ì§ˆë¬¸ë“¤ë¡œ í…ŒìŠ¤íŠ¸
complex_questions = [
    "Winston Smithì˜ ì¼ìƒìƒí™œê³¼ ì§ì—…ì— ëŒ€í•´ ì¢…í•©ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "1984ë…„ ì†Œì„¤ì— ë“±ì¥í•˜ëŠ” ì •ë¶€ ê¸°ê´€ë“¤ì˜ ì—­í• ê³¼ ê¸°ëŠ¥ì„ ë¹„êµí•´ì£¼ì„¸ìš”",
    "Victory Mansionsì˜ í™˜ê²½ê³¼ ê±°ì£¼ë¯¼ë“¤ì˜ ìƒí™œìƒì„ ìì„¸íˆ ë¬˜ì‚¬í•´ì£¼ì„¸ìš”"
]

for i, question in enumerate(complex_questions, 1):
    print(f"\n{'='*15} ìµœì í™” í…ŒìŠ¤íŠ¸ {i} {'='*15}")
    print(f"ğŸ“‹ ì§ˆë¬¸: {question}")
    
    result = optimized_chain.process_query(question)
    
    if result["success"]:
        print(f"ğŸ¤– ë‹µë³€: {result['answer'][:200]}...")
        print(f"ğŸ“Š í†µê³„:")
        print(f"   ğŸ“– ê²€ìƒ‰ ë¬¸ì„œ: {result['documents_found']}ê°œ")
        print(f"   âœ… ê´€ë ¨ ë¬¸ì„œ: {result['relevant_documents']}ê°œ") 
        print(f"   â±ï¸ MAP ì‹œê°„: {result['map_time']:.2f}ì´ˆ")
        print(f"   â±ï¸ REDUCE ì‹œê°„: {result['reduce_time']:.2f}ì´ˆ")
        print(f"   â±ï¸ ì´ ì‹œê°„: {result['total_time']:.2f}ì´ˆ")
    else:
        print(f"âŒ ì‹¤íŒ¨: {result['answer']}")

# ì„±ëŠ¥ ìš”ì•½
print(f"\nğŸ“ˆ ì„±ëŠ¥ ìš”ì•½:")
summary = optimized_chain.get_performance_summary()
for key, value in summary.items():
    print(f"   {key}: {value}")

print("\nâœ… ìµœì í™”ëœ Map-Reduce ì²´ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
```

### 3ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ Map-Reduce (ë¹„ìš© ìµœì í™”)
```python
from langchain.callbacks import get_openai_callback
import json

# === ë¹„ìš© ìµœì í™” Map-Reduce Chain ===
# ğŸ§  ê°œë…: í† í° ì‚¬ìš©ëŸ‰ê³¼ ë¹„ìš©ì„ ìµœì†Œí™”í•˜ëŠ” ì§€ëŠ¥í˜• ì „ëµ

class CostOptimizedMapReduceChain:
    """
    ğŸ¯ ë¹„ìš© ìµœì í™”ëœ Map-Reduce ì²´ì¸
    
    ìµœì í™” ì „ëµ:
    - ë¬¸ì„œ ì‚¬ì „ í•„í„°ë§ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ LLM í˜¸ì¶œ ì œê±°
    - ë™ì  ì²­í¬ í¬ê¸° ì¡°ì ˆ
    - ë‹¨ê³„ì  ì²˜ë¦¬ë¡œ ë¹„ìš© ì˜ˆì¸¡
    - ì˜ˆì‚° ê¸°ë°˜ ì²˜ë¦¬ ì œí•œ
    """
    
    def __init__(self, 
                 llm,
                 retriever,
                 max_budget: float = 1.0,  # ë‹¬ëŸ¬ ë‹¨ìœ„
                 cost_per_1k_tokens: float = 0.002):  # GPT-3.5-turbo ê¸°ì¤€
        
        self.llm = llm
        self.retriever = retriever
        self.max_budget = max_budget
        self.cost_per_1k_tokens = cost_per_1k_tokens
        
        self.spent_budget = 0.0
        
        # í”„ë¡¬í”„íŠ¸ ìµœì í™” (í† í° íš¨ìœ¨ì )
        self._initialize_optimized_prompts()
    
    def _initialize_optimized_prompts(self):
        """í† í° íš¨ìœ¨ì ì¸ í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™”"""
        
        # ê°„ê²°í•œ MAP í”„ë¡¬í”„íŠ¸
        self.map_prompt = ChatPromptTemplate.from_messages([
            ("system", "ë¬¸ì„œì—ì„œ ì§ˆë¬¸ ê´€ë ¨ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ. ì—†ìœ¼ë©´ 'NONE'."),
            ("human", "ë¬¸ì„œ: {context}\nì§ˆë¬¸: {question}\ní•µì‹¬ ì •ë³´:")
        ])
        
        # ê°„ê²°í•œ REDUCE í”„ë¡¬í”„íŠ¸
        self.reduce_prompt = ChatPromptTemplate.from_messages([
            ("system", "ì¶”ì¶œ ì •ë³´ë¡œ ì§ˆë¬¸ì— ë‹µë³€. ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ."),
            ("human", "ì •ë³´: {context}\nì§ˆë¬¸: {question}\në‹µë³€:")
        ])
    
    def _estimate_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •"""
        # ê°„ë‹¨í•œ ì¶”ì •: ë¬¸ì ìˆ˜ / 4 (ì˜ì–´ ê¸°ì¤€)
        return len(text) // 4
    
    def _estimate_cost(self, input_tokens: int, output_tokens: int = 150) -> float:
        """ë¹„ìš© ì¶”ì •"""
        total_tokens = input_tokens + output_tokens
        return (total_tokens / 1000) * self.cost_per_1k_tokens
    
    def _prefilter_documents(self, documents: List, question: str) -> List:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ì‚¬ì „ í•„í„°ë§"""
        
        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ë²•)
        question_words = set(question.lower().split())
        stop_words = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'the', 'a', 'an', 'and', 'or'}
        keywords = question_words - stop_words
        
        if not keywords:
            return documents[:3]  # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì²˜ìŒ 3ê°œë§Œ
        
        # ë¬¸ì„œë³„ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        scored_docs = []
        for doc in documents:
            content_words = set(doc.page_content.lower().split())
            relevance_score = len(keywords & content_words) / len(keywords)
            
            if relevance_score > 0:  # ìµœì†Œ 1ê°œ í‚¤ì›Œë“œëŠ” í¬í•¨
                scored_docs.append((doc, relevance_score))
        
        # ê´€ë ¨ì„± ì ìˆ˜ìˆœ ì •ë ¬
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ ë¬¸ì„œë“¤ë§Œ ì„ íƒ (ìµœëŒ€ 5ê°œ)
        filtered_docs = [doc for doc, score in scored_docs[:5]]
        
        print(f"   ğŸ” ì‚¬ì „ í•„í„°ë§: {len(documents)}ê°œ â†’ {len(filtered_docs)}ê°œ")
        return filtered_docs
    
    def _smart_map_phase(self, documents: List, question: str) -> tuple:
        """ì˜ˆì‚°ì„ ê³ ë ¤í•œ ìŠ¤ë§ˆíŠ¸ MAP ë‹¨ê³„"""
        
        print(f"   ğŸ’° í˜„ì¬ ì˜ˆì‚°: ${self.max_budget - self.spent_budget:.4f} ë‚¨ìŒ")
        
        extracted_info = []
        total_cost = 0.0
        processed_count = 0
        
        with get_openai_callback() as cb:
            for i, doc in enumerate(documents):
                # ë¹„ìš© ì‚¬ì „ ì²´í¬
                input_text = f"ë¬¸ì„œ: {doc.page_content}\nì§ˆë¬¸: {question}\ní•µì‹¬ ì •ë³´:"
                estimated_tokens = self._estimate_tokens(input_text)
                estimated_cost = self._estimate_cost(estimated_tokens)
                
                # ì˜ˆì‚° ì´ˆê³¼ ì²´í¬
                if self.spent_budget + total_cost + estimated_cost > self.max_budget:
                    print(f"   âš ï¸ ì˜ˆì‚° ì œí•œìœ¼ë¡œ ë¬¸ì„œ {i+1} ì´í›„ ì²˜ë¦¬ ì¤‘ë‹¨")
                    break
                
                try:
                    # MAP ì²´ì¸ ì‹¤í–‰
                    chain = self.map_prompt | self.llm
                    response = chain.invoke({
                        "context": doc.page_content[:1000],  # ê¸¸ì´ ì œí•œ
                        "question": question
                    })
                    
                    extracted_text = response.content.strip()
                    
                    # ìœ ìš©í•œ ì •ë³´ì¸ì§€ í™•ì¸
                    if (len(extracted_text) > 5 and 
                        "NONE" not in extracted_text.upper() and
                        "ì—†" not in extracted_text):
                        
                        extracted_info.append(f"[{i+1}] {extracted_text}")
                        print(f"   âœ… ë¬¸ì„œ {i+1}: ì •ë³´ ì¶”ì¶œ")
                    else:
                        print(f"   âš ï¸ ë¬¸ì„œ {i+1}: ê´€ë ¨ ì •ë³´ ì—†ìŒ")
                    
                    processed_count += 1
                    
                except Exception as e:
                    print(f"   âŒ ë¬¸ì„œ {i+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
        
            # ì‹¤ì œ ì‚¬ìš© ë¹„ìš© ê¸°ë¡
            actual_cost = cb.total_cost
            total_cost = actual_cost
        
        return extracted_info, total_cost, processed_count
    
    def process_with_budget_control(self, question: str) -> Dict[str, Any]:
        """ì˜ˆì‚° í†µì œ í•˜ì—ì„œ Map-Reduce ì²˜ë¦¬"""
        
        print(f"ğŸ’° ë¹„ìš© ìµœì í™” Map-Reduce ì‹œì‘")
        print(f"   ğŸ“Š ìµœëŒ€ ì˜ˆì‚°: ${self.max_budget:.4f}")
        
        total_start = time.time()
        
        try:
            # 1ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰
            documents = self.retriever.get_relevant_documents(question)
            print(f"   ğŸ“– ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(documents)}ê°œ")
            
            # 2ë‹¨ê³„: ì‚¬ì „ í•„í„°ë§
            filtered_docs = self._prefilter_documents(documents, question)
            
            # 3ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ MAP ë‹¨ê³„
            map_start = time.time()
            extracted_info, map_cost, processed_docs = self._smart_map_phase(
                filtered_docs, question
            )
            map_time = time.time() - map_start
            
            self.spent_budget += map_cost
            
            if not extracted_info:
                return {
                    "answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "success": False,
                    "cost": map_cost,
                    "processed_documents": processed_docs
                }
            
            # 4ë‹¨ê³„: REDUCE ë‹¨ê³„
            reduce_start = time.time()
            
            # ì •ë³´ í†µí•©
            combined_context = "\n".join(extracted_info)
            
            # ì˜ˆì‚° ì²´í¬
            reduce_input = f"ì •ë³´: {combined_context}\nì§ˆë¬¸: {question}\në‹µë³€:"
            estimated_reduce_cost = self._estimate_cost(
                self._estimate_tokens(reduce_input)
            )
            
            if self.spent_budget + estimated_reduce_cost > self.max_budget:
                return {
                    "answer": "ì˜ˆì‚° ë¶€ì¡±ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± ë¶ˆê°€",
                    "success": False,
                    "cost": map_cost,
                    "processed_documents": processed_docs
                }
            
            # ìµœì¢… ë‹µë³€ ìƒì„±
            with get_openai_callback() as cb:
                final_chain = self.reduce_prompt | self.llm
                final_response = final_chain.invoke({
                    "context": combined_context,
                    "question": question
                })
                reduce_cost = cb.total_cost
            
            self.spent_budget += reduce_cost
            reduce_time = time.time() - reduce_start
            total_time = time.time() - total_start
            
            return {
                "answer": final_response.content,
                "success": True,
                "cost": {
                    "map_cost": map_cost,
                    "reduce_cost": reduce_cost,
                    "total_cost": map_cost + reduce_cost
                },
                "processed_documents": processed_docs,
                "remaining_budget": self.max_budget - self.spent_budget,
                "time": {
                    "map_time": map_time,
                    "reduce_time": reduce_time,
                    "total_time": total_time
                }
            }
            
        except Exception as e:
            return {
                "answer": f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}",
                "success": False,
                "error": str(e)
            }

# === ë¹„ìš© ìµœì í™” ì²´ì¸ í…ŒìŠ¤íŠ¸ ===
print("\nğŸ’° ë¹„ìš© ìµœì í™” Map-Reduce ì²´ì¸ í…ŒìŠ¤íŠ¸:")
print("=" * 60)

# ë¹„ìš© ìµœì í™” ì²´ì¸ ì´ˆê¸°í™”
cost_chain = CostOptimizedMapReduceChain(
    llm=llm,
    retriever=retriever,
    max_budget=0.50,  # $0.50 ì˜ˆì‚°
    cost_per_1k_tokens=0.002
)

# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
budget_test_questions = [
    "Winston SmithëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    "Victory Mansionsì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "Ministry of Truthì—ì„œ í•˜ëŠ” ì¼ì€ ë¬´ì—‡ì¸ê°€ìš”?"
]

for i, question in enumerate(budget_test_questions, 1):
    print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ {i}: {question}")
    
    result = cost_chain.process_with_budget_control(question)
    
    if result["success"]:
        print(f"ğŸ¤– ë‹µë³€: {result['answer'][:150]}...")
        print(f"ğŸ’° ë¹„ìš© ì •ë³´:")
        print(f"   MAP ë¹„ìš©: ${result['cost']['map_cost']:.6f}")
        print(f"   REDUCE ë¹„ìš©: ${result['cost']['reduce_cost']:.6f}")
        print(f"   ì´ ë¹„ìš©: ${result['cost']['total_cost']:.6f}")
        print(f"   ë‚¨ì€ ì˜ˆì‚°: ${result['remaining_budget']:.6f}")
        print(f"ğŸ“Š ì²˜ë¦¬ ë¬¸ì„œ: {result['processed_documents']}ê°œ")
    else:
        print(f"âŒ ì‹¤íŒ¨: {result['answer']}")
        if 'cost' in result:
            print(f"ğŸ’° ì‚¬ìš© ë¹„ìš©: ${result['cost']:.6f}")

print(f"\nğŸ’° ìµœì¢… ì˜ˆì‚° ì‚¬ìš©: ${cost_chain.spent_budget:.6f} / ${cost_chain.max_budget:.6f}")

print("\nâœ… ë¹„ìš© ìµœì í™” Map-Reduce ì²´ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
```

## ğŸ’» ì‹¤ì „ ì˜ˆì œ

### Meeting GPTìš© Map-Reduce ì‹œìŠ¤í…œ
```python
from typing import List, Dict, Any, Optional
import re
from dataclasses import dataclass
from datetime import datetime

# === Meeting GPTë¥¼ ìœ„í•œ ì „ë¬¸ Map-Reduce ì‹œìŠ¤í…œ ===
# ğŸ§  ê°œë…: íšŒì˜ë¡ ìš”ì•½ì— íŠ¹í™”ëœ Map-Reduce êµ¬í˜„

@dataclass
class MeetingSegment:
    """íšŒì˜ ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„° êµ¬ì¡°"""
    timestamp: str
    speaker: str
    content: str
    duration: Optional[float] = None

class MeetingMapReduceProcessor:
    """
    ğŸ¯ íšŒì˜ë¡ ì²˜ë¦¬ ì „ë¬¸ Map-Reduce ì‹œìŠ¤í…œ
    
    íŠ¹í™” ê¸°ëŠ¥:
    - ë°œì–¸ìë³„ ë‚´ìš© ì •ë¦¬
    - ì‹œê°„ëŒ€ë³„ ë…¼ì˜ ì£¼ì œ ë¶„ì„
    - ì•¡ì…˜ ì•„ì´í…œ ìë™ ì¶”ì¶œ
    - ê²°ì •ì‚¬í•­ ì •ë¦¬
    """
    
    def __init__(self, llm):
        self.llm = llm
        self._initialize_meeting_prompts()
    
    def _initialize_meeting_prompts(self):
        """íšŒì˜ë¡ ì „ìš© í”„ë¡¬í”„íŠ¸ ì´ˆê¸°í™”"""
        
        # íšŒì˜ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ìš© MAP í”„ë¡¬í”„íŠ¸
        self.segment_analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """íšŒì˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì¶”ì¶œí•˜ì„¸ìš”:

            1. ì£¼ìš” ë…¼ì˜ ë‚´ìš© (í•µì‹¬ í¬ì¸íŠ¸ 3ê°œ ì´ë‚´)
            2. ê²°ì •ëœ ì‚¬í•­ (ìˆëŠ” ê²½ìš°)
            3. ì•¡ì…˜ ì•„ì´í…œ (ë‹´ë‹¹ìì™€ í•¨ê»˜)
            4. í›„ì† ë…¼ì˜ í•„ìš” ì‚¬í•­

            í˜•ì‹:
            **ë…¼ì˜ë‚´ìš©:** [ë‚´ìš©]
            **ê²°ì •ì‚¬í•­:** [ê²°ì •ì‚¬í•­ ë˜ëŠ” 'ì—†ìŒ']
            **ì•¡ì…˜ì•„ì´í…œ:** [ì•„ì´í…œ ë˜ëŠ” 'ì—†ìŒ']  
            **í›„ì†ë…¼ì˜:** [ì‚¬í•­ ë˜ëŠ” 'ì—†ìŒ']

            íšŒì˜ ì„¸ê·¸ë¨¼íŠ¸:
            {segment}"""),
            ("human", "ìœ„ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        ])
        
        # ì „ì²´ íšŒì˜ ìš”ì•½ìš© REDUCE í”„ë¡¬í”„íŠ¸
        self.meeting_summary_prompt = ChatPromptTemplate.from_messages([
            ("system", """íšŒì˜ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ 
            ì™„ì „í•œ íšŒì˜ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”:

            ìš”ì•½ êµ¬ì¡°:
            # íšŒì˜ ìš”ì•½
            
            ## ğŸ“‹ ì£¼ìš” ë…¼ì˜ ì‚¬í•­
            [í•µì‹¬ ë…¼ì˜ ë‚´ìš©ë“¤ì„ ì£¼ì œë³„ë¡œ ì •ë¦¬]
            
            ## âœ… ê²°ì • ì‚¬í•­  
            [íšŒì˜ì—ì„œ ë‚´ë ¤ì§„ ê²°ì •ë“¤ì„ ë²ˆí˜¸ë³„ë¡œ ì •ë¦¬]
            
            ## ğŸ“ ì•¡ì…˜ ì•„ì´í…œ
            [ë‹´ë‹¹ìì™€ ì¼ì •ì„ í¬í•¨í•œ ì•¡ì…˜ ì•„ì´í…œë“¤]
            
            ## ğŸ”„ í›„ì† ì¡°ì¹˜
            [ë‹¤ìŒ íšŒì˜ë‚˜ ì¶”ê°€ ë…¼ì˜ê°€ í•„ìš”í•œ ì‚¬í•­ë“¤]
            
            ë¶„ì„ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤:
            {segments_analysis}"""),
            ("human", "ìœ„ ì •ë³´ë“¤ì„ ì¢…í•©í•˜ì—¬ íšŒì˜ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.")
        ])
    
    def _parse_meeting_transcript(self, transcript: str) -> List[MeetingSegment]:
        """íšŒì˜ë¡ì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ íŒŒì‹±"""
        
        segments = []
        lines = transcript.strip().split('\n')
        
        current_speaker = None
        current_content = []
        current_timestamp = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ íŒ¨í„´ ê°ì§€ (ì˜ˆ: [00:15:30])
            timestamp_match = re.match(r'\[(\d{2}:\d{2}:\d{2})\]', line)
            if timestamp_match:
                current_timestamp = timestamp_match.group(1)
                continue
            
            # ë°œì–¸ì íŒ¨í„´ ê°ì§€ (ì˜ˆ: "ê¹€ëŒ€ë¦¬:" ë˜ëŠ” "Speaker A:")
            speaker_match = re.match(r'^([^:]+):\s*(.*)$', line)
            if speaker_match:
                # ì´ì „ ë°œì–¸ì ë‚´ìš© ì €ì¥
                if current_speaker and current_content:
                    segments.append(MeetingSegment(
                        timestamp=current_timestamp or "00:00:00",
                        speaker=current_speaker,
                        content=' '.join(current_content)
                    ))
                
                # ìƒˆë¡œìš´ ë°œì–¸ì
                current_speaker = speaker_match.group(1).strip()
                current_content = [speaker_match.group(2).strip()] if speaker_match.group(2).strip() else []
            else:
                # í˜„ì¬ ë°œì–¸ìì˜ ê³„ì†ëœ ë‚´ìš©
                if current_speaker:
                    current_content.append(line)
        
        # ë§ˆì§€ë§‰ ë°œì–¸ì ì²˜ë¦¬
        if current_speaker and current_content:
            segments.append(MeetingSegment(
                timestamp=current_timestamp or "00:00:00", 
                speaker=current_speaker,
                content=' '.join(current_content)
            ))
        
        return segments
    
    def _group_segments_by_topic(self, segments: List[MeetingSegment]) -> List[List[MeetingSegment]]:
        """ê´€ë ¨ëœ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ê·¸ë£¹í™”"""
        
        # ê°„ë‹¨í•œ ê·¸ë£¹í™”: ë°œì–¸ì ë³€ê²½ì´ë‚˜ ì‹œê°„ ê°„ê²©ì„ ê¸°ì¤€
        groups = []
        current_group = []
        
        for segment in segments:
            if not current_group:
                current_group = [segment]
            else:
                # ê°™ì€ í™”ìê°€ ì—°ì†ìœ¼ë¡œ ë§í•˜ê±°ë‚˜, ì§§ì€ ê°„ê²©ì¸ ê²½ìš° ê°™ì€ ê·¸ë£¹
                last_segment = current_group[-1]
                
                # ì‹œê°„ ì°¨ì´ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)
                if (segment.speaker == last_segment.speaker or 
                    len(current_group) < 3):  # ìµœì†Œ ê·¸ë£¹ í¬ê¸°
                    current_group.append(segment)
                else:
                    # ìƒˆë¡œìš´ ê·¸ë£¹ ì‹œì‘
                    groups.append(current_group)
                    current_group = [segment]
        
        if current_group:
            groups.append(current_group)
        
        return groups
    
    def _format_segment_group(self, segments: List[MeetingSegment]) -> str:
        """ì„¸ê·¸ë¨¼íŠ¸ ê·¸ë£¹ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        
        formatted_parts = []
        for segment in segments:
            formatted_parts.append(
                f"[{segment.timestamp}] {segment.speaker}: {segment.content}"
            )
        
        return '\n'.join(formatted_parts)
    
    def process_meeting_transcript(self, transcript: str) -> Dict[str, Any]:
        """íšŒì˜ë¡ ì „ì²´ ì²˜ë¦¬"""
        
        print("ğŸ“‹ íšŒì˜ë¡ Map-Reduce ì²˜ë¦¬ ì‹œì‘")
        
        try:
            # 1ë‹¨ê³„: íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ íŒŒì‹±
            print("   ğŸ” íŠ¸ëœìŠ¤í¬ë¦½íŠ¸ íŒŒì‹± ì¤‘...")
            segments = self._parse_meeting_transcript(transcript)
            print(f"   ğŸ“Š íŒŒì‹±ëœ ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ")
            
            if not segments:
                return {
                    "summary": "íŒŒì‹±í•  ìˆ˜ ìˆëŠ” íšŒì˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.",
                    "success": False
                }
            
            # 2ë‹¨ê³„: ì„¸ê·¸ë¨¼íŠ¸ ê·¸ë£¹í™”
            print("   ğŸ“‚ ì„¸ê·¸ë¨¼íŠ¸ ê·¸ë£¹í™” ì¤‘...")
            segment_groups = self._group_segments_by_topic(segments)
            print(f"   ğŸ“Š ê·¸ë£¹ ìˆ˜: {len(segment_groups)}ê°œ")
            
            # 3ë‹¨ê³„: MAP ë‹¨ê³„ - ê° ê·¸ë£¹ë³„ ë¶„ì„
            print("   ğŸ—ºï¸ MAP ë‹¨ê³„: ê·¸ë£¹ë³„ ë¶„ì„ ì¤‘...")
            group_analyses = []
            
            for i, group in enumerate(segment_groups, 1):
                print(f"   ğŸ“ ê·¸ë£¹ {i}/{len(segment_groups)} ë¶„ì„ ì¤‘...")
                
                formatted_segment = self._format_segment_group(group)
                
                try:
                    chain = self.segment_analysis_prompt | self.llm
                    analysis = chain.invoke({"segment": formatted_segment})
                    
                    group_analyses.append({
                        "group_index": i,
                        "timespan": f"{group[0].timestamp} - {group[-1].timestamp}",
                        "speakers": list(set(seg.speaker for seg in group)),
                        "analysis": analysis.content
                    })
                    
                    print(f"   âœ… ê·¸ë£¹ {i} ë¶„ì„ ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"   âŒ ê·¸ë£¹ {i} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            # 4ë‹¨ê³„: REDUCE ë‹¨ê³„ - ì „ì²´ ìš”ì•½
            print("   ğŸ”— REDUCE ë‹¨ê³„: ì „ì²´ ìš”ì•½ ìƒì„± ì¤‘...")
            
            # ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©
            combined_analysis = "\n\n".join([
                f"ê·¸ë£¹ {analysis['group_index']} ({analysis['timespan']}):\n{analysis['analysis']}"
                for analysis in group_analyses
            ])
            
            # ìµœì¢… ìš”ì•½ ìƒì„±
            summary_chain = self.meeting_summary_prompt | self.llm
            final_summary = summary_chain.invoke({
                "segments_analysis": combined_analysis
            })
            
            # ë°œì–¸ì í†µê³„ ìƒì„±
            speaker_stats = {}
            for segment in segments:
                speaker_stats[segment.speaker] = speaker_stats.get(segment.speaker, 0) + 1
            
            return {
                "summary": final_summary.content,
                "success": True,
                "statistics": {
                    "total_segments": len(segments),
                    "total_groups": len(segment_groups),
                    "speakers": speaker_stats,
                    "duration_estimate": f"ì•½ {len(segments) * 1.5:.0f}ë¶„"  # ì„¸ê·¸ë¨¼íŠ¸ë‹¹ í‰ê·  1.5ë¶„ ì¶”ì •
                },
                "group_analyses": group_analyses
            }
            
        except Exception as e:
            return {
                "summary": f"íšŒì˜ë¡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "success": False,
                "error": str(e)
            }

# === Meeting GPT Map-Reduce ì‚¬ìš© ì˜ˆì‹œ ===
print("\nğŸ“‹ Meeting GPT Map-Reduce ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸:")
print("=" * 60)

# ìƒ˜í”Œ íšŒì˜ë¡ ë°ì´í„°
sample_transcript = """
[00:02:15]
ê¹€ê³¼ì¥: ì•ˆë…•í•˜ì„¸ìš”, ì˜¤ëŠ˜ í”„ë¡œì íŠ¸ ì§„í–‰ ìƒí™© íšŒì˜ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. ë¨¼ì € ê°œë°œíŒ€ í˜„í™©ë¶€í„° ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

[00:02:45] 
ì´ê°œë°œì: ë„¤, í˜„ì¬ API ê°œë°œì€ 80% ì™„ë£Œë˜ì—ˆê³ , ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼ê¹Œì§€ëŠ” ì™„ë£Œë  ì˜ˆì •ì…ë‹ˆë‹¤. ë‹¤ë§Œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ë¶€ë¶„ì—ì„œ ì„±ëŠ¥ ì´ìŠˆê°€ ìˆì–´ì„œ ë°•DBAë‹˜ê³¼ í˜‘ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

[00:03:20]
ë°•DBA: ì„±ëŠ¥ ì´ìŠˆëŠ” ì¸ë±ìŠ¤ ìµœì í™”ë¡œ í•´ê²° ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‚´ì¼ê¹Œì§€ ì‘ì—… ì™„ë£Œí•˜ê² ìŠµë‹ˆë‹¤. ì´ê°œë°œìë‹˜ê³¼ ë‚´ì¼ ì˜¤í›„ 2ì‹œì— ë¯¸íŒ… ì¡ê² ìŠµë‹ˆë‹¤.

[00:04:00]
ê¹€ê³¼ì¥: ì¢‹ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ UI ê°œë°œì€ ì–´ë–¤ê°€ìš”?

[00:04:15]
ìµœë””ìì´ë„ˆ: UI ëª©ì—…ì€ ì™„ë£Œë˜ì—ˆê³ , í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„ë„ 70% ì •ë„ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. API ì™„ë£Œë˜ë©´ ë°”ë¡œ ì—°ë™ ì‘ì—…ì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[00:05:00]
ê¹€ê³¼ì¥: ì „ì²´ì ìœ¼ë¡œ ì¼ì •ì—ëŠ” ë¬¸ì œì—†ì„ ê²ƒ ê°™ë„¤ìš”. ê·¸ëŸ°ë° í…ŒìŠ¤íŠ¸ ê³„íšì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?

[00:05:30]
ì •QA: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‘ì„±ì€ ì™„ë£Œë˜ì—ˆê³ , API ì™„ë£Œ í›„ ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ì— ë“¤ì–´ê°ˆ ì˜ˆì •ì…ë‹ˆë‹¤. ì „ì²´ í…ŒìŠ¤íŠ¸ëŠ” ë‹¤ìŒ ì£¼ í™”ìš”ì¼ê¹Œì§€ ì™„ë£Œ ëª©í‘œì…ë‹ˆë‹¤.

[00:06:15]
ê¹€ê³¼ì¥: ì•Œê² ìŠµë‹ˆë‹¤. ê·¸ëŸ¼ ìµœì¢… ë°°í¬ëŠ” ë‹¤ìŒ ì£¼ ëª©ìš”ì¼ë¡œ ê³„íší•˜ê³ , í˜¹ì‹œ ì´ìŠˆê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ê³µìœ í•´ì£¼ì„¸ìš”. ë‹¤ìŒ íšŒì˜ëŠ” ê¸ˆìš”ì¼ ê°™ì€ ì‹œê°„ì— í•˜ê² ìŠµë‹ˆë‹¤.
"""

# Meeting GPT í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
meeting_processor = MeetingMapReduceProcessor(llm)

# íšŒì˜ë¡ ì²˜ë¦¬
print("ğŸ“‹ íšŒì˜ë¡ ì²˜ë¦¬ ì¤‘...")
result = meeting_processor.process_meeting_transcript(sample_transcript)

if result["success"]:
    print("âœ… íšŒì˜ë¡ ì²˜ë¦¬ ì™„ë£Œ!")
    print("\n" + "="*50)
    print("ğŸ“‹ íšŒì˜ ìš”ì•½:")
    print(result["summary"])
    
    print("\nğŸ“Š íšŒì˜ í†µê³„:")
    stats = result["statistics"]
    for key, value in stats.items():
        if key == "speakers":
            print(f"   ğŸ‘¥ ë°œì–¸ìë³„ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜:")
            for speaker, count in value.items():
                print(f"      - {speaker}: {count}ê°œ")
        else:
            print(f"   {key}: {value}")
else:
    print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result['summary']}")

print("\nâœ… Meeting GPT Map-Reduce ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
```

## ğŸ” ë³€ìˆ˜/í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…

### Map-Reduce í•µì‹¬ í•¨ìˆ˜ë“¤

#### List Comprehension ìµœì í™”
```python
def optimized_map_processing(documents: List, question: str, map_chain) -> List[str]:
    """
    ğŸ“‹ ê¸°ëŠ¥: Python ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ í™œìš©í•œ íš¨ìœ¨ì ì¸ MAP ì²˜ë¦¬
    ğŸ“¥ ì…ë ¥: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ì§ˆë¬¸, ë§µ ì²´ì¸
    ğŸ“¤ ì¶œë ¥: ì¶”ì¶œëœ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    ğŸ’¡ í™œìš©: í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° íŒ¨í„´ìœ¼ë¡œ ì½”ë“œ ê°„ì†Œí™”
    """
    
    # ğŸ¯ í•µì‹¬: í•œ ì¤„ë¡œ MAP ë‹¨ê³„ êµ¬í˜„
    return [
        map_chain.invoke({
            "context": doc.page_content, 
            "question": question
        }).content
        for doc in documents
        if len(doc.page_content.strip()) > 10  # ë¹ˆ ë¬¸ì„œ í•„í„°ë§
    ]

# ì „í†µì ì¸ ë°©ë²• vs ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ë¹„êµ
def traditional_map_processing(documents: List, question: str, map_chain) -> List[str]:
    """ì „í†µì ì¸ ë°˜ë³µë¬¸ ë°©ì‹"""
    results = []
    for doc in documents:
        if len(doc.page_content.strip()) > 10:
            response = map_chain.invoke({
                "context": doc.page_content,
                "question": question
            })
            results.append(response.content)
    return results

# ì„±ëŠ¥ ë¹„êµ
def compare_map_approaches(documents, question, map_chain):
    """ë‘ ë°©ì‹ì˜ ì„±ëŠ¥ ë¹„êµ"""
    import time
    
    # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜
    start = time.time()
    result1 = optimized_map_processing(documents, question, map_chain)
    time1 = time.time() - start
    
    # ì „í†µì  ë°©ë²•
    start = time.time() 
    result2 = traditional_map_processing(documents, question, map_chain)
    time2 = time.time() - start
    
    print(f"ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜: {time1:.3f}ì´ˆ")
    print(f"ì „í†µì  ë°©ë²•: {time2:.3f}ì´ˆ")
    print(f"ì„±ëŠ¥ ì°¨ì´: {((time2 - time1) / time2 * 100):.1f}% ê°œì„ ")
```

#### ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œë„ ë¡œì§
```python
import functools
import time
from typing import Callable, Any

def retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0):
    """
    ğŸ“‹ ê¸°ëŠ¥: ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
    ğŸ“¥ ì…ë ¥: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜, ê¸°ë³¸ ì§€ì—° ì‹œê°„
    ğŸ’¡ í™œìš©: LLM API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            last_exception = None
            
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    
                    if attempt == max_retries:
                        break
                    
                    delay = base_delay * (2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    print(f"   âš ï¸ ì‹œë„ {attempt + 1} ì‹¤íŒ¨, {delay}ì´ˆ í›„ ì¬ì‹œë„: {e}")
                    time.sleep(delay)
            
            # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
            raise last_exception
        
        return wrapper
    return decorator

@retry_with_backoff(max_retries=3, base_delay=1.0)
def robust_map_single_document(doc, question, map_chain):
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬"""
    return map_chain.invoke({
        "context": doc.page_content,
        "question": question
    })

def fault_tolerant_map_phase(documents: List, question: str, map_chain) -> tuple:
    """
    ğŸ“‹ ê¸°ëŠ¥: ì˜¤ë¥˜ í—ˆìš© MAP ë‹¨ê³„ êµ¬í˜„
    ğŸ“¤ ì¶œë ¥: (ì„±ê³µ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, ì‹¤íŒ¨ ì •ë³´ ë¦¬ìŠ¤íŠ¸)
    """
    
    successful_results = []
    failed_documents = []
    
    for i, doc in enumerate(documents):
        try:
            result = robust_map_single_document(doc, question, map_chain)
            
            if result.content.strip():
                successful_results.append({
                    "doc_index": i,
                    "content": result.content,
                    "success": True
                })
            
        except Exception as e:
            failed_documents.append({
                "doc_index": i,
                "error": str(e),
                "doc_preview": doc.page_content[:100]
            })
            print(f"   âŒ ë¬¸ì„œ {i} ìµœì¢… ì‹¤íŒ¨: {e}")
    
    print(f"   ğŸ“Š MAP ì™„ë£Œ: {len(successful_results)}ê°œ ì„±ê³µ, {len(failed_documents)}ê°œ ì‹¤íŒ¨")
    
    return successful_results, failed_documents
```

#### ë™ì  ì²­í¬ í¬ê¸° ì¡°ì ˆ
```python
def adaptive_chunk_sizing(documents: List, max_total_tokens: int = 3000) -> List:
    """
    ğŸ“‹ ê¸°ëŠ¥: í† í° ì œí•œì— ë§ì¶° ë¬¸ì„œë“¤ì˜ ì²­í¬ í¬ê¸°ë¥¼ ë™ì  ì¡°ì ˆ
    ğŸ“¥ ì…ë ¥: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ìµœëŒ€ ì´ í† í° ìˆ˜
    ğŸ“¤ ì¶œë ¥: í¬ê¸° ì¡°ì ˆëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    
    def estimate_tokens(text: str) -> int:
        """ê°„ë‹¨í•œ í† í° ìˆ˜ ì¶”ì •"""
        return len(text) // 4
    
    # ì´ í† í° ìˆ˜ ê³„ì‚°
    total_tokens = sum(estimate_tokens(doc.page_content) for doc in documents)
    
    if total_tokens <= max_total_tokens:
        return documents  # ì¡°ì ˆ ë¶ˆí•„ìš”
    
    # ì¶•ì†Œ ë¹„ìœ¨ ê³„ì‚°
    reduction_ratio = max_total_tokens / total_tokens
    
    adjusted_documents = []
    for doc in documents:
        original_content = doc.page_content
        original_tokens = estimate_tokens(original_content)
        
        # ìƒˆë¡œìš´ í¬ê¸° ê³„ì‚°
        new_size = int(len(original_content) * reduction_ratio)
        
        # ë¬¸ì„œ í¬ê¸° ì¡°ì ˆ (ì•ë¶€ë¶„ ìš°ì„  ë³´ì¡´)
        if new_size < len(original_content):
            # ë¬¸ì¥ ê²½ê³„ì—ì„œ ìë¥´ê¸° ì‹œë„
            truncated = original_content[:new_size]
            last_period = truncated.rfind('.')
            if last_period > new_size * 0.8:  # 80% ì´ìƒ ì§€ì ì—ì„œ ë§ˆì¹¨í‘œ ë°œê²¬ì‹œ
                truncated = truncated[:last_period + 1]
            
            # ìƒˆë¡œìš´ Document ê°ì²´ ìƒì„±
            adjusted_doc = type(doc)(
                page_content=truncated + "...",
                metadata=doc.metadata
            )
            adjusted_documents.append(adjusted_doc)
        else:
            adjusted_documents.append(doc)
    
    final_tokens = sum(estimate_tokens(doc.page_content) for doc in adjusted_documents)
    print(f"   ğŸ“Š ì²­í¬ í¬ê¸° ì¡°ì ˆ: {total_tokens} â†’ {final_tokens} í† í°")
    
    return adjusted_documents

def smart_document_selection(documents: List, question: str, max_docs: int = 5) -> List:
    """
    ğŸ“‹ ê¸°ëŠ¥: ì§ˆë¬¸ ê´€ë ¨ì„± ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¬¸ì„œ ì„ íƒ
    ğŸ“¥ ì…ë ¥: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ì§ˆë¬¸, ìµœëŒ€ ë¬¸ì„œ ìˆ˜
    ğŸ“¤ ì¶œë ¥: ì„ ë³„ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    
    if len(documents) <= max_docs:
        return documents
    
    # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    question_words = set(question.lower().split())
    stop_words = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ì—ì„œ', 'the', 'a', 'an'}
    keywords = question_words - stop_words
    
    # ë¬¸ì„œë³„ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    scored_docs = []
    for doc in documents:
        content_words = set(doc.page_content.lower().split())
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        keyword_matches = len(keywords & content_words)
        keyword_score = keyword_matches / max(len(keywords), 1)
        
        # ë¬¸ì„œ ê¸¸ì´ ì ìˆ˜ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ ê²ƒ ì„ í˜¸)
        content_length = len(doc.page_content)
        if 100 <= content_length <= 1000:
            length_score = 1.0
        elif 50 <= content_length < 100 or 1000 < content_length <= 2000:
            length_score = 0.7
        else:
            length_score = 0.3
        
        # ì¢…í•© ì ìˆ˜
        total_score = keyword_score * 0.7 + length_score * 0.3
        scored_docs.append((doc, total_score))
    
    # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ ë¬¸ì„œ ì„ íƒ
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    selected_docs = [doc for doc, score in scored_docs[:max_docs]]
    
    print(f"   ğŸ” ë¬¸ì„œ ì„ ë³„: {len(documents)}ê°œ â†’ {len(selected_docs)}ê°œ")
    return selected_docs
```

## ğŸ§ª ì‹¤ìŠµ ê³¼ì œ

### ğŸ”¨ ê¸°ë³¸ ê³¼ì œ
1. **Map-Reduce vs Stuff ì„±ëŠ¥ ë¹„êµ**: ë™ì¼í•œ ì§ˆë¬¸ìœ¼ë¡œ ë‘ ì „ëµì˜ ì°¨ì´ì  ë¶„ì„
```python
# TODO: ë¬¸ì„œ ìˆ˜ë¥¼ ëŠ˜ë ¤ê°€ë©° ë‘ ì „ëµì˜ ì„±ëŠ¥ê³¼ í’ˆì§ˆ ë³€í™” ì¸¡ì •
def compare_strategies_by_doc_count():
    doc_counts = [2, 5, 10, 20]
    # ê° ë¬¸ì„œ ìˆ˜ë³„ë¡œ ì„±ëŠ¥ê³¼ ë‹µë³€ í’ˆì§ˆ ë¹„êµ
    pass
```

2. **ì»¤ìŠ¤í…€ MAP í•¨ìˆ˜**: íŠ¹ì • ë„ë©”ì¸ì— íŠ¹í™”ëœ MAP í•¨ìˆ˜ êµ¬í˜„
```python
# TODO: ë²•ë¥  ë¬¸ì„œ, ì˜ë£Œ ë¬¸ì„œ ë“± ë„ë©”ì¸ë³„ íŠ¹í™” MAP í•¨ìˆ˜
def legal_document_map(inputs):
    # ë²•ë¥  ìš©ì–´ì™€ ì¡°í•­ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
    pass
```

### ğŸš€ ì‹¬í™” ê³¼ì œ
3. **ì ì‘í˜• Map-Reduce**: ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  ì „ëµ ì„ íƒ
```python
# TODO: ì§ˆë¬¸ ë¶„ì„ì„ í†µí•´ Stuff vs Map-Reduce ìë™ ì„ íƒ
class AdaptiveProcessingChain:
    def analyze_question_complexity(self, question): pass
    def select_optimal_strategy(self, complexity_score): pass
```

4. **ë¶„ì‚° Map-Reduce**: ì—¬ëŸ¬ ì›Œì»¤ì—ì„œ ë™ì‹œ ì²˜ë¦¬í•˜ëŠ” ë¶„ì‚° ì‹œìŠ¤í…œ
```python
# TODO: ë©€í‹°í”„ë¡œì„¸ì‹±ì„ í™œìš©í•œ ì§„ì§œ ë¶„ì‚° Map-Reduce êµ¬í˜„
from multiprocessing import Pool
class DistributedMapReduce:
    def distributed_map_phase(self, documents, question): pass
```

### ğŸ’¡ ì°½ì˜ ê³¼ì œ
5. **ê³„ì¸µì  Map-Reduce**: ë¬¸ì„œë¥¼ ê³„ì¸µë³„ë¡œ ë¶„ë¥˜í•˜ì—¬ ì²˜ë¦¬
```python
# TODO: ë¬¸ì„œë¥¼ ì£¼ì œë³„ë¡œ ê³„ì¸µí™”í•œ í›„ ê³„ì¸µë³„ Map-Reduce ì ìš©
class HierarchicalMapReduce:
    def classify_documents_by_topic(self, docs): pass
    def process_by_hierarchy(self, classified_docs): pass
```

6. **ìŠ¤íŠ¸ë¦¬ë° Map-Reduce**: ì‹¤ì‹œê°„ìœ¼ë¡œ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ì‹œìŠ¤í…œ
```python
# TODO: ê° MAP ë‹¨ê³„ ê²°ê³¼ë¥¼ ì¦‰ì‹œ ìŠ¤íŠ¸ë¦¬ë°í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì‹¤ì‹œê°„ í”¼ë“œë°±
async def streaming_map_reduce(documents, question):
    async for partial_result in map_phase_stream:
        yield partial_result
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### ë¹„ìš© ê´€ë¦¬
```python
# âŒ ë¹„íš¨ìœ¨ì ì¸ ë°©ë²•: ëª¨ë“  ë¬¸ì„œë¥¼ ë¬´ì¡°ê±´ ì²˜ë¦¬
def expensive_map_reduce(documents, question):
    results = []
    for doc in documents:  # 1000ê°œ ë¬¸ì„œ = 1000ë²ˆ LLM í˜¸ì¶œ!
        result = llm.invoke(f"Document: {doc.page_content}\nQuestion: {question}")
        results.append(result)
    return combine_results(results)

# âœ… íš¨ìœ¨ì ì¸ ë°©ë²•: ì‚¬ì „ í•„í„°ë§ê³¼ ë°°ì¹˜ ì²˜ë¦¬
def cost_effective_map_reduce(documents, question, max_budget=1.0):
    # 1ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§
    filtered_docs = prefilter_documents(documents, question)
    
    # 2ë‹¨ê³„: ë¹„ìš© ì¶”ì • ë° ì œí•œ
    estimated_cost = estimate_processing_cost(filtered_docs)
    if estimated_cost > max_budget:
        filtered_docs = reduce_document_count(filtered_docs, max_budget)
    
    # 3ë‹¨ê³„: íš¨ìœ¨ì ì¸ MAP ì²˜ë¦¬
    return process_with_budget_control(filtered_docs, question)
```

### ì„±ëŠ¥ ìµœì í™” ì „ëµ
```python
# Map-Reduce ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸
optimization_checklist = {
    "ì‚¬ì „_í•„í„°ë§": "í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ ë¬¸ì„œ ì œê±°",
    "ë³‘ë ¬_ì²˜ë¦¬": "ë…ë¦½ì ì¸ MAP ì‘ì—…ì€ ë³‘ë ¬ ì‹¤í–‰",
    "ë°°ì¹˜_í¬ê¸°": "ë©”ëª¨ë¦¬ì™€ ì„±ëŠ¥ì˜ ê· í˜•ì  ì°¾ê¸°",
    "ì—ëŸ¬_ë³µêµ¬": "ê°œë³„ ë¬¸ì„œ ì‹¤íŒ¨ê°€ ì „ì²´ë¥¼ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•Šê²Œ",
    "ê²°ê³¼_ìºì‹±": "ë™ì¼í•œ ë¬¸ì„œì— ëŒ€í•œ ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€",
    "í† í°_ìµœì í™”": "í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ìµœì†Œí™”",
    "ìŠ¤ë§ˆíŠ¸_ì„ íƒ": "ë¬¸ì„œ í’ˆì§ˆê³¼ ê´€ë ¨ì„± ê¸°ë°˜ ì„ ë³„"
}
```

### í’ˆì§ˆ ë³´ì¥ ë°©ë²•
- **ì •ë³´ ì†ì‹¤ ë°©ì§€**: ê° MAP ë‹¨ê³„ì—ì„œ í•µì‹¬ ì •ë³´ ë³´ì¡´ í™•ì¸
- **ì¼ê´€ì„± ìœ ì§€**: ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ì •ë³´ ê°„ ëª¨ìˆœ í•´ê²°
- **ì¶œì²˜ ì¶”ì **: ìµœì¢… ë‹µë³€ì—ì„œ ì–´ëŠ ë¬¸ì„œì—ì„œ ì˜¨ ì •ë³´ì¸ì§€ ì¶”ì 
- **ê²€ì¦ ë‹¨ê³„**: REDUCE ì´í›„ ë‹µë³€ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦

### í™•ì¥ì„± ê³ ë ¤ì‚¬í•­
- **ë¬¸ì„œ ìˆ˜ ì¦ê°€**: ì²œ ê°œ ì´ìƒì˜ ë¬¸ì„œ ì²˜ë¦¬ì‹œ ë¶„ì‚° ì‹œìŠ¤í…œ ê³ ë ¤
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ìŠ¤íŠ¸ë¦¬ë° ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ ì•„í‚¤í…ì²˜ ë³€ê²½
- **ë‹¤ì–‘í•œ í˜•ì‹**: í…ìŠ¤íŠ¸ ì™¸ ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬
- **ì–¸ì–´ ì§€ì›**: ë‹¤êµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì–¸ì–´ë³„ ìµœì í™”

## ğŸ”— ê´€ë ¨ ìë£Œ
- **ì´ì „ í•™ìŠµ**: [6.8 Stuff LCEL Chain](./6.8_Stuff_LCEL_Chain.md)
- **ë‹¤ìŒ í•™ìŠµ**: [6.10 Recap](./6.10_Recap.md)
- **ì°¸ê³  ë¬¸ì„œ**: [MapReduce Pattern](https://en.wikipedia.org/wiki/MapReduce)
- **ë³‘ë ¬ ì²˜ë¦¬**: [Python Concurrent Futures](https://docs.python.org/3/library/concurrent.futures.html)
- **ì‹¤ìŠµ íŒŒì¼**: [6.9 Map Reduce LCEL Chain.ipynb](../../00%20lecture/6.9%20Map%20Reduce%20LCEL%20Chain.ipynb)

---

ğŸ’¡ **í•µì‹¬ ì •ë¦¬**: Map-Reduce LCEL Chainì€ ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ì˜ í•´ë‹µì…ë‹ˆë‹¤. ê° ë¬¸ì„œë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ê´€ë ¨ ì •ë³´ë¥¼ ì¶”ì¶œí•œ í›„ í†µí•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, Stuff ì „ëµì˜ í† í° ì œí•œì„ ê·¹ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. RunnableLambdaë¥¼ í™œìš©í•˜ì—¬ ì»¤ìŠ¤í…€ ë¡œì§ì„ ì²´ì¸ì— í†µí•©í•˜ê³ , ë³‘ë ¬ ì²˜ë¦¬ì™€ ë¹„ìš© ìµœì í™”ë¥¼ í†µí•´ ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ í™œìš© ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Meeting GPTì™€ ê°™ì€ ì‹¤ì œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê·¸ ì§„ê°€ë¥¼ ë°œíœ˜í•©ë‹ˆë‹¤.