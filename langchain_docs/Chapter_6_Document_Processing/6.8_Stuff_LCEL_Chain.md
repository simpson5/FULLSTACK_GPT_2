# ğŸ“– Section 6.8: Stuff LCEL Chain - íˆ¬ëª…í•œ ì²´ì¸ êµ¬í˜„

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- âœ… LCEL(LangChain Expression Language)ì„ ì‚¬ìš©í•œ íˆ¬ëª…í•œ ì²´ì¸ êµ¬í˜„
- âœ… RunnablePassthroughì™€ ë³‘ë ¬ ì‹¤í–‰(RunnableParallel)ì˜ ì´í•´ì™€ í™œìš©
- âœ… ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥í•œ Stuff ì „ëµ ì²´ì¸ êµ¬ì¶•
- âœ… ê¸°ì¡´ "magical" chain ëŒ€ë¹„ LCELì˜ ì¥ì ê³¼ íˆ¬ëª…ì„± ì´í•´

## ğŸ§  í•µì‹¬ ê°œë…

### LCEL(LangChain Expression Language)ì´ë€?
**LCEL**ì€ LangChainì˜ í‘œí˜„ ì–¸ì–´ë¡œ, ì²´ì¸ì˜ ê° êµ¬ì„± ìš”ì†Œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜í•˜ì—¬ íˆ¬ëª…í•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥í•œ ì²´ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

```mermaid
graph TD
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B{RunnableParallel}
    B --> C[Retriever<br/>ë¬¸ì„œ ê²€ìƒ‰]
    B --> D[RunnablePassthrough<br/>ì§ˆë¬¸ ì „ë‹¬]
    
    C --> E[ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸]
    D --> F[ì›ë³¸ ì§ˆë¬¸]
    
    E --> G[ChatPromptTemplate<br/>ì»¨í…ìŠ¤íŠ¸ + ì§ˆë¬¸]
    F --> G
    
    G --> H[í¬ë§·ëœ í”„ë¡¬í”„íŠ¸]
    H --> I[ChatOpenAI<br/>LLM]
    I --> J[ìµœì¢… ë‹µë³€]
    
    style A fill:#E6F3FF
    style B fill:#FFE6CC
    style J fill:#E6FFE6
```

### ê¸°ì¡´ ë°©ì‹ vs LCEL ë¹„êµ

| íŠ¹ì„± | RetrievalQA Chain | LCEL Chain |
|------|-------------------|------------|
| **íˆ¬ëª…ì„±** | "Magical" ë¸”ë™ë°•ìŠ¤ | ëª¨ë“  ë‹¨ê³„ ëª…ì‹œì  ì •ì˜ |
| **ì»¤ìŠ¤í„°ë§ˆì´ì§•** | ì œí•œì  | ì™„ì „í•œ ììœ ë„ |
| **ë””ë²„ê¹…** | ì–´ë ¤ì›€ | ê° ë‹¨ê³„ë³„ ì¶”ì  ê°€ëŠ¥ |
| **ì„±ëŠ¥** | ìˆœì°¨ ì‹¤í–‰ | ë³‘ë ¬ ì‹¤í–‰ ìµœì í™” |
| **í•™ìŠµ ê³¡ì„ ** | ì‰¬ì›€ (ì„¤ì • ìµœì†Œ) | ë³´í†µ (êµ¬ì¡° ì´í•´ í•„ìš”) |

### RunnablePassthroughì˜ ì—­í• 
**RunnablePassthrough**ëŠ” ì…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œì¼œ ì²´ì¸ì˜ ë‹¤ë¥¸ ë¶€ë¶„ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
# ğŸ§  ê°œë…: ë™ì¼í•œ ì…ë ¥ì„ ì—¬ëŸ¬ ê³³ì—ì„œ ì‚¬ìš©
input_text = "Winstonì€ ì–´ë””ì— ì‚´ê³  ìˆë‚˜ìš”?"

# RunnablePassthroughëŠ” ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
passthrough_output = RunnablePassthrough().invoke(input_text)
# passthrough_output == "Winstonì€ ì–´ë””ì— ì‚´ê³  ìˆë‚˜ìš”?"
```

## ğŸ“‹ ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤

### LCEL í•µì‹¬ êµ¬ì„± ìš”ì†Œ
```python
from langchain.schema.runnable import RunnablePassthrough, RunnableParallel
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI

class RunnablePassthrough:
    def invoke(self, input_data):
        """
        ğŸ“‹ ê¸°ëŠ¥: ì…ë ¥ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚´
        ğŸ“¥ ì…ë ¥: ì„ì˜ì˜ ë°ì´í„°
        ğŸ“¤ ì¶œë ¥: ë™ì¼í•œ ë°ì´í„°
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ë™ì¼í•œ ì…ë ¥ì„ ì²´ì¸ì˜ ì—¬ëŸ¬ ë¶€ë¶„ì—ì„œ ì‚¬ìš©
        """
        return input_data

class RunnableParallel:
    def __init__(self, **kwargs):
        """
        ğŸ“‹ ê¸°ëŠ¥: ì—¬ëŸ¬ Runnableì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        ğŸ“¥ ì…ë ¥: í‚¤-ê°’ ìŒì˜ Runnable ë”•ì…”ë„ˆë¦¬
        ğŸ“¤ ì¶œë ¥: ê° Runnableì˜ ê²°ê³¼ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ë…ë¦½ì ì¸ ì‘ì—…ë“¤ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ
        """
```

### ChatPromptTemplate ê³ ê¸‰ ì‚¬ìš©ë²•
```python
from langchain.prompts import ChatPromptTemplate

# RAGìš© ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
rag_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
    ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•˜ê³ , ë‹µì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    
    ì»¨í…ìŠ¤íŠ¸:
    {context}"""),
    ("human", "{question}")
])
```

## ğŸ”§ ë™ì‘ ê³¼ì • ìƒì„¸

### 1ë‹¨ê³„: ê¸°ë³¸ LCEL Stuff Chain êµ¬í˜„
```python
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# === ê¸°ë³¸ LCEL Stuff Chain êµ¬í˜„ ===
# ğŸ§  ê°œë…: íˆ¬ëª…í•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥í•œ ì²´ì¸ êµ¬ì¶•

print("ğŸ”— LCEL Stuff Chain êµ¬ì¶•:")
print("=" * 50)

# ğŸ”§ 1ë‹¨ê³„: í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ ì¤€ë¹„
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.1  # ğŸ“Œ ë‚®ì€ ì°½ì˜ì„±ìœ¼ë¡œ ì¼ê´€ëœ ë‹µë³€
)

# ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œì—ì„œ retriever ìƒì„±
embeddings = OpenAIEmbeddings()
vector_store = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
retriever = vector_store.as_retriever()

# ğŸ”§ 2ë‹¨ê³„: RAG ì „ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì•„ë˜ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    ë‹µì„ ëª¨ë¥´ëŠ” ê²½ìš°ì—ëŠ” ì†”ì§íˆ 'ëª¨ë¥´ê² ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ê³ ,
    ë‹µì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
    
    ì»¨í…ìŠ¤íŠ¸:
    {context}"""),
    ("human", "{question}")
])

print("âœ… êµ¬ì„± ìš”ì†Œ ì¤€ë¹„ ì™„ë£Œ")
print(f"   ğŸ“Š LLM ëª¨ë¸: {llm.model_name}")
print(f"   ğŸ“Š Temperature: {llm.temperature}")
print(f"   ğŸ“Š Retriever ì„¤ì •: k={retriever.search_kwargs.get('k', 4)}")

# ğŸ”§ 3ë‹¨ê³„: LCEL ì²´ì¸ êµ¬ì„±
# í•µì‹¬: ê° êµ¬ì„± ìš”ì†Œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì—°ê²°
chain = (
    {
        "context": retriever,                    # ğŸ“Œ ì…ë ¥ ì§ˆë¬¸ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        "question": RunnablePassthrough()       # ğŸ“Œ ì…ë ¥ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
    }
    | prompt                                    # ğŸ“Œ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ í”„ë¡¬í”„íŠ¸ì— í¬ë§·
    | llm                                       # ğŸ“Œ í¬ë§·ëœ í”„ë¡¬í”„íŠ¸ë¥¼ LLMì— ì „ë‹¬
)

print("âœ… LCEL Chain êµ¬ì„± ì™„ë£Œ")
print(f"   ğŸ”— ì²´ì¸ êµ¬ì¡°: {{context: retriever, question: passthrough}} â†’ prompt â†’ llm")

# ğŸ”§ 4ë‹¨ê³„: ì²´ì¸ í…ŒìŠ¤íŠ¸
test_questions = [
    "Winston SmithëŠ” ì–´ë””ì— ì‚´ê³  ìˆë‚˜ìš”?",
    "Victory Mansionsì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "Ministry of Loveì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€ìš”?"
]

for i, question in enumerate(test_questions, 1):
    print(f"\n{'='*20} í…ŒìŠ¤íŠ¸ {i} {'='*20}")
    print(f"ğŸ“‹ ì§ˆë¬¸: {question}")
    
    try:
        # LCEL ì²´ì¸ ì‹¤í–‰
        start_time = time.time()
        response = chain.invoke(question)
        execution_time = time.time() - start_time
        
        print(f"ğŸ¤– ë‹µë³€: {response.content}")
        print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

print("\nâœ… ê¸°ë³¸ LCEL Stuff Chain í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
```

### 2ë‹¨ê³„: ê³ ê¸‰ LCEL Chain (ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦ í¬í•¨)
```python
from langchain.schema.runnable import RunnableLambda
from typing import List, Dict, Any

# === ê³ ê¸‰ LCEL Chain with ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦ ===
# ğŸ§  ê°œë…: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ì—¬ ë‹µë³€ ì‹ ë¢°ë„ í–¥ìƒ

def format_docs(docs: List) -> str:
    """
    ğŸ“‹ ê¸°ëŠ¥: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ í¬ë§·
    ğŸ“¥ ì…ë ¥: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    ğŸ“¤ ì¶œë ¥: í¬ë§·ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•  ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬
    """
    if not docs:
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    formatted_docs = []
    for i, doc in enumerate(docs, 1):
        content = doc.page_content.strip()
        # ë¬¸ì„œê°€ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 500ìë§Œ ì‚¬ìš©
        if len(content) > 500:
            content = content[:500] + "..."
        
        formatted_docs.append(f"[ë¬¸ì„œ {i}]\n{content}")
    
    return "\n\n".join(formatted_docs)

def filter_relevant_docs(docs: List, query: str, min_score: float = 0.7) -> List:
    """
    ğŸ“‹ ê¸°ëŠ¥: ê´€ë ¨ì„±ì´ ë‚®ì€ ë¬¸ì„œ í•„í„°ë§
    ğŸ“¥ ì…ë ¥: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ì§ˆì˜, ìµœì†Œ ì ìˆ˜
    ğŸ“¤ ì¶œë ¥: í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ë…¸ì´ì¦ˆ ë¬¸ì„œ ì œê±°ë¡œ ë‹µë³€ í’ˆì§ˆ í–¥ìƒ
    """
    # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•œ í•„í„°ë§ ë¡œì§ ì‚¬ìš©
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§
    
    filtered_docs = []
    query_words = set(query.lower().split())
    
    for doc in docs:
        content_words = set(doc.page_content.lower().split())
        
        # ì§ˆì˜ ë‹¨ì–´ì™€ì˜ ê²¹ì¹¨ ì •ë„ ê³„ì‚°
        intersection = len(query_words & content_words)
        similarity = intersection / len(query_words) if query_words else 0
        
        if similarity >= min_score or len(filtered_docs) == 0:  # ìµœì†Œ 1ê°œëŠ” ìœ ì§€
            filtered_docs.append(doc)
    
    return filtered_docs[:4]  # ìµœëŒ€ 4ê°œ ë¬¸ì„œë§Œ ì‚¬ìš©

def create_retrieval_chain_with_quality_check() -> dict:
    """ê³ ê¸‰ ê²€ìƒ‰ ì²´ì¸ ìƒì„± (ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦ í¬í•¨)"""
    
    def enhanced_retrieval(query: str) -> str:
        """í’ˆì§ˆ ê²€ì¦ì´ í¬í•¨ëœ ê²€ìƒ‰ í•¨ìˆ˜"""
        # 1ë‹¨ê³„: ê¸°ë³¸ ê²€ìƒ‰
        raw_docs = retriever.get_relevant_documents(query)
        
        # 2ë‹¨ê³„: ê´€ë ¨ì„± í•„í„°ë§
        filtered_docs = filter_relevant_docs(raw_docs, query, min_score=0.3)
        
        # 3ë‹¨ê³„: ë¬¸ì„œ í¬ë§·íŒ…
        formatted_context = format_docs(filtered_docs)
        
        return formatted_context
    
    return RunnableLambda(enhanced_retrieval)

print("\nğŸ”§ ê³ ê¸‰ LCEL Chain êµ¬ì„±:")
print("=" * 50)

# ê³ ê¸‰ ê²€ìƒ‰ ì²´ì¸ ìƒì„±
enhanced_retriever = create_retrieval_chain_with_quality_check()

# í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
enhanced_prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì£¼ì˜ ê¹Šê²Œ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
    
    ë‹µë³€ ê°€ì´ë“œë¼ì¸:
    1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”
    2. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
    3. ê´€ë ¨ ë¬¸ì„œê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§íˆ ë§í•˜ì„¸ìš”
    4. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
    
    ì»¨í…ìŠ¤íŠ¸:
    {context}
    
    ìœ„ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´, "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”."""),
    ("human", "{question}")
])

# ê³ ê¸‰ LCEL ì²´ì¸ êµ¬ì„±
enhanced_chain = (
    {
        "context": enhanced_retriever,
        "question": RunnablePassthrough()
    }
    | enhanced_prompt
    | llm
)

print("âœ… ê³ ê¸‰ LCEL Chain êµ¬ì„± ì™„ë£Œ")

# ê³ ê¸‰ ì²´ì¸ í…ŒìŠ¤íŠ¸
test_cases = [
    {
        "question": "Winston Smithì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "description": "êµ¬ì²´ì  ì‚¬ì‹¤ ì§ˆë¬¸"
    },
    {
        "question": "ì†Œì„¤ì—ì„œ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ë“±ì¥ì¸ë¬¼ì˜ ì·¨ë¯¸ëŠ”?",
        "description": "ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ ì§ˆë¬¸ (ë¶ˆí™•ì‹¤ì„± í…ŒìŠ¤íŠ¸)"
    },
    {
        "question": "1984ë…„ ì†Œì„¤ì˜ ì „ë°˜ì ì¸ ë¶„ìœ„ê¸°ëŠ” ì–´ë–¤ê°€ìš”?",
        "description": "í•´ì„ì´ í•„ìš”í•œ ì§ˆë¬¸"
    }
]

for i, test_case in enumerate(test_cases, 1):
    print(f"\n{'='*25} ê³ ê¸‰ í…ŒìŠ¤íŠ¸ {i} {'='*25}")
    print(f"ğŸ“‹ ì§ˆë¬¸: {test_case['question']}")
    print(f"ğŸ“ ìœ í˜•: {test_case['description']}")
    
    try:
        start_time = time.time()
        response = enhanced_chain.invoke(test_case['question'])
        execution_time = time.time() - start_time
        
        print(f"ğŸ¤– ë‹µë³€: {response.content}")
        print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        
        # ë‹µë³€ í’ˆì§ˆ í‰ê°€
        answer_length = len(response.content.split())
        has_uncertainty = any(phrase in response.content.lower() 
                            for phrase in ['ëª¨ë¥´ê² ', 'ì°¾ì„ ìˆ˜ ì—†', 'not sure', "don't know"])
        
        print(f"ğŸ“Š ë‹µë³€ ê¸¸ì´: {answer_length} ë‹¨ì–´")
        print(f"ğŸ“Š ë¶ˆí™•ì‹¤ì„± í‘œí˜„: {'ìˆìŒ' if has_uncertainty else 'ì—†ìŒ'}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
```

### 3ë‹¨ê³„: ë³‘ë ¬ ì‹¤í–‰ê³¼ ì„±ëŠ¥ ìµœì í™”
```python
import asyncio
from langchain.schema.runnable import RunnableParallel
import time

# === ë³‘ë ¬ ì‹¤í–‰ê³¼ ì„±ëŠ¥ ë¶„ì„ ===
# ğŸ§  ê°œë…: RunnableParallelì˜ ì„±ëŠ¥ ì´ì  ì¸¡ì •

def create_parallel_analysis_chain():
    """ë³‘ë ¬ ì‹¤í–‰ ë¶„ì„ì„ ìœ„í•œ ì²´ì¸ ìƒì„±"""
    
    # ì‹œê°„ ì¸¡ì •ìš© ë˜í¼ í•¨ìˆ˜ë“¤
    def timed_retrieval(query: str) -> str:
        """ì‹œê°„ ì¸¡ì •ì´ í¬í•¨ëœ ê²€ìƒ‰"""
        start = time.time()
        docs = retriever.get_relevant_documents(query)
        end = time.time()
        
        formatted = format_docs(docs)
        print(f"   ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {end - start:.3f}ì´ˆ")
        return formatted
    
    def timed_passthrough(query: str) -> str:
        """ì‹œê°„ ì¸¡ì •ì´ í¬í•¨ëœ passthrough"""
        start = time.time()
        result = query
        end = time.time()
        
        print(f"   â¡ï¸ Passthrough ì™„ë£Œ: {end - start:.3f}ì´ˆ")
        return result
    
    # ë³‘ë ¬ ì‹¤í–‰ ì²´ì¸
    parallel_chain = RunnableParallel(
        context=RunnableLambda(timed_retrieval),
        question=RunnableLambda(timed_passthrough)
    )
    
    return parallel_chain

def create_sequential_analysis_chain():
    """ìˆœì°¨ ì‹¤í–‰ ë¹„êµë¥¼ ìœ„í•œ ì²´ì¸ ìƒì„±"""
    
    def sequential_processing(query: str) -> dict:
        """ìˆœì°¨ì  ì²˜ë¦¬"""
        print("   ğŸ“Š ìˆœì°¨ ì‹¤í–‰ ì‹œì‘:")
        
        # 1ë‹¨ê³„: ê²€ìƒ‰
        start = time.time()
        docs = retriever.get_relevant_documents(query)
        context = format_docs(docs)
        end = time.time()
        print(f"   ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {end - start:.3f}ì´ˆ")
        
        # 2ë‹¨ê³„: Passthrough (ì¸ìœ„ì  ì§€ì—° ì‹œë®¬ë ˆì´ì…˜)
        start = time.time()
        question = query
        end = time.time()
        print(f"   â¡ï¸ Passthrough ì™„ë£Œ: {end - start:.3f}ì´ˆ")
        
        return {
            "context": context,
            "question": question
        }
    
    return RunnableLambda(sequential_processing)

print("\nâš¡ ë³‘ë ¬ vs ìˆœì°¨ ì‹¤í–‰ ì„±ëŠ¥ ë¹„êµ:")
print("=" * 60)

# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
test_query = "Winston Smithì˜ ì¼ìƒìƒí™œì€ ì–´ë–¤ê°€ìš”?"

# ë³‘ë ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸: {test_query}")
print(f"\nğŸ”„ ë³‘ë ¬ ì‹¤í–‰ (RunnableParallel) í…ŒìŠ¤íŠ¸:")

parallel_chain = create_parallel_analysis_chain()

start_time = time.time()
parallel_result = parallel_chain.invoke(test_query)
parallel_total_time = time.time() - start_time

print(f"   âœ… ë³‘ë ¬ ì‹¤í–‰ ì´ ì‹œê°„: {parallel_total_time:.3f}ì´ˆ")

# ìˆœì°¨ ì‹¤í–‰ í…ŒìŠ¤íŠ¸  
print(f"\nğŸ”„ ìˆœì°¨ ì‹¤í–‰ í…ŒìŠ¤íŠ¸:")

sequential_chain = create_sequential_analysis_chain()

start_time = time.time()
sequential_result = sequential_chain.invoke(test_query)
sequential_total_time = time.time() - start_time

print(f"   âœ… ìˆœì°¨ ì‹¤í–‰ ì´ ì‹œê°„: {sequential_total_time:.3f}ì´ˆ")

# ì„±ëŠ¥ ë¹„êµ
if sequential_total_time > 0:
    speedup = sequential_total_time / parallel_total_time
    efficiency = ((sequential_total_time - parallel_total_time) / sequential_total_time) * 100
    
    print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
    print(f"   ğŸš€ ì†ë„ í–¥ìƒ: {speedup:.2f}ë°°")
    print(f"   ğŸ“Š íš¨ìœ¨ì„± ê°œì„ : {efficiency:.1f}%")
    
    if speedup > 1.1:
        print(f"   ğŸ’¡ ë³‘ë ¬ ì‹¤í–‰ì´ íš¨ê³¼ì ì…ë‹ˆë‹¤!")
    else:
        print(f"   ğŸ’¡ ì´ ê²½ìš° ë³‘ë ¬ ì‹¤í–‰ ì´ì ì´ ì œí•œì ì…ë‹ˆë‹¤.")
```

### 4ë‹¨ê³„: ì»¤ìŠ¤í…€ ì²´ì¸ êµ¬ì„± ìš”ì†Œ ê°œë°œ
```python
from langchain.schema.runnable import Runnable
from langchain.callbacks.manager import CallbackManagerForChainRun
from typing import Any, Dict, List, Optional

# === ì»¤ìŠ¤í…€ Runnable êµ¬ì„± ìš”ì†Œ ê°œë°œ ===
# ğŸ§  ê°œë…: íŠ¹í™”ëœ ê¸°ëŠ¥ì„ ê°€ì§„ ì»¤ìŠ¤í…€ êµ¬ì„± ìš”ì†Œ ìƒì„±

class ContextualRetriever(Runnable):
    """
    ğŸ¯ ìƒí™© ì¸ì‹ ê²€ìƒ‰ê¸°
    
    íŠ¹ì§•:
    - ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë™ì  ê²€ìƒ‰ ì „ëµ
    - ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ ë¬¸ì„œ í•„í„°ë§
    - ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ìµœì í™”
    """
    
    def __init__(self, vector_store, default_k: int = 4):
        self.vector_store = vector_store
        self.retriever = vector_store.as_retriever()
        self.default_k = default_k
    
    def invoke(
        self, 
        input_data: str, 
        config: Optional[Dict] = None,
        **kwargs: Any
    ) -> str:
        """ìƒí™©ì— ë§ëŠ” ì§€ëŠ¥í˜• ê²€ìƒ‰ ìˆ˜í–‰"""
        
        query = input_data
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        question_type = self._analyze_question_type(query)
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ê²€ìƒ‰ ì „ëµ ì„ íƒ
        search_params = self._get_search_params(question_type)
        
        print(f"   ğŸ” ì§ˆë¬¸ ìœ í˜•: {question_type}")
        print(f"   âš™ï¸ ê²€ìƒ‰ ì„¤ì •: {search_params}")
        
        # ê²€ìƒ‰ ì‹¤í–‰
        docs = self.vector_store.similarity_search(
            query, 
            k=search_params['k']
        )
        
        # ê²°ê³¼ í›„ì²˜ë¦¬
        filtered_docs = self._post_process_docs(docs, query, question_type)
        
        # í¬ë§·íŒ…
        formatted_context = self._format_context(filtered_docs, question_type)
        
        return formatted_context
    
    def _analyze_question_type(self, query: str) -> str:
        """ì§ˆë¬¸ ìœ í˜• ìë™ ë¶„ë¥˜"""
        
        query_lower = query.lower()
        
        # íŒ¨í„´ ê¸°ë°˜ ë¶„ë¥˜
        if any(word in query_lower for word in ['ëˆ„êµ¬', 'who', 'ì¸ë¬¼', 'ë“±ì¥ì¸ë¬¼']):
            return "character"
        elif any(word in query_lower for word in ['ì–´ë””', 'where', 'ì¥ì†Œ', 'ìœ„ì¹˜']):
            return "location"
        elif any(word in query_lower for word in ['ì–¸ì œ', 'when', 'ì‹œê°„', 'ì‹œê¸°']):
            return "time"
        elif any(word in query_lower for word in ['ë¬´ì—‡', 'what', 'ì„¤ëª…', 'describe']):
            return "description"
        elif any(word in query_lower for word in ['ì™œ', 'why', 'ì´ìœ ', 'ì›ì¸']):
            return "explanation"
        elif any(word in query_lower for word in ['ì–´ë–»ê²Œ', 'how', 'ë°©ë²•', 'ê³¼ì •']):
            return "process"
        else:
            return "general"
    
    def _get_search_params(self, question_type: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ ìœ í˜•ë³„ ìµœì  ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜"""
        
        params_map = {
            "character": {"k": 3},  # ì¸ë¬¼ ì •ë³´ëŠ” ì ì€ ë¬¸ì„œë¡œë„ ì¶©ë¶„
            "location": {"k": 2},   # ì¥ì†Œ ì •ë³´ë„ ì§‘ì¤‘ëœ ê²€ìƒ‰
            "time": {"k": 3},       # ì‹œê°„ ì •ë³´
            "description": {"k": 4}, # ì„¤ëª…ì€ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í•„ìš”
            "explanation": {"k": 5}, # ì„¤ëª…/ë¶„ì„ì€ í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸
            "process": {"k": 4},    # ê³¼ì • ì„¤ëª…
            "general": {"k": 4}     # ì¼ë°˜ì ì¸ ì§ˆë¬¸
        }
        
        return params_map.get(question_type, {"k": self.default_k})
    
    def _post_process_docs(self, docs: List, query: str, question_type: str) -> List:
        """ê²€ìƒ‰ ê²°ê³¼ í›„ì²˜ë¦¬"""
        
        # ì¤‘ë³µ ì œê±°
        unique_docs = []
        seen_contents = set()
        
        for doc in docs:
            content_hash = hash(doc.page_content[:100])  # ì²˜ìŒ 100ì ê¸°ì¤€ ì¤‘ë³µ ì²´í¬
            if content_hash not in seen_contents:
                unique_docs.append(doc)
                seen_contents.add(content_hash)
        
        # ì§ˆë¬¸ ìœ í˜•ë³„ ì¶”ê°€ í•„í„°ë§
        if question_type == "character":
            # ì¸ë¬¼ ê´€ë ¨ ì§ˆë¬¸ì€ ì¸ë¬¼ëª…ì´ í¬í•¨ëœ ë¬¸ì„œ ìš°ì„ 
            character_docs = [doc for doc in unique_docs if self._contains_character_info(doc.page_content)]
            if character_docs:
                unique_docs = character_docs[:3]
        
        return unique_docs[:5]  # ìµœëŒ€ 5ê°œ ë¬¸ì„œ
    
    def _contains_character_info(self, content: str) -> bool:
        """ë¬¸ì„œê°€ ì¸ë¬¼ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸"""
        character_indicators = ['said', 'ë§í–ˆ', 'thought', 'ìƒê°í–ˆ', 'felt', 'ëŠê¼ˆ']
        return any(indicator in content.lower() for indicator in character_indicators)
    
    def _format_context(self, docs: List, question_type: str) -> str:
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        
        if not docs:
            return f"[{question_type} ìœ í˜• ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.]"
        
        formatted_parts = []
        for i, doc in enumerate(docs, 1):
            content = doc.page_content.strip()
            
            # ì§ˆë¬¸ ìœ í˜•ë³„ ë‚´ìš© ê¸¸ì´ ì¡°ì ˆ
            max_length = {
                "character": 300,
                "location": 250,
                "description": 400,
                "explanation": 500,
                "general": 350
            }.get(question_type, 350)
            
            if len(content) > max_length:
                content = content[:max_length] + "..."
            
            formatted_parts.append(f"[ì°¸ê³ ìë£Œ {i}]\n{content}")
        
        return "\n\n".join(formatted_parts)

class SmartPromptTemplate(Runnable):
    """
    ğŸ¯ ì§€ëŠ¥í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    
    íŠ¹ì§•:
    - ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë™ì  í”„ë¡¬í”„íŠ¸ ì¡°ì •
    - ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆì— ë”°ë¥¸ ì§€ì‹œì‚¬í•­ ë³€ê²½
    - ë‹µë³€ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ë§ì¶¤í˜• ì§€ì¹¨
    """
    
    def __init__(self):
        self.base_templates = self._create_base_templates()
    
    def invoke(self, input_data: Dict, config: Optional[Dict] = None, **kwargs) -> List:
        """ì…ë ¥ì— ë”°ë¥¸ ë™ì  í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        context = input_data.get("context", "")
        question = input_data.get("question", "")
        
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        question_type = self._analyze_question_type(question)
        
        # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€
        context_quality = self._assess_context_quality(context, question)
        
        # ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        prompt_template = self._select_prompt_template(question_type, context_quality)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        messages = prompt_template.format_messages(context=context, question=question)
        
        return messages
    
    def _create_base_templates(self) -> Dict[str, ChatPromptTemplate]:
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ë“¤"""
        
        return {
            "high_quality": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ë¬¸í•™ ì‘í’ˆ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                ì œê³µëœ ê³ í’ˆì§ˆ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
                
                ì»¨í…ìŠ¤íŠ¸:
                {context}"""),
                ("human", "{question}")
            ]),
            
            "medium_quality": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì£¼ì˜ê¹Šê²Œ ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
                ì •ë³´ê°€ ë¶€ì¡±í•œ ë¶€ë¶„ì€ ì†”ì§íˆ ì¸ì •í•˜ì„¸ìš”.
                
                ì»¨í…ìŠ¤íŠ¸:
                {context}"""),
                ("human", "{question}")
            ]),
            
            "low_quality": ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ì‹ ì¤‘í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ, í™•ì‹¤í•œ ì •ë³´ë§Œ ë‹µë³€í•˜ì„¸ìš”.
                ì¶”ì¸¡í•˜ì§€ ë§ê³ , ë¶ˆí™•ì‹¤í•œ ê²½ìš° ì†”ì§íˆ ë§í•˜ì„¸ìš”.
                
                ì»¨í…ìŠ¤íŠ¸:
                {context}"""),
                ("human", "{question}")
            ])
        }
    
    def _analyze_question_type(self, question: str) -> str:
        """ì§ˆë¬¸ ìœ í˜• ë¶„ì„ (ContextualRetrieverì™€ ë™ì¼í•œ ë¡œì§)"""
        query_lower = question.lower()
        
        if any(word in query_lower for word in ['ëˆ„êµ¬', 'who', 'ì¸ë¬¼']):
            return "character"
        elif any(word in query_lower for word in ['ì–´ë””', 'where', 'ì¥ì†Œ']):
            return "location"
        elif any(word in query_lower for word in ['ì„¤ëª…', 'describe', 'ì–´ë–¤']):
            return "description"
        elif any(word in query_lower for word in ['ì™œ', 'why', 'ì´ìœ ']):
            return "explanation"
        else:
            return "general"
    
    def _assess_context_quality(self, context: str, question: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€"""
        
        if "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in context:
            return "low_quality"
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ì¤€
        context_length = len(context.split())
        
        # ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ì˜ ê´€ë ¨ì„± ì²´í¬
        question_words = set(question.lower().split())
        context_words = set(context.lower().split())
        relevance = len(question_words & context_words) / max(len(question_words), 1)
        
        if context_length > 200 and relevance > 0.3:
            return "high_quality"
        elif context_length > 100 and relevance > 0.2:
            return "medium_quality"
        else:
            return "low_quality"
    
    def _select_prompt_template(self, question_type: str, context_quality: str) -> ChatPromptTemplate:
        """ì§ˆë¬¸ ìœ í˜•ê³¼ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ"""
        
        return self.base_templates[context_quality]

# === ì»¤ìŠ¤í…€ êµ¬ì„± ìš”ì†Œë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ LCEL Chain ===
print("\nğŸš€ ì»¤ìŠ¤í…€ êµ¬ì„± ìš”ì†Œ ê¸°ë°˜ ê³ ê¸‰ LCEL Chain:")
print("=" * 60)

# ì»¤ìŠ¤í…€ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
contextual_retriever = ContextualRetriever(vector_store)
smart_prompt = SmartPromptTemplate()

# ê³ ê¸‰ ì»¤ìŠ¤í…€ ì²´ì¸ êµ¬ì„±
advanced_custom_chain = (
    {
        "context": contextual_retriever,
        "question": RunnablePassthrough()
    }
    | smart_prompt
    | llm
)

print("âœ… ì»¤ìŠ¤í…€ LCEL Chain êµ¬ì„± ì™„ë£Œ")

# ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
advanced_test_cases = [
    "Winston SmithëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",        # character
    "Winstonì€ ì–´ë””ì— ì‚´ê³  ìˆë‚˜ìš”?",       # location  
    "Victory Mansionsë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",     # description
    "Winstonì´ ì¼ê¸°ë¥¼ ì“°ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",  # explanation
    "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë“±ì¥ì¸ë¬¼ì˜ ì´ë¦„ì€?",     # ì •ë³´ ë¶€ì¡± ìƒí™©
]

for i, question in enumerate(advanced_test_cases, 1):
    print(f"\n{'='*15} ì»¤ìŠ¤í…€ ì²´ì¸ í…ŒìŠ¤íŠ¸ {i} {'='*15}")
    print(f"ğŸ“‹ ì§ˆë¬¸: {question}")
    
    try:
        start_time = time.time()
        response = advanced_custom_chain.invoke(question)
        execution_time = time.time() - start_time
        
        print(f"ğŸ¤– ë‹µë³€: {response.content}")
        print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

print("\nâœ… ì»¤ìŠ¤í…€ LCEL Chain í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
```

## ğŸ’» ì‹¤ì „ ì˜ˆì œ

### í”„ë¡œë•ì…˜ ê¸‰ LCEL Document GPT
```python
from typing import Optional, Dict, Any, List
import logging
from langchain.callbacks import get_openai_callback
from langchain.schema.runnable import Runnable, RunnablePassthrough
import json
import time

class ProductionLCELDocumentGPT:
    """
    ğŸ¯ í”„ë¡œë•ì…˜ í™˜ê²½ìš© LCEL Document GPT
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ëª¨ë“ˆì‹ êµ¬ì„± ìš”ì†Œ ì„¤ê³„
    - ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    - í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜
    - ìƒì„¸ ë¡œê¹… ë° ë””ë²„ê¹…
    """
    
    def __init__(self, 
                 vector_store,
                 llm_config: Dict[str, Any] = None,
                 retriever_config: Dict[str, Any] = None,
                 enable_monitoring: bool = True):
        
        self.vector_store = vector_store
        self.enable_monitoring = enable_monitoring
        
        # ì„¤ì • ì´ˆê¸°í™”
        self.llm_config = llm_config or {"model": "gpt-3.5-turbo", "temperature": 0.1}
        self.retriever_config = retriever_config or {"k": 4, "score_threshold": 0.7}
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self._initialize_components()
        
        # ëª¨ë‹ˆí„°ë§ ì„¤ì •
        if enable_monitoring:
            logging.basicConfig(level=logging.INFO)
            self.logger = logging.getLogger(__name__)
            self.metrics = {
                "total_queries": 0,
                "successful_queries": 0,
                "avg_response_time": 0.0,
                "total_cost": 0.0,
                "error_count": 0
            }
        
        # ì²´ì¸ êµ¬ì„±
        self.chain = self._build_chain()
    
    def _initialize_components(self):
        """í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(**self.llm_config)
        
        # Retriever ì´ˆê¸°í™”
        self.retriever = self.vector_store.as_retriever(
            search_kwargs=self.retriever_config
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¬¸ì„œ ë¶„ì„ AIì…ë‹ˆë‹¤.
            
            ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”:
            
            ì§€ì¹¨:
            1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
            2. ì •ë³´ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ ì†”ì§íˆ ì¸ì •í•˜ì„¸ìš”
            3. ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì€ í•˜ì§€ ë§ˆì„¸ìš”
            4. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
            
            ì»¨í…ìŠ¤íŠ¸:
            {context}"""),
            ("human", "{question}")
        ])
    
    def _build_chain(self) -> Runnable:
        """LCEL ì²´ì¸ êµ¬ì„±"""
        
        # ì—ëŸ¬ ì²˜ë¦¬ê°€ í¬í•¨ëœ retriever ë˜í¼
        def safe_retrieval(query: str) -> str:
            try:
                docs = self.retriever.get_relevant_documents(query)
                if not docs:
                    return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                # ë¬¸ì„œ í¬ë§·íŒ…
                formatted_docs = []
                for i, doc in enumerate(docs[:4], 1):  # ìµœëŒ€ 4ê°œ
                    content = doc.page_content.strip()
                    if len(content) > 500:
                        content = content[:500] + "..."
                    formatted_docs.append(f"[ë¬¸ì„œ {i}]\n{content}")
                
                return "\n\n".join(formatted_docs)
                
            except Exception as e:
                if self.enable_monitoring:
                    self.logger.error(f"Retrieval error: {e}")
                return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        
        # LCEL ì²´ì¸ êµ¬ì„±
        chain = (
            {
                "context": RunnableLambda(safe_retrieval),
                "question": RunnablePassthrough()
            }
            | self.prompt_template
            | self.llm
        )
        
        return chain
    
    def ask(self, 
            question: str,
            include_metadata: bool = False,
            timeout: float = 30.0) -> Dict[str, Any]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        ğŸ“¥ ì…ë ¥: ì§ˆë¬¸, ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€, íƒ€ì„ì•„ì›ƒ
        ğŸ“¤ ì¶œë ¥: ë‹µë³€ê³¼ ë©”íƒ€ë°ì´í„°
        """
        
        start_time = time.time()
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        if self.enable_monitoring:
            self.metrics["total_queries"] += 1
            self.logger.info(f"Processing query: {question[:50]}...")
        
        try:
            # ë¹„ìš© ì¶”ì 
            if self.enable_monitoring:
                with get_openai_callback() as cb:
                    response = self.chain.invoke(question)
                    cost = cb.total_cost
                    input_tokens = cb.prompt_tokens
                    output_tokens = cb.completion_tokens
            else:
                response = self.chain.invoke(question)
                cost = 0.0
                input_tokens = 0
                output_tokens = 0
            
            # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            execution_time = time.time() - start_time
            
            # ì„±ê³µ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            if self.enable_monitoring:
                self.metrics["successful_queries"] += 1
                self.metrics["total_cost"] += cost
                
                # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
                total_queries = self.metrics["total_queries"]
                current_avg = self.metrics["avg_response_time"]
                self.metrics["avg_response_time"] = (
                    (current_avg * (total_queries - 1) + execution_time) / total_queries
                )
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                "answer": response.content,
                "success": True,
                "execution_time": execution_time
            }
            
            if include_metadata:
                result.update({
                    "cost": cost,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "model": self.llm_config["model"],
                    "timestamp": time.time()
                })
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            if self.enable_monitoring:
                self.metrics["error_count"] += 1
                self.logger.error(f"Query failed: {e}")
            
            return {
                "error": str(e),
                "success": False,
                "execution_time": time.time() - start_time
            }
    
    def batch_ask(self, 
                  questions: List[str],
                  include_metadata: bool = False,
                  max_concurrent: int = 3) -> List[Dict[str, Any]]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ì—¬ëŸ¬ ì§ˆë¬¸ ë°°ì¹˜ ì²˜ë¦¬
        ğŸ“¥ ì…ë ¥: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€, ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
        ğŸ“¤ ì¶œë ¥: ê° ì§ˆë¬¸ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        
        results = []
        batch_start = time.time()
        
        if self.enable_monitoring:
            self.logger.info(f"Starting batch processing: {len(questions)} questions")
        
        for i, question in enumerate(questions, 1):
            if self.enable_monitoring:
                self.logger.info(f"Processing {i}/{len(questions)}: {question[:30]}...")
            
            result = self.ask(question, include_metadata=include_metadata)
            results.append(result)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if i % 5 == 0 or i == len(questions):
                success_count = sum(1 for r in results if r.get('success', False))
                print(f"ğŸ“Š ì§„í–‰: {i}/{len(questions)} (ì„±ê³µ: {success_count})")
        
        batch_time = time.time() - batch_start
        success_count = sum(1 for r in results if r.get('success', False))
        
        if self.enable_monitoring:
            self.logger.info(f"Batch completed: {success_count}/{len(questions)} successful, "
                           f"Total time: {batch_time:.2f}s")
        
        return results
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        if not self.enable_monitoring:
            return {"error": "Monitoring is disabled"}
        
        success_rate = (
            self.metrics["successful_queries"] / max(self.metrics["total_queries"], 1) * 100
        )
        
        return {
            "summary": {
                "total_queries": self.metrics["total_queries"],
                "successful_queries": self.metrics["successful_queries"],
                "success_rate": f"{success_rate:.1f}%",
                "total_cost": f"${self.metrics['total_cost']:.6f}",
                "avg_response_time": f"{self.metrics['avg_response_time']:.3f}s",
                "error_count": self.metrics["error_count"]
            },
            "configuration": {
                "llm_model": self.llm_config["model"],
                "temperature": self.llm_config["temperature"],
                "retriever_k": self.retriever_config["k"]
            },
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì„±ê³µë¥  ê¸°ë°˜
        success_rate = self.metrics["successful_queries"] / max(self.metrics["total_queries"], 1)
        if success_rate < 0.9:
            recommendations.append("ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤. ì—ëŸ¬ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë°˜
        if self.metrics["avg_response_time"] > 10.0:
            recommendations.append("ì‘ë‹µ ì‹œê°„ì´ ê¹ë‹ˆë‹¤. Retriever ì„¤ì •ì„ ìµœì í™”í•˜ì„¸ìš”.")
        
        # ë¹„ìš© ê¸°ë°˜
        if self.metrics["total_cost"] > 1.0:
            recommendations.append("ë¹„ìš©ì´ ë†’ìŠµë‹ˆë‹¤. ìºì‹±ì´ë‚˜ ë” ì €ë ´í•œ ëª¨ë¸ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì—ëŸ¬ìœ¨ ê¸°ë°˜
        error_rate = self.metrics["error_count"] / max(self.metrics["total_queries"], 1)
        if error_rate > 0.1:
            recommendations.append("ì—ëŸ¬ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ì…ë ¥ ê²€ì¦ì„ ê°•í™”í•˜ì„¸ìš”.")
        
        return recommendations or ["ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤."]
    
    def update_configuration(self, **kwargs):
        """ì„¤ì • ë™ì  ì—…ë°ì´íŠ¸"""
        
        updated = False
        
        # LLM ì„¤ì • ì—…ë°ì´íŠ¸
        if 'temperature' in kwargs:
            self.llm_config['temperature'] = kwargs['temperature']
            updated = True
        
        # Retriever ì„¤ì • ì—…ë°ì´íŠ¸
        if 'retriever_k' in kwargs:
            self.retriever_config['k'] = kwargs['retriever_k']
            updated = True
        
        if updated:
            # êµ¬ì„± ìš”ì†Œ ì¬ì´ˆê¸°í™”
            self._initialize_components()
            self.chain = self._build_chain()
            
            if self.enable_monitoring:
                self.logger.info(f"Configuration updated: {kwargs}")

# === í”„ë¡œë•ì…˜ LCEL Document GPT ì‚¬ìš© ì˜ˆì‹œ ===
print("\nğŸš€ í”„ë¡œë•ì…˜ LCEL Document GPT ì‹œìŠ¤í…œ:")
print("=" * 60)

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
prod_gpt = ProductionLCELDocumentGPT(
    vector_store=vector_store,
    llm_config={
        "model": "gpt-3.5-turbo",
        "temperature": 0.1
    },
    retriever_config={
        "k": 4,
        "score_threshold": 0.7
    },
    enable_monitoring=True
)

# ê°œë³„ ì§ˆì˜ í…ŒìŠ¤íŠ¸
test_questions = [
    "Winston Smithì˜ ì§ì—…ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "1984ë…„ ì†Œì„¤ì˜ ë°°ê²½ì€ ì–´ë””ì¸ê°€ìš”?",
    "Big BrotherëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    "ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤."  # ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
]

print("\nğŸ“ ê°œë³„ ì§ˆì˜ í…ŒìŠ¤íŠ¸:")
for i, question in enumerate(test_questions, 1):
    print(f"\nğŸ“‹ ì§ˆì˜ {i}: {question}")
    
    result = prod_gpt.ask(question, include_metadata=True)
    
    if result['success']:
        print(f"ğŸ¤– ë‹µë³€: {result['answer'][:100]}...")
        print(f"â±ï¸ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
        print(f"ğŸ’° ë¹„ìš©: ${result['cost']:.6f}")
    else:
        print(f"âŒ ì˜¤ë¥˜: {result['error']}")

# ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
print(f"\nğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸:")
batch_questions = test_questions[:3]  # ì²˜ìŒ 3ê°œë§Œ
batch_results = prod_gpt.batch_ask(batch_questions, include_metadata=True)

# ì„±ëŠ¥ ë¦¬í¬íŠ¸
print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¦¬í¬íŠ¸:")
report = prod_gpt.get_performance_report()

print(f"ğŸ“Š ìš”ì•½:")
for key, value in report['summary'].items():
    print(f"   {key}: {value}")

print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
for rec in report['recommendations']:
    print(f"   - {rec}")

print("\nâœ… í”„ë¡œë•ì…˜ LCEL Document GPT í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
```

## ğŸ” ë³€ìˆ˜/í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…

### LCEL í•µì‹¬ í•¨ìˆ˜ë“¤

#### RunnablePassthrough í™œìš© íŒ¨í„´
```python
from langchain.schema.runnable import RunnablePassthrough

# ê¸°ë³¸ ì‚¬ìš©ë²•
def basic_passthrough_usage():
    """RunnablePassthrough ê¸°ë³¸ í™œìš©"""
    
    # ë‹¨ìˆœ ì „ë‹¬
    passthrough = RunnablePassthrough()
    result = passthrough.invoke("Hello World")
    # result == "Hello World"
    
    # ì²´ì¸ì—ì„œ í™œìš©
    chain = {
        "original": RunnablePassthrough(),
        "processed": RunnableLambda(lambda x: x.upper())
    }
    
    result = chain.invoke("hello")
    # result == {"original": "hello", "processed": "HELLO"}

def advanced_passthrough_patterns():
    """ê³ ê¸‰ RunnablePassthrough íŒ¨í„´"""
    
    # ë³€í™˜ê³¼ í•¨ê»˜ ì‚¬ìš©
    def add_metadata(text: str) -> Dict[str, str]:
        return {
            "content": text,
            "length": len(text),
            "timestamp": time.time()
        }
    
    # ë³µí•© ì²´ì¸
    complex_chain = {
        "raw_input": RunnablePassthrough(),
        "metadata": RunnableLambda(add_metadata),
        "processed": RunnableLambda(lambda x: x.strip().title())
    }
    
    return complex_chain

# ì¡°ê±´ë¶€ Passthrough
def conditional_passthrough(condition_func):
    """ì¡°ê±´ì— ë”°ë¥¸ ì„ íƒì  ì „ë‹¬"""
    
    def conditional_invoke(input_data):
        if condition_func(input_data):
            return input_data
        else:
            return None  # ë˜ëŠ” ê¸°ë³¸ê°’
    
    return RunnableLambda(conditional_invoke)
```

#### ì²´ì¸ ë””ë²„ê¹… ë° ëª¨ë‹ˆí„°ë§
```python
from langchain.callbacks import BaseCallbackHandler
import json

class LCELDebugCallback(BaseCallbackHandler):
    """LCEL ì²´ì¸ ì‹¤í–‰ ê³¼ì • ë””ë²„ê¹…"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.execution_log = []
    
    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs):
        if self.verbose:
            print(f"ğŸ”— ì²´ì¸ ì‹œì‘: {serialized.get('name', 'Unknown')}")
            print(f"ğŸ“¥ ì…ë ¥: {str(inputs)[:100]}...")
        
        self.execution_log.append({
            "type": "chain_start",
            "name": serialized.get('name'),
            "inputs": inputs,
            "timestamp": time.time()
        })
    
    def on_chain_end(self, outputs: Dict[str, Any], **kwargs):
        if self.verbose:
            print(f"âœ… ì²´ì¸ ì™„ë£Œ")
            print(f"ğŸ“¤ ì¶œë ¥: {str(outputs)[:100]}...")
        
        self.execution_log.append({
            "type": "chain_end",
            "outputs": outputs,
            "timestamp": time.time()
        })
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):
        if self.verbose:
            print(f"ğŸ¤– LLM í˜¸ì¶œ: {serialized.get('name')}")
            print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}")
    
    def on_llm_end(self, response, **kwargs):
        if self.verbose:
            print(f"ğŸ¤– LLM ì™„ë£Œ: {len(response.generations)} ì‘ë‹µ")
    
    def get_execution_summary(self) -> Dict[str, Any]:
        """ì‹¤í–‰ ìš”ì•½ ìƒì„±"""
        
        if not self.execution_log:
            return {"message": "ì‹¤í–‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}
        
        start_time = self.execution_log[0]["timestamp"]
        end_time = self.execution_log[-1]["timestamp"]
        total_time = end_time - start_time
        
        chain_steps = len([log for log in self.execution_log if log["type"] == "chain_start"])
        
        return {
            "total_execution_time": f"{total_time:.3f}ì´ˆ",
            "chain_steps": chain_steps,
            "start_time": time.ctime(start_time),
            "end_time": time.ctime(end_time),
            "detailed_log": self.execution_log
        }

def create_debuggable_chain(retriever, llm) -> tuple:
    """ë””ë²„ê¹… ê°€ëŠ¥í•œ ì²´ì¸ê³¼ ì½œë°± ìƒì„±"""
    
    debug_callback = LCELDebugCallback(verbose=True)
    
    # ë””ë²„ê¹…ì´ í¬í•¨ëœ ì²´ì¸
    chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | ChatPromptTemplate.from_messages([
            ("system", "Context: {context}"),
            ("human", "{question}")
        ])
        | llm
    )
    
    return chain, debug_callback

# ì‚¬ìš© ì˜ˆì‹œ
debuggable_chain, debug_cb = create_debuggable_chain(retriever, llm)

# ë””ë²„ê·¸ ëª¨ë“œë¡œ ì‹¤í–‰
result = debuggable_chain.invoke(
    "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸", 
    config={"callbacks": [debug_cb]}
)

# ì‹¤í–‰ ìš”ì•½ í™•ì¸
summary = debug_cb.get_execution_summary()
print(json.dumps(summary, indent=2, ensure_ascii=False))
```

### ì„±ëŠ¥ ìµœì í™” ë„êµ¬
```python
def benchmark_lcel_vs_traditional(retriever, llm, test_queries: List[str]) -> Dict[str, Any]:
    """
    ğŸ“‹ ê¸°ëŠ¥: LCEL vs ì „í†µì  ì²´ì¸ ì„±ëŠ¥ ë¹„êµ
    ğŸ“¥ ì…ë ¥: ê²€ìƒ‰ê¸°, LLM, í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    ğŸ“¤ ì¶œë ¥: ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸
    """
    
    # LCEL ì²´ì¸
    lcel_chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | ChatPromptTemplate.from_messages([
            ("system", "Context: {context}"),
            ("human", "{question}")
        ])
        | llm
    )
    
    # ì „í†µì  RetrievalQA ì²´ì¸
    traditional_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever
    )
    
    results = {
        "lcel": {"times": [], "success": 0, "errors": []},
        "traditional": {"times": [], "success": 0, "errors": []}
    }
    
    # LCEL í…ŒìŠ¤íŠ¸
    print("ğŸ”— LCEL ì²´ì¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    for query in test_queries:
        try:
            start = time.time()
            lcel_chain.invoke(query)
            results["lcel"]["times"].append(time.time() - start)
            results["lcel"]["success"] += 1
        except Exception as e:
            results["lcel"]["errors"].append(str(e))
    
    # ì „í†µì  ì²´ì¸ í…ŒìŠ¤íŠ¸
    print("ğŸ”— ì „í†µì  ì²´ì¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
    for query in test_queries:
        try:
            start = time.time()
            traditional_chain.run(query)
            results["traditional"]["times"].append(time.time() - start)
            results["traditional"]["success"] += 1
        except Exception as e:
            results["traditional"]["errors"].append(str(e))
    
    # ê²°ê³¼ ë¶„ì„
    lcel_avg = sum(results["lcel"]["times"]) / max(len(results["lcel"]["times"]), 1)
    traditional_avg = sum(results["traditional"]["times"]) / max(len(results["traditional"]["times"]), 1)
    
    return {
        "query_count": len(test_queries),
        "lcel_performance": {
            "avg_time": f"{lcel_avg:.3f}ì´ˆ",
            "success_rate": f"{results['lcel']['success'] / len(test_queries) * 100:.1f}%",
            "error_count": len(results["lcel"]["errors"])
        },
        "traditional_performance": {
            "avg_time": f"{traditional_avg:.3f}ì´ˆ", 
            "success_rate": f"{results['traditional']['success'] / len(test_queries) * 100:.1f}%",
            "error_count": len(results["traditional"]["errors"])
        },
        "comparison": {
            "speed_difference": f"{'LCELì´' if lcel_avg < traditional_avg else 'ì „í†µì  ë°©ì‹ì´'} "
                              f"{abs(lcel_avg - traditional_avg):.3f}ì´ˆ ë¹ ë¦„",
            "winner": "LCEL" if lcel_avg < traditional_avg else "Traditional"
        }
    }
```

## ğŸ§ª ì‹¤ìŠµ ê³¼ì œ

### ğŸ”¨ ê¸°ë³¸ ê³¼ì œ
1. **ì»¤ìŠ¤í…€ Runnable ê°œë°œ**: íŠ¹í™”ëœ ê¸°ëŠ¥ì„ ê°€ì§„ ì»¤ìŠ¤í…€ Runnable í´ë˜ìŠ¤ êµ¬í˜„
```python
# TODO: ì§ˆë¬¸ ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì»¤ìŠ¤í…€ Runnable êµ¬í˜„
class QuestionPreprocessor(Runnable):
    def invoke(self, input_data):
        # ì§ˆë¬¸ ì •ì œ, ì˜¤íƒ€ ìˆ˜ì •, í˜•ì‹ í†µì¼ ë“±
        pass
```

2. **ì¡°ê±´ë¶€ ì²´ì¸**: ì…ë ¥ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬ ê²½ë¡œë¥¼ ì„ íƒí•˜ëŠ” ë™ì  ì²´ì¸ êµ¬í˜„
```python
# TODO: ì§ˆë¬¸ ë³µì¡ë„ì— ë”°ë¼ ë‹¤ë¥¸ ê²€ìƒ‰ ì „ëµì„ ì„ íƒí•˜ëŠ” ì²´ì¸
def create_adaptive_chain(simple_retriever, complex_retriever, llm):
    # ì§ˆë¬¸ ë¶„ì„ í›„ ì ì ˆí•œ retriever ì„ íƒ
    pass
```

### ğŸš€ ì‹¬í™” ê³¼ì œ
3. **ìŠ¤íŠ¸ë¦¬ë° LCEL ì²´ì¸**: ì‹¤ì‹œê°„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°ì´ ê°€ëŠ¥í•œ ì²´ì¸ êµ¬í˜„
```python
# TODO: í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” LCEL ì²´ì¸
async def create_streaming_chain():
    # ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥í•œ ì²´ì¸ êµ¬ì„±
    pass
```

4. **ë©€í‹°ëª¨ë‹¬ LCEL ì²´ì¸**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” ì²´ì¸ êµ¬í˜„
```python
# TODO: ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ í™œìš©í•˜ëŠ” ë©€í‹°ëª¨ë‹¬ ì²´ì¸
class MultimodalDocumentChain:
    def process_text_and_images(self, query, text_docs, image_docs):
        pass
```

### ğŸ’¡ ì°½ì˜ ê³¼ì œ
5. **ìê°€ ê°œì„  ì²´ì¸**: ì‚¬ìš©ì í”¼ë“œë°±ì„ í•™ìŠµí•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ì²´ì¸
```python
# TODO: ì‚¬ìš©ì í‰ì ì„ ìˆ˜ì§‘í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë‚˜ ê²€ìƒ‰ ì „ëµì„ ìë™ ê°œì„ 
class SelfImprovingChain:
    def learn_from_feedback(self, query, answer, user_rating):
        pass
```

6. **ì²´ì¸ ì•™ìƒë¸”**: ì—¬ëŸ¬ LCEL ì²´ì¸ì˜ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ëŠ” ì•™ìƒë¸” ì‹œìŠ¤í…œ
```python
# TODO: ë‹¤ì–‘í•œ ì „ëµì˜ ì²´ì¸ ê²°ê³¼ë¥¼ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
class ChainEnsemble:
    def combine_chain_outputs(self, outputs, weights):
        pass
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### LCEL ì„¤ê³„ ì›ì¹™
```python
# âœ… ì˜¬ë°”ë¥¸ LCEL ì²´ì¸ ì„¤ê³„
def good_lcel_chain():
    return (
        {"context": retriever, "question": RunnablePassthrough()}  # ëª…í™•í•œ êµ¬ì¡°
        | prompt_template                                           # ë‹¨ê³„ë³„ ë¶„ë¦¬
        | llm                                                      # ì²´ì¸ì˜ ë§ˆì§€ë§‰
    )

# âŒ ì˜ëª»ëœ LCEL ì²´ì¸ ì„¤ê³„
def bad_lcel_chain():
    # ë³µì¡í•˜ê³  ì´í•´í•˜ê¸° ì–´ë ¤ìš´ ì¤‘ì²© êµ¬ì¡°
    return (
        {
            "nested": {
                "context": retriever,
                "meta": {"question": RunnablePassthrough()}
            }
        }
        | complex_transform
        | another_transform
        | prompt_template
        | llm
    )
```

### ì—ëŸ¬ ì²˜ë¦¬ íŒ¨í„´
- **Graceful Degradation**: êµ¬ì„± ìš”ì†Œ ì‹¤íŒ¨ì‹œ ë¶€ë¶„ì  ê¸°ëŠ¥ ì œê³µ
- **ëª…ì‹œì  ì—ëŸ¬ ì²˜ë¦¬**: ê° ë‹¨ê³„ë³„ ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬
- **ì‚¬ìš©ì ì¹œí™”ì  ë©”ì‹œì§€**: ê¸°ìˆ ì  ì˜¤ë¥˜ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ë³€í™˜

### ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­
```python
# ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œë¼ì¸
performance_tips = {
    "ë³‘ë ¬í™”": "ë…ë¦½ì ì¸ ì‘ì—…ì€ RunnableParallel ì‚¬ìš©",
    "ìºì‹±": "ë°˜ë³µë˜ëŠ” ì—°ì‚° ê²°ê³¼ ìºì‹±",
    "ë°°ì¹˜ì²˜ë¦¬": "ì—¬ëŸ¬ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬",
    "ì§€ì—°ë¡œë”©": "í•„ìš”í•œ ì‹œì ì—ë§Œ ë¦¬ì†ŒìŠ¤ ë¡œë”©",
    "ìŠ¤íŠ¸ë¦¬ë°": "ê¸´ ì‘ë‹µì˜ ê²½ìš° ìŠ¤íŠ¸ë¦¬ë° ì ìš©"
}
```

### ë””ë²„ê¹… ì „ëµ
- **ë‹¨ê³„ë³„ ê²€ì¦**: ê° êµ¬ì„± ìš”ì†Œì˜ ì…ì¶œë ¥ í™•ì¸
- **ë¡œê¹… í™œìš©**: ì‹¤í–‰ ê³¼ì • ìƒì„¸ ê¸°ë¡
- **ì½œë°± í™œìš©**: ì¤‘ê°„ ë‹¨ê³„ ëª¨ë‹ˆí„°ë§
- **í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ê²€ì¦

## ğŸ”— ê´€ë ¨ ìë£Œ
- **ì´ì „ í•™ìŠµ**: [6.6 RetrievalQA](./6.6_RetrievalQA.md)
- **ë‹¤ìŒ í•™ìŠµ**: [6.9 Map Reduce LCEL Chain](./6.9_Map_Reduce_LCEL_Chain.md)
- **ì°¸ê³  ë¬¸ì„œ**: [LCEL Documentation](https://python.langchain.com/docs/expression_language/)
- **ê³ ê¸‰ íŒ¨í„´**: [LCEL Cookbook](https://python.langchain.com/docs/expression_language/cookbook/)
- **ì‹¤ìŠµ íŒŒì¼**: [6.8 Stuff LCEL Chain.ipynb](../../00%20lecture/6.8%20Stuff%20LCEL%20Chain.ipynb)

---

ğŸ’¡ **í•µì‹¬ ì •ë¦¬**: LCEL(LangChain Expression Language)ì€ íˆ¬ëª…í•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥í•œ ì²´ì¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. RunnablePassthroughì™€ RunnableParallelì„ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì´ê³  ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥í•œ ì²´ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ì˜ "magical" chainë³´ë‹¤ ë” ë‚˜ì€ ì œì–´ì™€ ë””ë²„ê¹… ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì—ëŸ¬ ì²˜ë¦¬, ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ì„¤ì • ê´€ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.