# ğŸ“– Section 6.2: Tiktoken - ì •í™•í•œ í† í° ê³„ì‚°ì˜ í•µì‹¬

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- âœ… í† í°(Token)ê³¼ ë¬¸ì(Character)ì˜ ì°¨ì´ì  ì™„ì „ ì´í•´
- âœ… OpenAI ëª¨ë¸ì´ ì‹¤ì œë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ í•™ìŠµ
- âœ… Tiktokenì„ í™œìš©í•œ ì •í™•í•œ í† í° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í•  êµ¬í˜„
- âœ… ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•œ í† í° ê³„ì‚° ì „ëµ ìŠµë“

## ğŸ§  í•µì‹¬ ê°œë…

### í† í°(Token)ì´ë€?
**í† í°**ì€ LLMì´ ì‹¤ì œë¡œ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” í…ìŠ¤íŠ¸ì˜ ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤. ë¬¸ìë‚˜ ë‹¨ì–´ì™€ëŠ” ë‹¤ë¥¸ ë…íŠ¹í•œ íŠ¹ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.

```mermaid
graph LR
    A["Hello everyone my name is Nicolas"] --> B[Tokenizer]
    B --> C["[Hello] [everyone] [my] [name] [is] [Nic] [olas]"]
    C --> D["Token IDs: [15496, 5390, 856, 836, 374, 83803, 627]"]
    
    style A fill:#E6F3FF
    style C fill:#FFE6CC  
    style D fill:#E6FFE6
```

### ë¬¸ì vs í† í° ë¹„êµ

| íŠ¹ì„± | ë¬¸ì(Character) | í† í°(Token) |
|------|-----------------|-------------|
| **ê³„ì‚° ë°©ì‹** | `len("Hello")` = 5 | `encode("Hello")` = 1 í† í° |
| **ì–¸ì–´ë³„ ì°¨ì´** | í•œê¸€ 1ì = 1ë¬¸ì | í•œê¸€ 1ì = ë³´í†µ 2-3 í† í° |
| **ì²˜ë¦¬ ë‹¨ìœ„** | ê°œë³„ ë¬¸ì | ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ ì¡°ê° |
| **ë¹„ìš© ê¸°ì¤€** | ì‚¬ìš©í•˜ì§€ ì•ŠìŒ | OpenAI ìš”ê¸ˆ ê¸°ì¤€ |

### í† í°í™” ì˜ˆì‹œ ë¶„ì„
```python
# ğŸ§  ê°œë…: ë™ì¼í•œ í…ìŠ¤íŠ¸ì˜ ë¬¸ì ìˆ˜ vs í† í° ìˆ˜ ë¹„êµ
text = "Hello everyone my name is Nicolas nice to meet you"

# ë¬¸ì ìˆ˜ ê³„ì‚°
char_count = len(text)  # ê²°ê³¼: 49 ë¬¸ì

# í† í° ìˆ˜ ê³„ì‚° (OpenAI ê¸°ì¤€)
# "Hello" â†’ 1 í† í°
# " everyone" â†’ 1 í† í° (ê³µë°± í¬í•¨)
# " my" â†’ 1 í† í°
# " name" â†’ 1 í† í°
# " is" â†’ 1 í† í°  
# " Nicolas" â†’ 1 í† í°ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” " Nic" + "olas" = 2 í† í°
# ì´ 13 í† í°
```

## ğŸ“‹ ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤

### Tiktoken ê¸°ë³¸ ì‚¬ìš©ë²•
```python
import tiktoken

# OpenAI ëª¨ë¸ë³„ ì¸ì½”ë” ê°€ì ¸ì˜¤ê¸°
encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")
# ğŸ“Œ ì§€ì› ëª¨ë¸: "gpt-4", "gpt-3.5-turbo", "text-davinci-003" ë“±

# ë˜ëŠ” ì¸ì½”ë”© ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
encoder = tiktoken.get_encoding("cl100k_base")  
# ğŸ“Œ gpt-3.5-turboì™€ gpt-4ê°€ ì‚¬ìš©í•˜ëŠ” ì¸ì½”ë”©
```

### í† í° ì¸ì½”ë”©/ë””ì½”ë”©
```python
class Encoding:
    def encode(self, text: str) -> List[int]:
        """
        ğŸ“‹ ê¸°ëŠ¥: í…ìŠ¤íŠ¸ë¥¼ í† í° ID ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        ğŸ“¥ ì…ë ¥: ë¬¸ìì—´ í…ìŠ¤íŠ¸
        ğŸ“¤ ì¶œë ¥: í† í° ID ë¦¬ìŠ¤íŠ¸
        """
    
    def decode(self, tokens: List[int]) -> str:
        """
        ğŸ“‹ ê¸°ëŠ¥: í† í° ID ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³µì›
        ğŸ“¥ ì…ë ¥: í† í° ID ë¦¬ìŠ¤íŠ¸  
        ğŸ“¤ ì¶œë ¥: ì›ë³¸ ë¬¸ìì—´
        """
    
    def encode_batch(self, texts: List[str]) -> List[List[int]]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì¸ì½”ë”© (íš¨ìœ¨ì )
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œ
        """
```

### LangChain í†µí•©
```python
from langchain.text_splitter import CharacterTextSplitter

# í† í° ê¸°ë°˜ ë¶„í• ê¸° ìƒì„±
splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n",           # ğŸ“Œ ìš©ë„: ë¶„í•  ê¸°ì¤€ ë¬¸ì
    chunk_size=600,          # ğŸ“Œ ìš©ë„: ìµœëŒ€ í† í° ìˆ˜ (ë¬¸ì ìˆ˜ ì•„ë‹˜!)
    chunk_overlap=100,       # ğŸ“Œ ìš©ë„: ê²¹ì¹˜ëŠ” í† í° ìˆ˜
    model_name="gpt-3.5-turbo"  # ğŸ“Œ ìš©ë„: í† í°í™” ê¸°ì¤€ ëª¨ë¸
)
```

## ğŸ”§ ë™ì‘ ê³¼ì • ìƒì„¸

### 1ë‹¨ê³„: ê¸°ë³¸ í† í°í™” ì´í•´
```python
import tiktoken

# === OpenAI í† í°í™” ì‹¤ìŠµ ===
# ğŸ§  ê°œë…: ì‹¤ì œ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì–´ë–»ê²Œ ì¸ì‹í•˜ëŠ”ì§€ í™•ì¸

encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")

# í•œêµ­ì–´ì™€ ì˜ì–´ì˜ í† í°í™” ì°¨ì´
texts = [
    "Hello world!",
    "ì•ˆë…•í•˜ì„¸ìš”!",  
    "Hello everyone my name is Nicolas",
    "ì•ˆë…•í•˜ì„¸ìš” ì—¬ëŸ¬ë¶„ ì œ ì´ë¦„ì€ ë‹ˆì½œë¼ìŠ¤ì…ë‹ˆë‹¤"
]

print("ğŸ” í† í°í™” ë¶„ì„ ê²°ê³¼:")
print("-" * 60)

for text in texts:
    # ë¬¸ì ìˆ˜ ê³„ì‚°
    char_count = len(text)
    
    # í† í° ì¸ì½”ë”©
    tokens = encoder.encode(text)
    token_count = len(tokens)
    
    # íš¨ìœ¨ì„± ë¹„ìœ¨ ê³„ì‚°  
    efficiency = char_count / token_count
    
    print(f"í…ìŠ¤íŠ¸: {text}")
    print(f"ğŸ“Š ë¬¸ì ìˆ˜: {char_count}")
    print(f"ğŸ“Š í† í° ìˆ˜: {token_count}")
    print(f"ğŸ“Š í† í° íš¨ìœ¨ì„±: {efficiency:.2f} (ë¬¸ì/í† í°)")
    print(f"ğŸ“Š í† í° IDs: {tokens[:10]}..." if len(tokens) > 10 else f"ğŸ“Š í† í° IDs: {tokens}")
    print("-" * 60)
```

### 2ë‹¨ê³„: í† í° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• 
```python
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter

# === ê¸°ì¡´ ë¬¸ì ê¸°ë°˜ vs í† í° ê¸°ë°˜ ë¶„í•  ë¹„êµ ===

# ğŸ“Œ ë¬¸ì„œ ë¡œë”©
loader = UnstructuredFileLoader("./files/chapter_one.docx")
docs = loader.load()

print("ğŸ“ˆ ë¶„í•  ë°©ì‹ ë¹„êµ ë¶„ì„:")
print("=" * 80)

# ğŸ”´ ê¸°ì¡´ ë°©ì‹: ë¬¸ì ê¸°ë°˜ ë¶„í• 
char_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=600,      # ğŸ“Œ 600 ë¬¸ì
    chunk_overlap=100    # ğŸ“Œ 100 ë¬¸ì ê²¹ì¹¨
)

char_chunks = char_splitter.split_documents(docs)

# ğŸŸ¢ ê°œì„ ëœ ë°©ì‹: í† í° ê¸°ë°˜ ë¶„í• 
token_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n", 
    chunk_size=600,      # ğŸ“Œ 600 í† í° (ì‹¤ì œ ëª¨ë¸ ê¸°ì¤€!)
    chunk_overlap=100,   # ğŸ“Œ 100 í† í° ê²¹ì¹¨
    model_name="gpt-3.5-turbo"
)

token_chunks = token_splitter.split_documents(docs)

# ê²°ê³¼ ë¹„êµ
print(f"ğŸ“Š ë¬¸ì ê¸°ë°˜ ë¶„í• : {len(char_chunks)}ê°œ ì²­í¬")
print(f"ğŸ“Š í† í° ê¸°ë°˜ ë¶„í• : {len(token_chunks)}ê°œ ì²­í¬") 
print(f"ğŸ“Š ì°¨ì´: {abs(len(char_chunks) - len(token_chunks))}ê°œ ì²­í¬")
```

### 3ë‹¨ê³„: ì •í™•í•œ ë¹„ìš© ê³„ì‚°
```python
import tiktoken

def calculate_embedding_cost(documents, model="text-embedding-ada-002"):
    """
    ğŸ“‹ ê¸°ëŠ¥: ì„ë² ë”© ìƒì„± ë¹„ìš©ì„ ì •í™•í•˜ê²Œ ê³„ì‚°
    ğŸ“¥ ì…ë ¥: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ì„ë² ë”© ëª¨ë¸ëª…
    ğŸ“¤ ì¶œë ¥: ì˜ˆìƒ ë¹„ìš© (USD)
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì „ ë¹„ìš© ì˜ˆì¸¡
    """
    
    # ëª¨ë¸ë³„ í† í°í™” ì¸ì½”ë”
    encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")  # ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©
    
    total_tokens = 0
    
    for doc in documents:
        tokens = encoder.encode(doc.page_content)
        total_tokens += len(tokens)
    
    # OpenAI ì„ë² ë”© ê°€ê²© (2024ë…„ ê¸°ì¤€)
    # text-embedding-ada-002: $0.0001 per 1K tokens
    cost_per_1k_tokens = 0.0001
    estimated_cost = (total_tokens / 1000) * cost_per_1k_tokens
    
    print(f"ğŸ’° ë¹„ìš© ê³„ì‚° ê²°ê³¼:")
    print(f"   ğŸ“Š ì´ í† í° ìˆ˜: {total_tokens:,}")
    print(f"   ğŸ’µ ì˜ˆìƒ ì„ë² ë”© ë¹„ìš©: ${estimated_cost:.6f}")
    print(f"   ğŸ“ˆ 1ë§Œ í† í°ë‹¹ ë¹„ìš©: ${cost_per_1k_tokens * 10:.4f}")
    
    return estimated_cost

# ì‚¬ìš© ì˜ˆì‹œ
estimated_cost = calculate_embedding_cost(token_chunks)
```

## ğŸ’» ì‹¤ì „ ì˜ˆì œ

### í† í° íš¨ìœ¨ì„± ìµœì í™” ì‹œìŠ¤í…œ
```python
import tiktoken
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import UnstructuredFileLoader
from typing import List, Dict, Any

class TokenOptimizedDocumentProcessor:
    """
    ğŸ¯ í† í° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë¬¸ì„œ ì²˜ë¦¬ê¸°
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ì •í™•í•œ í† í° ê¸°ë°˜ ë¶„í• 
    - ë¹„ìš© ì˜ˆì¸¡ ë° ìµœì í™”  
    - ë‹¤ì–‘í•œ ì²­í¬ í¬ê¸° ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
    """
    
    def __init__(self, model_name: str = "gpt-3.5-turbo"):
        self.model_name = model_name
        self.encoder = tiktoken.encoding_for_model(model_name)
        
        # OpenAI ëª¨ë¸ë³„ í† í° ì œí•œ
        self.token_limits = {
            "gpt-3.5-turbo": 4096,
            "gpt-4": 8192, 
            "gpt-4-32k": 32768
        }
    
    def analyze_text_complexity(self, text: str) -> Dict[str, Any]:
        """
        ğŸ“‹ ê¸°ëŠ¥: í…ìŠ¤íŠ¸ì˜ í† í° ë³µì¡ë„ ë¶„ì„
        ğŸ“Š ì§€í‘œ: í† í° íš¨ìœ¨ì„±, ì••ì¶•ë¥ , ë‹¤ì–‘ì„± ë“±
        """
        
        char_count = len(text)
        tokens = self.encoder.encode(text)
        token_count = len(tokens)
        
        # í† í° íš¨ìœ¨ì„± (ë†’ì„ìˆ˜ë¡ í† í° ì ˆì•½)
        efficiency = char_count / token_count if token_count > 0 else 0
        
        # í† í° ë‹¤ì–‘ì„± (ê³ ìœ  í† í° ë¹„ìœ¨)
        unique_tokens = len(set(tokens))
        diversity = unique_tokens / token_count if token_count > 0 else 0
        
        return {
            "character_count": char_count,
            "token_count": token_count,
            "efficiency_ratio": efficiency,
            "diversity_ratio": diversity,
            "compression_ratio": token_count / char_count if char_count > 0 else 0
        }
    
    def find_optimal_chunk_size(self, documents: List, target_count: int = None) -> Dict[str, Any]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ìµœì ì˜ ì²­í¬ í¬ê¸° ìë™ ê³„ì‚°
        ğŸ’¡ ì „ëµ: í† í° ì œí•œ, ë¹„ìš©, ê²€ìƒ‰ íš¨ìœ¨ì„±ì„ ì¢…í•© ê³ ë ¤
        """
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ë¶„ì„
        full_text = "\n".join([doc.page_content for doc in documents])
        analysis = self.analyze_text_complexity(full_text)
        
        total_tokens = analysis["token_count"]
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì²­í¬ í¬ê¸° í›„ë³´
        candidates = [200, 400, 600, 800, 1000, 1500]
        
        best_scenario = None
        best_score = 0
        
        results = []
        
        for chunk_size in candidates:
            # ì˜ˆìƒ ì²­í¬ ìˆ˜
            estimated_chunks = total_tokens // chunk_size
            
            # ëª¨ë¸ í† í° ì œí•œ ë‚´ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì²­í¬ ìˆ˜
            model_limit = self.token_limits.get(self.model_name, 4096)
            max_context_chunks = (model_limit - 500) // chunk_size  # 500í† í°ì€ ì§ˆë¬¸+ì‹œìŠ¤í…œë©”ì‹œì§€ìš©
            
            # ìŠ¤ì½”ì–´ ê³„ì‚° (ì—¬ëŸ¬ ìš”ì†Œ ì¢…í•©)
            size_score = min(chunk_size / 600, 1.0)  # 600í† í°ì„ ìµœì ìœ¼ë¡œ ê°€ì •
            efficiency_score = min(estimated_chunks / 50, 1.0)  # 50ê°œ ì²­í¬ë¥¼ ê¸°ì¤€
            context_score = min(max_context_chunks / 5, 1.0)  # 5ê°œ ì²­í¬ ë™ì‹œ ì‚¬ìš© ê°€ëŠ¥
            
            total_score = (size_score + efficiency_score + context_score) / 3
            
            scenario = {
                "chunk_size": chunk_size,
                "estimated_chunks": estimated_chunks,
                "max_context_chunks": max_context_chunks,
                "total_score": total_score
            }
            
            results.append(scenario)
            
            if total_score > best_score:
                best_score = total_score
                best_scenario = scenario
        
        return {
            "recommended": best_scenario,
            "all_scenarios": results,
            "text_analysis": analysis
        }
    
    def create_optimized_splitter(self, chunk_size: int = None) -> CharacterTextSplitter:
        """
        ğŸ“‹ ê¸°ëŠ¥: ìµœì í™”ëœ í† í° ê¸°ë°˜ ë¶„í• ê¸° ìƒì„±
        """
        
        if chunk_size is None:
            chunk_size = 600  # ê¸°ë³¸ê°’
        
        # ê²¹ì¹¨ í¬ê¸°ëŠ” ì²­í¬ í¬ê¸°ì˜ 15% ì •ë„ê°€ ì ì ˆ
        chunk_overlap = min(chunk_size // 7, 150)
        
        return CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n",
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            model_name=self.model_name
        )
    
    def process_document_with_optimization(self, file_path: str) -> Dict[str, Any]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ìµœì í™” í¬í•¨)
        """
        
        print(f"ğŸš€ í† í° ìµœì í™” ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path}")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”©
        loader = UnstructuredFileLoader(file_path)
        docs = loader.load()
        
        # 2ë‹¨ê³„: ìµœì  ì²­í¬ í¬ê¸° ë¶„ì„
        optimization_result = self.find_optimal_chunk_size(docs)
        recommended_size = optimization_result["recommended"]["chunk_size"]
        
        print(f"ğŸ“Š ìµœì í™” ë¶„ì„ ê²°ê³¼:")
        print(f"   ğŸ¯ ê¶Œì¥ ì²­í¬ í¬ê¸°: {recommended_size} í† í°")
        print(f"   ğŸ“ˆ ì˜ˆìƒ ì²­í¬ ìˆ˜: {optimization_result['recommended']['estimated_chunks']}ê°œ")
        print(f"   ğŸ”„ ë™ì‹œ ì»¨í…ìŠ¤íŠ¸ ê°€ëŠ¥: {optimization_result['recommended']['max_context_chunks']}ê°œ")
        
        # 3ë‹¨ê³„: ìµœì í™”ëœ ë¶„í• ê¸°ë¡œ ì²˜ë¦¬
        optimized_splitter = self.create_optimized_splitter(recommended_size)
        final_chunks = optimized_splitter.split_documents(docs)
        
        # 4ë‹¨ê³„: ë¹„ìš© ê³„ì‚°
        total_tokens = sum(len(self.encoder.encode(chunk.page_content)) for chunk in final_chunks)
        embedding_cost = (total_tokens / 1000) * 0.0001  # text-embedding-ada-002 ê¸°ì¤€
        
        print(f"\nğŸ’° ë¹„ìš© ë¶„ì„:")
        print(f"   ğŸ“Š ì´ í† í° ìˆ˜: {total_tokens:,}")
        print(f"   ğŸ’µ ì„ë² ë”© ë¹„ìš©: ${embedding_cost:.6f}")
        print(f"   ğŸ“ˆ ì²­í¬ë‹¹ í‰ê·  í† í°: {total_tokens // len(final_chunks)}")
        
        return {
            "chunks": final_chunks,
            "optimization_analysis": optimization_result,
            "cost_analysis": {
                "total_tokens": total_tokens,
                "embedding_cost": embedding_cost,
                "chunks_count": len(final_chunks)
            }
        }

# === ì‚¬ìš© ì˜ˆì‹œ ===
processor = TokenOptimizedDocumentProcessor("gpt-3.5-turbo")
result = processor.process_document_with_optimization("./files/chapter_one.docx")

print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
print(f"ğŸ“¦ ìµœì¢… ì²­í¬ ìˆ˜: {len(result['chunks'])}")
print(f"ğŸ’° ì˜ˆìƒ ì„ë² ë”© ë¹„ìš©: ${result['cost_analysis']['embedding_cost']:.6f}")
```

## ğŸ” ë³€ìˆ˜/í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…

### í† í° ê³„ì‚° ìµœì í™” í•¨ìˆ˜ë“¤
```python
def count_tokens_efficiently(texts: List[str], model: str = "gpt-3.5-turbo") -> List[int]:
    """
    ğŸ“‹ ê¸°ëŠ¥: ëŒ€ëŸ‰ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°
    ğŸ“¥ ì…ë ¥: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸, ëª¨ë¸ëª…
    ğŸ“¤ ì¶œë ¥: ê° í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ë¦¬ìŠ¤íŠ¸
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ìˆ˜ì²œ ê°œì˜ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•  ë•Œ
    """
    encoder = tiktoken.encoding_for_model(model)
    
    # ë°°ì¹˜ ì¸ì½”ë”©ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
    token_counts = []
    batch_size = 100  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•œ ë°°ì¹˜ í¬ê¸°
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        batch_tokens = encoder.encode_batch(batch)
        token_counts.extend([len(tokens) for tokens in batch_tokens])
    
    return token_counts

def estimate_context_window_usage(chunks: List[str], question: str, 
                                system_prompt: str = "", model: str = "gpt-3.5-turbo") -> Dict[str, int]:
    """
    ğŸ“‹ ê¸°ëŠ¥: RAG ì§ˆì˜ ì‹œ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡
    ğŸ“¥ ì…ë ¥: ì²­í¬ ë¦¬ìŠ¤íŠ¸, ì§ˆë¬¸, ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸, ëª¨ë¸ëª…
    ğŸ“¤ ì¶œë ¥: í† í° ì‚¬ìš©ëŸ‰ ìƒì„¸ ë¶„ì„
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ëª¨ë¸ í† í° ì œí•œ ë‚´ì—ì„œ ìµœëŒ€í•œ ë§ì€ ì²­í¬ í™œìš©
    """
    encoder = tiktoken.encoding_for_model(model)
    
    # ê° ìš”ì†Œë³„ í† í° ìˆ˜ ê³„ì‚°
    system_tokens = len(encoder.encode(system_prompt)) if system_prompt else 0
    question_tokens = len(encoder.encode(question))
    
    chunk_tokens = [len(encoder.encode(chunk)) for chunk in chunks]
    total_chunk_tokens = sum(chunk_tokens)
    
    # ëª¨ë¸ë³„ í† í° ì œí•œ
    model_limits = {
        "gpt-3.5-turbo": 4096,
        "gpt-4": 8192,
        "gpt-4-32k": 32768
    }
    
    max_tokens = model_limits.get(model, 4096)
    
    # ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ ì—¬ìœ  í† í° (ë³´í†µ 500-1000í† í° í•„ìš”)
    response_buffer = 800
    
    available_tokens = max_tokens - system_tokens - question_tokens - response_buffer
    
    return {
        "system_tokens": system_tokens,
        "question_tokens": question_tokens, 
        "chunk_tokens": total_chunk_tokens,
        "total_input_tokens": system_tokens + question_tokens + total_chunk_tokens,
        "available_tokens": available_tokens,
        "token_limit": max_tokens,
        "can_fit_all_chunks": total_chunk_tokens <= available_tokens,
        "max_chunks_can_fit": len([t for t in chunk_tokens if sum(chunk_tokens[:chunk_tokens.index(t)+1]) <= available_tokens])
    }
```

### ë¹„ìš© ìµœì í™” ì „ëµ
```python
class TokenCostOptimizer:
    """í† í° ê¸°ë°˜ ë¹„ìš© ìµœì í™” ë„êµ¬"""
    
    def __init__(self):
        # 2024ë…„ OpenAI ê°€ê²©í‘œ (USD per 1K tokens)
        self.pricing = {
            "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
            "gpt-4": {"input": 0.03, "output": 0.06},
            "text-embedding-ada-002": {"embedding": 0.0001}
        }
    
    def calculate_rag_session_cost(self, chunks: List[str], questions: List[str], 
                                 model: str = "gpt-3.5-turbo") -> Dict[str, float]:
        """
        ğŸ“‹ ê¸°ëŠ¥: RAG ì„¸ì…˜ ì „ì²´ ë¹„ìš© ê³„ì‚°
        ğŸ’° í¬í•¨ í•­ëª©: ì„ë² ë”© ìƒì„± + ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬
        """
        encoder = tiktoken.encoding_for_model(model)
        
        # ì„ë² ë”© ë¹„ìš© ê³„ì‚°
        chunk_tokens = sum(len(encoder.encode(chunk)) for chunk in chunks)
        embedding_cost = (chunk_tokens / 1000) * self.pricing["text-embedding-ada-002"]["embedding"]
        
        # ì§ˆì˜ì‘ë‹µ ë¹„ìš© ê³„ì‚°
        qa_cost = 0
        for question in questions:
            # í‰ê· ì ìœ¼ë¡œ ìƒìœ„ 3ê°œ ì²­í¬ ì‚¬ìš© ê°€ì •
            context_tokens = sum(len(encoder.encode(chunk)) for chunk in chunks[:3])
            question_tokens = len(encoder.encode(question))
            response_tokens = 150  # í‰ê·  ì‘ë‹µ ê¸¸ì´ ì¶”ì •
            
            input_cost = ((context_tokens + question_tokens) / 1000) * self.pricing[model]["input"]
            output_cost = (response_tokens / 1000) * self.pricing[model]["output"]
            
            qa_cost += input_cost + output_cost
        
        return {
            "embedding_cost": embedding_cost,
            "qa_cost": qa_cost,
            "total_cost": embedding_cost + qa_cost,
            "cost_per_question": qa_cost / len(questions) if questions else 0
        }
```

## ğŸ§ª ì‹¤ìŠµ ê³¼ì œ

### ğŸ”¨ ê¸°ë³¸ ê³¼ì œ
1. **í† í°-ë¬¸ì ë¹„êµ ë¶„ì„**: ë‹¤ì–‘í•œ ì–¸ì–´ì˜ í† í° íš¨ìœ¨ì„± ë¹„êµ
```python
# TODO: ì˜ì–´, í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ì˜ í† í°í™” íš¨ìœ¨ì„± ë¶„ì„
languages = {
    "English": "Hello everyone, my name is Nicolas",
    "Korean": "ì•ˆë…•í•˜ì„¸ìš” ì—¬ëŸ¬ë¶„, ì œ ì´ë¦„ì€ ë‹ˆì½œë¼ìŠ¤ì…ë‹ˆë‹¤", 
    "Japanese": "ã“ã‚“ã«ã¡ã¯çš†ã•ã‚“ã€ç§ã®åå‰ã¯ãƒ‹ã‚³ãƒ©ã‚¹ã§ã™",
    "Chinese": "å¤§å®¶å¥½ï¼Œæˆ‘çš„åå­—æ˜¯å°¼å¤æ‹‰æ–¯"
}
# íŒíŠ¸: ê° ì–¸ì–´ë³„ í† í° ìˆ˜ì™€ ë¬¸ì ìˆ˜ ë¹„ìœ¨ ê³„ì‚°
```

2. **ëª¨ë¸ë³„ í† í°í™” ì°¨ì´**: GPT-3.5ì™€ GPT-4ì˜ í† í°í™” ì°¨ì´ ë¶„ì„
```python
# TODO: ë™ì¼í•œ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë¥¸ ëª¨ë¸ë¡œ í† í°í™”í•˜ì—¬ ì°¨ì´ì  ë¶„ì„
models = ["gpt-3.5-turbo", "gpt-4"]
# íŒíŠ¸: ëŒ€ë¶€ë¶„ ë™ì¼í•˜ì§€ë§Œ íŠ¹ìˆ˜ ë¬¸ìë‚˜ ì½”ë“œì—ì„œ ì°¨ì´ ë°œìƒ ê°€ëŠ¥
```

### ğŸš€ ì‹¬í™” ê³¼ì œ
3. **ë™ì  ì²­í¬ í¬ê¸° ì¡°ì ˆ**: í…ìŠ¤íŠ¸ ë³µì¡ë„ì— ë”°ë¥¸ ì²­í¬ í¬ê¸° ìë™ ì¡°ì ˆ
```python
# TODO: í…ìŠ¤íŠ¸ì˜ í† í° ë°€ë„ì— ë”°ë¼ ì²­í¬ í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì ˆ
def adaptive_chunk_size(text_complexity: float, base_size: int = 600) -> int:
    """ë³µì¡ë„ê°€ ë†’ìœ¼ë©´ ì²­í¬ë¥¼ í¬ê²Œ, ë‚®ìœ¼ë©´ ì‘ê²Œ ì¡°ì ˆ"""
    pass
```

4. **ë¹„ìš© ìµœì í™” ì‹œë®¬ë ˆì´í„°**: ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì˜ ë¹„ìš© ë¹„êµ
```python
# TODO: ì²­í¬ í¬ê¸°, ì§ˆë¬¸ ìˆ˜, ëª¨ë¸ ì„ íƒì— ë”°ë¥¸ ë¹„ìš© ì‹œë®¬ë ˆì´ì…˜
def simulate_cost_scenarios(document_size: int, questions_per_day: int) -> Dict:
    """ì¼ì¼ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© ìµœì í™” ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„"""
    pass
```

### ğŸ’¡ ì°½ì˜ ê³¼ì œ
5. **í† í° ì••ì¶• ì•Œê³ ë¦¬ì¦˜**: ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ í† í° ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê¸°ë²•
```python
# TODO: ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°, ë¬¸ì¥ ì••ì¶• ë“±ìœ¼ë¡œ í† í° ì ˆì•½
def compress_text_for_tokens(text: str, target_ratio: float = 0.8) -> str:
    """ì˜ë¯¸ ë³´ì¡´í•˜ë©´ì„œ í† í° 20% ì ˆì•½í•˜ëŠ” í…ìŠ¤íŠ¸ ì••ì¶•"""
    pass
```

6. **ì‹¤ì‹œê°„ í† í° ëª¨ë‹ˆí„°ë§**: í† í° ì‚¬ìš©ëŸ‰ ì‹¤ì‹œê°„ ì¶”ì  ì‹œìŠ¤í…œ
```python
# TODO: ì‹¤ì‹œê°„ìœ¼ë¡œ í† í° ì‚¬ìš©ëŸ‰ê³¼ ë¹„ìš©ì„ ì¶”ì í•˜ëŠ” ì‹œìŠ¤í…œ
class TokenUsageMonitor:
    """ì‹¤ì‹œê°„ í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ"""
    pass
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### í† í° ê³„ì‚°ì˜ ì •í™•ì„±
```python
# âŒ ì˜ëª»ëœ ë°©ë²•: ë¬¸ì ìˆ˜ë¡œ í† í° ìˆ˜ ì¶”ì •
def estimate_tokens_wrong(text: str) -> int:
    return len(text) // 4  # ë¶€ì •í™•í•œ ì¶”ì •

# âœ… ì˜¬ë°”ë¥¸ ë°©ë²•: Tiktoken ì‚¬ìš©
def estimate_tokens_correct(text: str, model: str = "gpt-3.5-turbo") -> int:
    encoder = tiktoken.encoding_for_model(model)
    return len(encoder.encode(text))
```

### ëª¨ë¸ë³„ ì°¨ì´ì  ê³ ë ¤
- **GPT-3.5-turbo**: cl100k_base ì¸ì½”ë”© ì‚¬ìš©
- **GPT-4**: ë™ì¼í•œ cl100k_base ì¸ì½”ë”©
- **ì´ì „ ëª¨ë¸ë“¤**: ë‹¤ë¥¸ ì¸ì½”ë”© ë°©ì‹ ì‚¬ìš© (p50k_base ë“±)

### ì–¸ì–´ë³„ í† í° íš¨ìœ¨ì„±
- **ì˜ì–´**: 1í† í° â‰ˆ 4ë¬¸ì (ë†’ì€ íš¨ìœ¨ì„±)
- **í•œêµ­ì–´**: 1í† í° â‰ˆ 2ë¬¸ì (ì¤‘ê°„ íš¨ìœ¨ì„±)  
- **ì¤‘êµ­ì–´/ì¼ë³¸ì–´**: 1í† í° â‰ˆ 1.5ë¬¸ì (ë‚®ì€ íš¨ìœ¨ì„±)

### íŠ¹ìˆ˜ ë¬¸ìì™€ ì½”ë“œ
- **ì½”ë“œ**: ë“¤ì—¬ì“°ê¸°, íŠ¹ìˆ˜ë¬¸ìë¡œ ì¸í•´ í† í° ìˆ˜ ì¦ê°€
- **ë§ˆí¬ë‹¤ìš´**: í¬ë§·íŒ… ë¬¸ìë“¤ì´ ë³„ë„ í† í°ìœ¼ë¡œ ì²˜ë¦¬
- **URL/ì´ë©”ì¼**: íŠ¹ìˆ˜ íŒ¨í„´ìœ¼ë¡œ ì¸í•œ ë¹„íš¨ìœ¨ì„±

## ğŸ”— ê´€ë ¨ ìë£Œ
- **ì´ì „ í•™ìŠµ**: [6.1 Data Loaders and Splitters](./6.1_Data_Loaders_and_Splitters.md)
- **ë‹¤ìŒ í•™ìŠµ**: [6.3 Vectors and Embeddings](./6.3_Vectors_and_Embeddings.md)
- **ì°¸ê³  ë¬¸ì„œ**: [OpenAI Tiktoken](https://github.com/openai/tiktoken)
- **í† í° ê³„ì‚°ê¸°**: [OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer)
- **ì‹¤ìŠµ íŒŒì¼**: [6.2 Tiktoken.ipynb](../../00%20lecture/6.2%20Tiktoken.ipynb)

---

ğŸ’¡ **í•µì‹¬ ì •ë¦¬**: Tiktokenì€ OpenAI ëª¨ë¸ì´ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ëŠ” í† í°í™” ë°©ì‹ìœ¼ë¡œ, ì •í™•í•œ ë¹„ìš© ê³„ì‚°ê³¼ í† í° ì œí•œ ê´€ë¦¬ì— í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë¬¸ì ìˆ˜ê°€ ì•„ë‹Œ í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ê³  ë¹„ìš©ì„ ê³„ì‚°í•´ì•¼ ì‹¤ì œ ì‚¬ìš©ëŸ‰ê³¼ ì¼ì¹˜í•˜ëŠ” ì •í™•í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.