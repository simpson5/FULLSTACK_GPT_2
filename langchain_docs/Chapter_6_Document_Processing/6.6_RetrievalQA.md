# ğŸ“– Section 6.6: RetrievalQA - ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì²´ì¸

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- âœ… RetrievalQA Chainì˜ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬ ì™„ì „ ì´í•´
- âœ… 4ê°€ì§€ ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ(Stuff, Refine, Map-Reduce, Map-Rerank) í•™ìŠµ
- âœ… Retriever ì¸í„°í˜ì´ìŠ¤ì™€ ë²¡í„° ì €ì¥ì†Œ ì—°ë™ êµ¬í˜„
- âœ… ê° ì „ëµë³„ ë¹„ìš©, ì„±ëŠ¥, ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„

## ğŸ§  í•µì‹¬ ê°œë…

### RetrievalQA Chainì´ë€?
**RetrievalQA Chain**ì€ ë¬¸ì„œ ê²€ìƒ‰ê³¼ ì§ˆì˜ì‘ë‹µì„ ê²°í•©í•œ ì²´ì¸ìœ¼ë¡œ, ì‚¬ìš©ì ì§ˆë¬¸ì— ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

```mermaid
graph TD
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B[Retriever ê²€ìƒ‰]
    B --> C[ê´€ë ¨ ë¬¸ì„œ ì¶”ì¶œ]
    C --> D{ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ}
    
    D -->|Stuff| E[ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— í¬í•¨]
    D -->|Refine| F[ë¬¸ì„œë³„ë¡œ ì ì§„ì  ë‹µë³€ ê°œì„ ]
    D -->|Map-Reduce| G[ë¬¸ì„œë³„ ìš”ì•½ í›„ ì¢…í•©]
    D -->|Map-Rerank| H[ë¬¸ì„œë³„ ë‹µë³€ì— ì ìˆ˜ ë¶€ì—¬]
    
    E --> I[LLM ìµœì¢… ë‹µë³€ ìƒì„±]
    F --> I
    G --> I
    H --> I
    
    style A fill:#E6F3FF
    style I fill:#E6FFE6
    style D fill:#FFE6CC
```

### Retriever ì¸í„°í˜ì´ìŠ¤
**Retriever**ëŠ” êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ì§ˆì˜ì— ëŒ€í•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ë°˜í™˜í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤. ë²¡í„° ì €ì¥ì†Œë³´ë‹¤ ë” ì¼ë°˜ì ì¸ ê°œë…ì…ë‹ˆë‹¤.

| íŠ¹ì„± | Vector Store | Retriever |
|------|--------------|-----------|
| **ì €ì¥ ê¸°ëŠ¥** | ë¬¸ì„œ ì €ì¥ + ê²€ìƒ‰ | ê²€ìƒ‰ë§Œ ìˆ˜í–‰ |
| **ë°ì´í„° ì†ŒìŠ¤** | ìì²´ ì €ì¥ì†Œ | ë‹¤ì–‘í•œ ì†ŒìŠ¤(DB, API, íŒŒì¼) |
| **ê²€ìƒ‰ ë°©ì‹** | ë²¡í„° ìœ ì‚¬ì„± ê¸°ë°˜ | ë‹¤ì–‘í•œ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ |
| **ì‚¬ìš© ë²”ìœ„** | ì„ë² ë”©ëœ ë¬¸ì„œ | ëª¨ë“  ì¢…ë¥˜ì˜ ë¬¸ì„œ |

## ğŸ“‹ ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤

### RetrievalQA Chain
```python
from langchain.chains import RetrievalQA

class RetrievalQA:
    @classmethod
    def from_chain_type(
        cls,
        llm,                                       # ğŸ“Œ í•„ìˆ˜: ì–¸ì–´ ëª¨ë¸
        chain_type: str = "stuff",                 # ğŸ“Œ ìš©ë„: ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ
        retriever,                                 # ğŸ“Œ í•„ìˆ˜: ë¬¸ì„œ ê²€ìƒ‰ê¸°
        return_source_documents: bool = False,     # ğŸ“Œ ìš©ë„: ì†ŒìŠ¤ ë¬¸ì„œ ë°˜í™˜ ì—¬ë¶€
        **kwargs
    ):
        """
        ğŸ“‹ ê¸°ëŠ¥: ê²€ìƒ‰ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì²´ì¸ ìƒì„±
        ğŸ“¥ ì…ë ¥: LLM, ì²´ì¸ íƒ€ì…, ë¦¬íŠ¸ë¦¬ë²„, ì˜µì…˜
        ğŸ“¤ ì¶œë ¥: RetrievalQA ì¸ìŠ¤í„´ìŠ¤
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ë¬¸ì„œ ì»¬ë ‰ì…˜ì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
        
        âš ï¸  ì£¼ì˜: ì´ í´ë˜ìŠ¤ëŠ” Legacyë¡œ ë¶„ë¥˜ë¨ (LCEL ì‚¬ìš© ê¶Œì¥)
        """
    
    def run(self, query: str) -> str:
        """
        ğŸ“‹ ê¸°ëŠ¥: ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
        ğŸ“¥ ì…ë ¥: ì‚¬ìš©ì ì§ˆë¬¸
        ğŸ“¤ ì¶œë ¥: ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€
        """

# ì§€ì›ë˜ëŠ” chain_type ì˜µì…˜
CHAIN_TYPES = {
    "stuff": "ëª¨ë“  ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— í¬í•¨",
    "refine": "ë¬¸ì„œë³„ë¡œ ë‹µë³€ì„ ì ì§„ì ìœ¼ë¡œ ê°œì„ ",
    "map_reduce": "ë¬¸ì„œë³„ ìš”ì•½ í›„ ìµœì¢… ì¢…í•©",
    "map_rerank": "ë¬¸ì„œë³„ ë‹µë³€ì— ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ì—¬ ìµœê³ ì  ì„ íƒ"
}
```

### Retriever ë³€í™˜
```python
from langchain.vectorstores import Chroma

class VectorStoreRetriever:
    def __init__(
        self,
        vectorstore,                               # ğŸ“Œ í•„ìˆ˜: ë²¡í„° ì €ì¥ì†Œ
        search_kwargs: dict = {"k": 4}             # ğŸ“Œ ìš©ë„: ê²€ìƒ‰ ì˜µì…˜ (ë°˜í™˜ ë¬¸ì„œ ìˆ˜)
    ):
        """
        ğŸ“‹ ê¸°ëŠ¥: ë²¡í„° ì €ì¥ì†Œë¥¼ Retrieverë¡œ ë³€í™˜
        ğŸ’¡ í™œìš©: ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ RetrievalQAì—ì„œ ì‚¬ìš© ê°€ëŠ¥
        """

# ë²¡í„° ì €ì¥ì†Œì—ì„œ Retriever ìƒì„±
vector_store = Chroma.from_documents(docs, embeddings)
retriever = vector_store.as_retriever(
    search_kwargs={"k": 4}  # ìƒìœ„ 4ê°œ ë¬¸ì„œ ë°˜í™˜
)
```

## ğŸ”§ ë™ì‘ ê³¼ì • ìƒì„¸

### 1ë‹¨ê³„: ê¸°ë³¸ RetrievalQA ì²´ì¸ êµ¬ì¶•
```python
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# === ê¸°ë³¸ RetrievalQA ì²´ì¸ ì„¤ì • ===
# ğŸ§  ê°œë…: ë¬¸ì„œ ê²€ìƒ‰ê³¼ ë‹µë³€ ìƒì„±ì„ ìë™í™”í•˜ëŠ” ì²´ì¸

print("ğŸ”— RetrievalQA ì²´ì¸ êµ¬ì¶•:")
print("=" * 50)

# ğŸ”§ 1ë‹¨ê³„: í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ ì¤€ë¹„
llm = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0  # ğŸ“Œ ì¼ê´€ëœ ë‹µë³€ì„ ìœ„í•´ temperature=0
)

# ì´ì „ì— ìƒì„±í•œ ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©
embeddings = OpenAIEmbeddings()
vector_store = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# ğŸ”§ 2ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œë¥¼ Retrieverë¡œ ë³€í™˜
retriever = vector_store.as_retriever(
    search_kwargs={
        "k": 4,  # ğŸ“Œ ìƒìœ„ 4ê°œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        "score_threshold": 0.7  # ğŸ“Œ ìµœì†Œ ìœ ì‚¬ì„± ì„ê³„ê°’ (ì„ íƒì‚¬í•­)
    }
)

print(f"âœ… Retriever ì„¤ì • ì™„ë£Œ (ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: 4ê°œ)")

# ğŸ”§ 3ë‹¨ê³„: RetrievalQA ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # ğŸ“Œ ê¸°ë³¸ ì „ëµ: ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì—
    retriever=retriever,
    return_source_documents=True,  # ğŸ“Œ ì†ŒìŠ¤ ë¬¸ì„œë„ í•¨ê»˜ ë°˜í™˜
    verbose=True  # ğŸ“Œ ë””ë²„ê¹…ìš©: ì¤‘ê°„ ê³¼ì • ì¶œë ¥
)

print(f"âœ… RetrievalQA ì²´ì¸ ìƒì„± ì™„ë£Œ (ì „ëµ: stuff)")

# ğŸ”§ 4ë‹¨ê³„: ì²´ì¸ í…ŒìŠ¤íŠ¸
test_queries = [
    "Winstonì€ ì–´ë””ì— ì‚´ê³  ìˆë‚˜ìš”?",
    "Victory Mansionsì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "Ministry of LoveëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
]

for i, query in enumerate(test_queries, 1):
    print(f"\n{'='*20} í…ŒìŠ¤íŠ¸ {i} {'='*20}")
    print(f"ğŸ“‹ ì§ˆë¬¸: {query}")
    
    # ì²´ì¸ ì‹¤í–‰
    result = qa_chain({"query": query})
    
    print(f"ğŸ¤– ë‹µë³€: {result['result']}")
    print(f"ğŸ“Š ì‚¬ìš©ëœ ë¬¸ì„œ ìˆ˜: {len(result['source_documents'])}")
    
    # ì†ŒìŠ¤ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
    for j, doc in enumerate(result['source_documents'][:2], 1):
        preview = doc.page_content[:100].replace('\n', ' ')
        print(f"   ğŸ“„ ë¬¸ì„œ {j}: {preview}...")
```

### 2ë‹¨ê³„: 4ê°€ì§€ ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ ë¹„êµ
```python
import time
from typing import Dict, List

# === ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ ë¹„êµ ë¶„ì„ ===
# ğŸ§  ê°œë…: ê° ì „ëµì˜ íŠ¹ì„±ê³¼ ì„±ëŠ¥ ë¹„êµ

class ChainStrategyComparison:
    """ë¬¸ì„œ ì²˜ë¦¬ ì „ëµë³„ ì„±ëŠ¥ ë° ê²°ê³¼ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever
        self.strategies = ["stuff", "refine", "map_reduce", "map_rerank"]
        
    def create_chain(self, strategy: str) -> RetrievalQA:
        """ì „ëµë³„ ì²´ì¸ ìƒì„±"""
        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type=strategy,
            retriever=self.retriever,
            return_source_documents=True
        )
    
    def compare_strategies(self, query: str) -> Dict:
        """ëª¨ë“  ì „ëµìœ¼ë¡œ ë™ì¼í•œ ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ì—¬ ë¹„êµ"""
        
        print(f"ğŸ” ì „ëµë³„ ë¹„êµ ë¶„ì„: '{query}'")
        print("=" * 80)
        
        results = {}
        
        for strategy in self.strategies:
            print(f"\nğŸ“Š {strategy.upper()} ì „ëµ ì‹¤í–‰ ì¤‘...")
            
            try:
                # ì²´ì¸ ìƒì„±
                chain = self.create_chain(strategy)
                
                # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
                start_time = time.time()
                result = chain({"query": query})
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ë¶„ì„
                answer_length = len(result['result'])
                source_count = len(result.get('source_documents', []))
                
                results[strategy] = {
                    "answer": result['result'],
                    "execution_time": execution_time,
                    "answer_length": answer_length,
                    "source_documents_count": source_count,
                    "success": True
                }
                
                print(f"   â±ï¸ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
                print(f"   ğŸ“ ë‹µë³€ ê¸¸ì´: {answer_length} ë¬¸ì")
                print(f"   ğŸ“š ì‚¬ìš© ë¬¸ì„œ: {source_count}ê°œ")
                print(f"   ğŸ¯ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°: {result['result'][:100]}...")
                
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                results[strategy] = {
                    "error": str(e),
                    "success": False
                }
        
        return results
    
    def analyze_results(self, results: Dict) -> None:
        """ê²°ê³¼ ë¶„ì„ ë° ìš”ì•½"""
        
        print("\nğŸ“Š ì „ëµë³„ ì„±ëŠ¥ ìš”ì•½:")
        print("-" * 60)
        
        successful_results = {k: v for k, v in results.items() if v.get('success')}
        
        if not successful_results:
            print("âŒ ì„±ê³µí•œ ì „ëµì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        fastest_strategy = min(successful_results.keys(), 
                             key=lambda x: successful_results[x]['execution_time'])
        longest_answer = max(successful_results.keys(),
                           key=lambda x: successful_results[x]['answer_length'])
        
        print(f"ğŸš€ ê°€ì¥ ë¹ ë¥¸ ì „ëµ: {fastest_strategy} "
              f"({successful_results[fastest_strategy]['execution_time']:.2f}ì´ˆ)")
        print(f"ğŸ“ ê°€ì¥ ìƒì„¸í•œ ë‹µë³€: {longest_answer} "
              f"({successful_results[longest_answer]['answer_length']} ë¬¸ì)")
        
        # ê° ì „ëµë³„ íŠ¹ì„± ìš”ì•½
        strategy_characteristics = {
            "stuff": "ğŸ’° ë¹„ìš© íš¨ìœ¨ì , âš¡ ë¹ ë¦„, âš ï¸ í† í° ì œí•œ",
            "refine": "ğŸ¯ ì ì§„ì  ê°œì„ , ğŸ’°ğŸ’° ë¹„ìš© ë†’ìŒ, ğŸ¨ ì°½ì˜ì ",
            "map_reduce": "ğŸ“š ëŒ€ìš©ëŸ‰ ì²˜ë¦¬, ğŸ’°ğŸ’° ë¹„ìš© ë†’ìŒ, ğŸ”„ ë³‘ë ¬ ê°€ëŠ¥",
            "map_rerank": "ğŸ† ì‹ ë¢°ë„ ê¸°ë°˜, ğŸ’°ğŸ’° ë¹„ìš© ë†’ìŒ, ğŸ“Š ì ìˆ˜ ì œê³µ"
        }
        
        print(f"\nğŸ“‹ ì „ëµë³„ íŠ¹ì„±:")
        for strategy, chars in strategy_characteristics.items():
            status = "âœ…" if strategy in successful_results else "âŒ"
            print(f"   {status} {strategy.upper()}: {chars}")

# === ì‹¤ì œ ë¹„êµ ì‹¤í–‰ ===
comparator = ChainStrategyComparison(llm, retriever)

# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
comparison_queries = [
    "Winston Smithì˜ ì¼ìƒìƒí™œì€ ì–´ë–¤ê°€ìš”?",
    "1984ë…„ ì†Œì„¤ì˜ ì£¼ìš” í…Œë§ˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
]

for query in comparison_queries:
    results = comparator.compare_strategies(query)
    comparator.analyze_results(results)
    print("\n" + "="*80 + "\n")
```

### 3ë‹¨ê³„: ê° ì „ëµì˜ ìƒì„¸ ë™ì‘ ë¶„ì„
```python
# === Stuff Strategy ìƒì„¸ ë¶„ì„ ===
# ğŸ§  ê°œë…: ëª¨ë“  ê´€ë ¨ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— í¬í•¨

def analyze_stuff_strategy(qa_chain, query: str):
    """Stuff ì „ëµì˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë¶„ì„"""
    
    print("ğŸ” STUFF ì „ëµ ìƒì„¸ ë¶„ì„:")
    print("=" * 50)
    
    # ë¨¼ì € retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰
    retrieved_docs = qa_chain.retriever.get_relevant_documents(query)
    
    print(f"ğŸ“‹ ì§ˆë¬¸: {query}")
    print(f"ğŸ“š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
    
    # ë¬¸ì„œë“¤ì˜ ì´ í† í° ìˆ˜ ì¶”ì •
    total_chars = sum(len(doc.page_content) for doc in retrieved_docs)
    estimated_tokens = total_chars // 4  # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì •
    
    print(f"ğŸ“Š ì´ ë¬¸ì ìˆ˜: {total_chars}")
    print(f"ğŸ“Š ì˜ˆìƒ í† í° ìˆ˜: {estimated_tokens}")
    
    if estimated_tokens > 3000:  # GPT-3.5-turbo ê¸°ì¤€
        print("âš ï¸ ê²½ê³ : í† í° ìˆ˜ê°€ ë§ì•„ ëª¨ë¸ ì œí•œì— ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ê° ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
    print(f"\nğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:")
    for i, doc in enumerate(retrieved_docs, 1):
        preview = doc.page_content[:150].replace('\n', ' ')
        print(f"   {i}. [{len(doc.page_content)}ì] {preview}...")
    
    # ì‹¤ì œ ì²´ì¸ ì‹¤í–‰
    result = qa_chain({"query": query})
    
    print(f"\nğŸ¤– ìµœì¢… ë‹µë³€:")
    print(f"{result['result']}")
    
    return result

# Stuff ì „ëµ ë¶„ì„ ì‹¤í–‰
stuff_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)

stuff_result = analyze_stuff_strategy(
    stuff_chain, 
    "Winston SmithëŠ” ì–´ë–¤ ì¢…ë¥˜ì˜ ì‚¬ëŒì¸ê°€ìš”?"
)
```

### 4ë‹¨ê³„: ë²¡í„° ì €ì¥ì†Œ ì„±ëŠ¥ ë¹„êµ
```python
from langchain.vectorstores import FAISS
import numpy as np

# === ë²¡í„° ì €ì¥ì†Œë³„ ì„±ëŠ¥ ë¹„êµ ===
# ğŸ§  ê°œë…: Chroma vs FAISS ì„±ëŠ¥ ë° íŠ¹ì„± ë¹„êµ

class VectorStoreComparison:
    """ë²¡í„° ì €ì¥ì†Œë³„ ì„±ëŠ¥ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, documents, embeddings):
        self.documents = documents
        self.embeddings = embeddings
        
    def create_chroma_store(self) -> Chroma:
        """Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        print("ğŸ”µ Chroma ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
        start_time = time.time()
        
        chroma_store = Chroma.from_documents(
            self.documents,
            self.embeddings,
            persist_directory="./chroma_comparison"
        )
        
        creation_time = time.time() - start_time
        print(f"   â±ï¸ ìƒì„± ì‹œê°„: {creation_time:.2f}ì´ˆ")
        
        return chroma_store
    
    def create_faiss_store(self) -> FAISS:
        """FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        print("ğŸŸ  FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
        start_time = time.time()
        
        faiss_store = FAISS.from_documents(
            self.documents,
            self.embeddings
        )
        
        creation_time = time.time() - start_time
        print(f"   â±ï¸ ìƒì„± ì‹œê°„: {creation_time:.2f}ì´ˆ")
        
        return faiss_store
    
    def compare_search_performance(self, query: str, k: int = 4):
        """ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ"""
        
        print(f"\nğŸ” ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ: '{query}'")
        print("-" * 60)
        
        # Chroma ê²€ìƒ‰
        chroma_store = self.create_chroma_store()
        
        start_time = time.time()
        chroma_results = chroma_store.similarity_search(query, k=k)
        chroma_search_time = time.time() - start_time
        
        print(f"ğŸ”µ Chroma ê²€ìƒ‰ ì‹œê°„: {chroma_search_time:.4f}ì´ˆ")
        
        # FAISS ê²€ìƒ‰  
        faiss_store = self.create_faiss_store()
        
        start_time = time.time()
        faiss_results = faiss_store.similarity_search(query, k=k)
        faiss_search_time = time.time() - start_time
        
        print(f"ğŸŸ  FAISS ê²€ìƒ‰ ì‹œê°„: {faiss_search_time:.4f}ì´ˆ")
        
        # ì„±ëŠ¥ ë¹„êµ
        speed_ratio = chroma_search_time / faiss_search_time
        faster_store = "FAISS" if speed_ratio > 1 else "Chroma"
        
        print(f"ğŸ† ë” ë¹ ë¥¸ ì €ì¥ì†Œ: {faster_store} "
              f"({abs(speed_ratio):.1f}ë°°)")
        
        # ê²°ê³¼ ì¼ê´€ì„± í™•ì¸
        chroma_content = [doc.page_content for doc in chroma_results]
        faiss_content = [doc.page_content for doc in faiss_results]
        
        matching_docs = len(set(chroma_content) & set(faiss_content))
        consistency = matching_docs / k * 100
        
        print(f"ğŸ“Š ê²°ê³¼ ì¼ê´€ì„±: {consistency:.1f}% ({matching_docs}/{k}ê°œ ì¼ì¹˜)")
        
        return {
            "chroma_time": chroma_search_time,
            "faiss_time": faiss_search_time,
            "faster_store": faster_store,
            "consistency": consistency
        }

# ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰ (ì´ì „ì— ë¶„í• ëœ ë¬¸ì„œë“¤ ì‚¬ìš©)
if 'docs' in locals() and docs:  # docsê°€ ì •ì˜ë˜ì–´ ìˆë‹¤ë©´
    comparison = VectorStoreComparison(docs[:10], embeddings)  # ì²˜ìŒ 10ê°œ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸
    
    test_queries = [
        "Winstonì˜ ê±°ì£¼ì§€",
        "Ministry of Love",
        "Victory Mansions"
    ]
    
    for query in test_queries:
        result = comparison.compare_search_performance(query)
        print()
else:
    print("âš ï¸ ë¹„êµë¥¼ ìœ„í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ë¡œë”©í•˜ì„¸ìš”.")
```

## ğŸ’» ì‹¤ì „ ì˜ˆì œ

### ê³ ê¸‰ Document GPT ì‹œìŠ¤í…œ
```python
from typing import Dict, List, Optional, Any
import logging
from langchain.schema import BaseRetriever
from langchain.callbacks import get_openai_callback

class AdvancedDocumentGPT:
    """
    ğŸ¯ ê³ ê¸‰ Document GPT ì‹œìŠ¤í…œ
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ë‹¤ì¤‘ ì²´ì¸ ì „ëµ ì§€ì›
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¹„ìš© ì¶”ì 
    - ë™ì  ì „ëµ ì„ íƒ
    - ê²°ê³¼ í’ˆì§ˆ í‰ê°€
    """
    
    def __init__(self, 
                 llm,
                 retriever: BaseRetriever,
                 default_strategy: str = "stuff",
                 enable_monitoring: bool = True):
        
        self.llm = llm
        self.retriever = retriever
        self.default_strategy = default_strategy
        self.enable_monitoring = enable_monitoring
        
        # ì „ëµë³„ ì²´ì¸ ìºì‹œ
        self._chains = {}
        
        # ì‚¬ìš© í†µê³„
        self.usage_stats = {
            "total_queries": 0,
            "strategy_usage": {},
            "total_cost": 0.0,
            "avg_response_time": 0.0
        }
        
        # ë¡œê±° ì„¤ì •
        if enable_monitoring:
            logging.basicConfig(level=logging.INFO)
            self.logger = logging.getLogger(__name__)
        
    def _get_chain(self, strategy: str) -> RetrievalQA:
        """ì „ëµë³„ ì²´ì¸ ìºì‹œ ë° ë°˜í™˜"""
        if strategy not in self._chains:
            self._chains[strategy] = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type=strategy,
                retriever=self.retriever,
                return_source_documents=True
            )
        return self._chains[strategy]
    
    def _select_optimal_strategy(self, query: str) -> str:
        """ì§ˆì˜ íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  ì „ëµ ì„ íƒ"""
        
        # ì§ˆì˜ ê¸¸ì´ ê¸°ë°˜ ë¶„ì„
        query_length = len(query.split())
        
        # ë³µì¡í•œ ì§ˆë¬¸ íŒ¨í„´ ê°ì§€
        complex_patterns = [
            "ë¶„ì„", "ë¹„êµ", "ì„¤ëª…", "ìš”ì•½", "í‰ê°€",
            "analyze", "compare", "describe", "summarize"
        ]
        
        is_complex = any(pattern in query.lower() for pattern in complex_patterns)
        
        # ì „ëµ ì„ íƒ ë¡œì§
        if query_length > 20 and is_complex:
            return "refine"  # ë³µì¡í•œ ê¸´ ì§ˆë¬¸ì€ refine
        elif "ë¹„êµ" in query or "compare" in query.lower():
            return "map_rerank"  # ë¹„êµ ì§ˆë¬¸ì€ map_rerank
        elif query_length > 15:
            return "map_reduce"  # ì¤‘ê°„ ê¸¸ì´ëŠ” map_reduce
        else:
            return "stuff"  # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ stuff
    
    def ask(self, 
            query: str,
            strategy: Optional[str] = None,
            auto_select_strategy: bool = False) -> Dict[str, Any]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
        ğŸ“¥ ì…ë ¥: ì§ˆë¬¸, ì „ëµ(ì„ íƒ), ìë™ ì „ëµ ì„ íƒ ì—¬ë¶€
        ğŸ“¤ ì¶œë ¥: ë‹µë³€ê³¼ ë©”íƒ€ë°ì´í„°
        """
        
        # ì „ëµ ê²°ì •
        if auto_select_strategy:
            selected_strategy = self._select_optimal_strategy(query)
        else:
            selected_strategy = strategy or self.default_strategy
        
        if self.enable_monitoring:
            self.logger.info(f"Query: {query[:50]}... | Strategy: {selected_strategy}")
        
        # ë¹„ìš© ë° ì‹œê°„ ì¶”ì 
        start_time = time.time()
        
        try:
            if self.enable_monitoring:
                with get_openai_callback() as cb:
                    chain = self._get_chain(selected_strategy)
                    result = chain({"query": query})
                    
                    execution_time = time.time() - start_time
                    cost = cb.total_cost
            else:
                chain = self._get_chain(selected_strategy)
                result = chain({"query": query})
                execution_time = time.time() - start_time
                cost = 0.0
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(selected_strategy, execution_time, cost)
            
            # ê²°ê³¼ í’ˆì§ˆ í‰ê°€
            quality_score = self._evaluate_answer_quality(
                query, result['result'], result['source_documents']
            )
            
            return {
                "answer": result['result'],
                "source_documents": result['source_documents'],
                "strategy_used": selected_strategy,
                "execution_time": execution_time,
                "cost": cost,
                "quality_score": quality_score,
                "success": True
            }
            
        except Exception as e:
            if self.enable_monitoring:
                self.logger.error(f"Query failed: {e}")
            
            return {
                "error": str(e),
                "strategy_used": selected_strategy,
                "success": False
            }
    
    def _update_stats(self, strategy: str, execution_time: float, cost: float):
        """ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸"""
        self.usage_stats["total_queries"] += 1
        self.usage_stats["strategy_usage"][strategy] = \
            self.usage_stats["strategy_usage"].get(strategy, 0) + 1
        self.usage_stats["total_cost"] += cost
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_queries = self.usage_stats["total_queries"]
        current_avg = self.usage_stats["avg_response_time"]
        self.usage_stats["avg_response_time"] = \
            (current_avg * (total_queries - 1) + execution_time) / total_queries
    
    def _evaluate_answer_quality(self, query: str, answer: str, sources: List) -> float:
        """ë‹µë³€ í’ˆì§ˆ í‰ê°€ (0.0-1.0 ì ìˆ˜)"""
        
        quality_factors = []
        
        # 1. ë‹µë³€ ê¸¸ì´ ì ì ˆì„± (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•ŠìŒ)
        answer_length = len(answer.split())
        if 10 <= answer_length <= 200:
            quality_factors.append(0.8)
        elif 5 <= answer_length < 10 or 200 < answer_length <= 500:
            quality_factors.append(0.6)
        else:
            quality_factors.append(0.3)
        
        # 2. ì†ŒìŠ¤ ë¬¸ì„œ í™œìš©ë„
        if len(sources) >= 2:
            quality_factors.append(0.9)
        elif len(sources) == 1:
            quality_factors.append(0.7)
        else:
            quality_factors.append(0.3)
        
        # 3. "ëª¨ë¥´ê² ë‹¤" ë‹µë³€ ì²´í¬ (ì‹ ë¢°ì„±)
        uncertainty_phrases = ["ëª¨ë¥´ê² ", "don't know", "í™•ì‹¤í•˜ì§€", "not sure"]
        if any(phrase in answer.lower() for phrase in uncertainty_phrases):
            # ì ì ˆí•œ ë¶ˆí™•ì‹¤ì„± í‘œí˜„ì€ ì¢‹ì€ ì‹ í˜¸
            quality_factors.append(0.8)
        else:
            quality_factors.append(0.9)
        
        return sum(quality_factors) / len(quality_factors)
    
    def get_usage_report(self) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ ë¦¬í¬íŠ¸"""
        return {
            "summary": self.usage_stats,
            "most_used_strategy": max(
                self.usage_stats["strategy_usage"].items(),
                key=lambda x: x[1],
                default=("none", 0)
            )[0],
            "cost_per_query": (
                self.usage_stats["total_cost"] / max(self.usage_stats["total_queries"], 1)
            )
        }
    
    def batch_ask(self, queries: List[str], **kwargs) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì§ˆì˜ ì²˜ë¦¬"""
        
        results = []
        total_start = time.time()
        
        print(f"ğŸš€ ë°°ì¹˜ ì§ˆì˜ ì²˜ë¦¬ ì‹œì‘: {len(queries)}ê°œ")
        
        for i, query in enumerate(queries, 1):
            print(f"\nğŸ“‹ ì§ˆì˜ {i}/{len(queries)}: {query[:50]}...")
            
            result = self.ask(query, **kwargs)
            results.append(result)
            
            if result['success']:
                print(f"   âœ… ì™„ë£Œ ({result['execution_time']:.2f}ì´ˆ, í’ˆì§ˆ: {result['quality_score']:.2f})")
            else:
                print(f"   âŒ ì‹¤íŒ¨: {result['error']}")
        
        total_time = time.time() - total_start
        success_count = sum(1 for r in results if r['success'])
        
        print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   âœ… ì„±ê³µ: {success_count}/{len(queries)}")
        print(f"   â±ï¸ ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   ğŸ“ˆ í‰ê·  ì‹œê°„: {total_time/len(queries):.2f}ì´ˆ/ì§ˆì˜")
        
        return results

# === ê³ ê¸‰ Document GPT ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ ===
print("ğŸš€ ê³ ê¸‰ Document GPT ì‹œìŠ¤í…œ ì‹œì‘")
print("=" * 60)

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
advanced_gpt = AdvancedDocumentGPT(
    llm=llm,
    retriever=retriever,
    default_strategy="stuff",
    enable_monitoring=True
)

# ë‹¤ì–‘í•œ ì§ˆì˜ í…ŒìŠ¤íŠ¸
test_queries = [
    "Winston SmithëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",  # ê°„ë‹¨í•œ ì§ˆë¬¸ â†’ stuff
    "1984ë…„ ì†Œì„¤ì—ì„œ ê°ì‹œ ì‚¬íšŒì˜ íŠ¹ì§•ì„ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”",  # ë³µì¡í•œ ì§ˆë¬¸ â†’ refine
    "Ministry of Loveì™€ Ministry of Truthë¥¼ ë¹„êµí•´ì£¼ì„¸ìš”",  # ë¹„êµ ì§ˆë¬¸ â†’ map_rerank
    "ì†Œì„¤ì˜ ì£¼ìš” í…Œë§ˆë“¤ì„ ìš”ì•½í•´ì£¼ì„¸ìš”"  # ìš”ì•½ ì§ˆë¬¸ â†’ map_reduce
]

# ê°œë³„ ì§ˆì˜ í…ŒìŠ¤íŠ¸ (ìë™ ì „ëµ ì„ íƒ)
print("\nğŸ¯ ìë™ ì „ëµ ì„ íƒ í…ŒìŠ¤íŠ¸:")
for query in test_queries:
    result = advanced_gpt.ask(query, auto_select_strategy=True)
    
    if result['success']:
        print(f"\nğŸ“‹ ì§ˆë¬¸: {query}")
        print(f"ğŸ”§ ì „ëµ: {result['strategy_used']}")
        print(f"â±ï¸ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
        print(f"ğŸ“Š í’ˆì§ˆ: {result['quality_score']:.2f}/1.0")
        print(f"ğŸ¤– ë‹µë³€: {result['answer'][:100]}...")
    else:
        print(f"âŒ ì‹¤íŒ¨: {result['error']}")

# ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
print("\nğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸:")
batch_results = advanced_gpt.batch_ask(
    test_queries[:2], 
    auto_select_strategy=True
)

# ì‚¬ìš© í†µê³„ ë¦¬í¬íŠ¸
print("\nğŸ“ˆ ì‚¬ìš© í†µê³„ ë¦¬í¬íŠ¸:")
report = advanced_gpt.get_usage_report()
for key, value in report.items():
    print(f"   {key}: {value}")

print("\nâœ… ê³ ê¸‰ Document GPT ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
```

## ğŸ” ë³€ìˆ˜/í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…

### ì²´ì¸ ì „ëµë³„ íŠ¹ì„± ë¶„ì„

#### Stuff ì „ëµ ìµœì í™”
```python
def optimize_stuff_strategy(retriever, query: str, max_tokens: int = 3500) -> List:
    """
    ğŸ“‹ ê¸°ëŠ¥: Stuff ì „ëµì„ ìœ„í•œ ë¬¸ì„œ ìˆ˜ ìµœì í™”
    ğŸ“¥ ì…ë ¥: ë¦¬íŠ¸ë¦¬ë²„, ì§ˆì˜, ìµœëŒ€ í† í° ìˆ˜
    ğŸ“¤ ì¶œë ¥: ìµœì í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: í† í° ì œí•œ ë‚´ì—ì„œ ìµœëŒ€í•œ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì„ íƒ
    """
    
    # ëª¨ë“  ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ë” ë§ì€ í›„ë³´ í™•ë³´)
    all_docs = retriever.get_relevant_documents(query, k=10)
    
    selected_docs = []
    current_tokens = 0
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì§ˆë¬¸ ë“±ì„ ìœ„í•œ í† í° ì˜ˆì•½
    reserved_tokens = 500
    available_tokens = max_tokens - reserved_tokens
    
    for doc in all_docs:
        # í† í° ìˆ˜ ì¶”ì • (ë¬¸ì ìˆ˜ / 4)
        doc_tokens = len(doc.page_content) // 4
        
        if current_tokens + doc_tokens <= available_tokens:
            selected_docs.append(doc)
            current_tokens += doc_tokens
        else:
            # í† í° ì œí•œ ì´ˆê³¼ì‹œ ì¤‘ë‹¨
            break
    
    print(f"ğŸ“Š ì„ íƒëœ ë¬¸ì„œ: {len(selected_docs)}ê°œ")
    print(f"ğŸ“Š ì˜ˆìƒ í† í°: {current_tokens}/{available_tokens}")
    
    return selected_docs

def estimate_refine_cost(docs: List, model: str = "gpt-3.5-turbo") -> Dict[str, float]:
    """
    ğŸ“‹ ê¸°ëŠ¥: Refine ì „ëµì˜ ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
    ğŸ“¥ ì…ë ¥: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸, ëª¨ë¸ëª…
    ğŸ“¤ ì¶œë ¥: ë¹„ìš© ë¶„ì„ ë¦¬í¬íŠ¸
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: Refine ì „ëµ ì‚¬ìš© ì „ ë¹„ìš© ì˜ˆì¸¡
    """
    
    # OpenAI ê°€ê²© (2024ë…„ ê¸°ì¤€)
    pricing = {
        "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
        "gpt-4": {"input": 0.03, "output": 0.06}
    }
    
    if model not in pricing:
        model = "gpt-3.5-turbo"
    
    # ê° ë¬¸ì„œë³„ ì²˜ë¦¬ ë¹„ìš© ê³„ì‚°
    doc_count = len(docs)
    avg_doc_length = sum(len(doc.page_content) for doc in docs) / doc_count
    
    # í† í° ì¶”ì • (ë¬¸ì / 4)
    tokens_per_doc = avg_doc_length // 4
    
    # Refineì€ ë¬¸ì„œë³„ë¡œ ì ì§„ì  ì²˜ë¦¬
    total_input_tokens = 0
    total_output_tokens = 0
    
    for i in range(doc_count):
        # ië²ˆì§¸ ë¬¸ì„œ ì²˜ë¦¬ì‹œ í•„ìš”í•œ í† í°
        # í˜„ì¬ ë¬¸ì„œ + ì´ì „ê¹Œì§€ì˜ ë‹µë³€ (ëˆ„ì )
        current_input = tokens_per_doc + (i * 100)  # ë‹µë³€ ëˆ„ì  ì¶”ì •
        current_output = 150  # ë‹µë³€ í† í° ì¶”ì •
        
        total_input_tokens += current_input
        total_output_tokens += current_output
    
    # ë¹„ìš© ê³„ì‚°
    input_cost = (total_input_tokens / 1000) * pricing[model]["input"]
    output_cost = (total_output_tokens / 1000) * pricing[model]["output"]
    total_cost = input_cost + output_cost
    
    return {
        "document_count": doc_count,
        "estimated_input_tokens": total_input_tokens,
        "estimated_output_tokens": total_output_tokens,
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost,
        "cost_per_document": total_cost / doc_count
    }
```

#### ê²°ê³¼ í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜
```python
def evaluate_answer_comprehensiveness(answer: str, query: str, sources: List) -> Dict[str, float]:
    """
    ğŸ“‹ ê¸°ëŠ¥: ë‹µë³€ì˜ í¬ê´„ì„±ê³¼ í’ˆì§ˆ í‰ê°€
    ğŸ“¥ ì…ë ¥: ë‹µë³€, ì§ˆë¬¸, ì†ŒìŠ¤ ë¬¸ì„œë“¤
    ğŸ“¤ ì¶œë ¥: í’ˆì§ˆ ì§€í‘œë“¤
    """
    
    import re
    from collections import Counter
    
    metrics = {}
    
    # 1. ë‹µë³€ ê¸¸ì´ ì ì ˆì„±
    word_count = len(answer.split())
    if 50 <= word_count <= 300:
        metrics["length_score"] = 1.0
    elif 20 <= word_count < 50 or 300 < word_count <= 500:
        metrics["length_score"] = 0.7
    else:
        metrics["length_score"] = 0.4
    
    # 2. í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€
    query_words = set(re.findall(r'\b\w+\b', query.lower()))
    answer_words = set(re.findall(r'\b\w+\b', answer.lower()))
    
    keyword_coverage = len(query_words & answer_words) / max(len(query_words), 1)
    metrics["keyword_coverage"] = keyword_coverage
    
    # 3. ì†ŒìŠ¤ í™œìš©ë„
    source_utilization = min(len(sources) / 3, 1.0)  # ìµœì  3-4ê°œ ë¬¸ì„œ
    metrics["source_utilization"] = source_utilization
    
    # 4. êµ¬ì¡°ì  ì™„ì„±ë„ (ë¬¸ì¥ ì™„ì„±, ë…¼ë¦¬ì  íë¦„)
    sentences = re.split(r'[.!?]', answer)
    complete_sentences = [s for s in sentences if len(s.strip()) > 5]
    structure_score = min(len(complete_sentences) / 3, 1.0)
    metrics["structure_score"] = structure_score
    
    # 5. ì‹ ë¢°ì„± ì§€í‘œ (ë¶ˆí™•ì‹¤ì„± ì ì ˆí•œ í‘œí˜„)
    uncertainty_phrases = [
        "í™•ì‹¤í•˜ì§€", "ì•„ë§ˆë„", "ê²ƒìœ¼ë¡œ ë³´ì„", "ê°€ëŠ¥ì„±ì´", 
        "not certain", "appears", "likely", "seems"
    ]
    
    has_uncertainty = any(phrase in answer.lower() for phrase in uncertainty_phrases)
    
    # ì§§ì€ ë‹µë³€ì—ì„œ ë¶ˆí™•ì‹¤ì„± í‘œí˜„ì€ ì¢‹ì€ ì‹ í˜¸
    if word_count < 50 and has_uncertainty:
        reliability_score = 0.9
    elif word_count >= 50 and not has_uncertainty:
        reliability_score = 0.8  # ê¸´ ë‹µë³€ì—ì„œëŠ” í™•ì‹  ìˆëŠ” ë‹µë³€ì´ ì¢‹ì„ ìˆ˜ ìˆìŒ
    else:
        reliability_score = 0.7
    
    metrics["reliability_score"] = reliability_score
    
    # 6. ì¢…í•© ì ìˆ˜
    weights = {
        "length_score": 0.2,
        "keyword_coverage": 0.3,
        "source_utilization": 0.2,
        "structure_score": 0.2,
        "reliability_score": 0.1
    }
    
    overall_score = sum(metrics[key] * weights[key] for key in weights)
    metrics["overall_score"] = overall_score
    
    return metrics

def compare_chain_outputs(query: str, results: Dict[str, str]) -> Dict[str, Any]:
    """
    ğŸ“‹ ê¸°ëŠ¥: ì—¬ëŸ¬ ì²´ì¸ ì „ëµ ê²°ê³¼ ë¹„êµ ë¶„ì„
    ğŸ“¥ ì…ë ¥: ì§ˆì˜, ì „ëµë³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    ğŸ“¤ ì¶œë ¥: ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸
    """
    
    comparison = {
        "query": query,
        "strategies_compared": list(results.keys()),
        "detailed_analysis": {}
    }
    
    for strategy, result in results.items():
        if result.get('success', False):
            answer = result['answer']
            sources = result.get('source_documents', [])
            
            # ê°œë³„ í‰ê°€
            quality_metrics = evaluate_answer_comprehensiveness(answer, query, sources)
            
            comparison["detailed_analysis"][strategy] = {
                "answer_preview": answer[:100] + "...",
                "word_count": len(answer.split()),
                "source_count": len(sources),
                "execution_time": result.get('execution_time', 0),
                "quality_metrics": quality_metrics
            }
    
    # ìµœê³  ì„±ëŠ¥ ì „ëµ ì‹ë³„
    if comparison["detailed_analysis"]:
        best_quality = max(
            comparison["detailed_analysis"].items(),
            key=lambda x: x[1]["quality_metrics"]["overall_score"]
        )
        
        fastest = min(
            comparison["detailed_analysis"].items(),
            key=lambda x: x[1]["execution_time"]
        )
        
        comparison["recommendations"] = {
            "best_quality": best_quality[0],
            "fastest": fastest[0],
            "quality_score": best_quality[1]["quality_metrics"]["overall_score"],
            "fastest_time": fastest[1]["execution_time"]
        }
    
    return comparison
```

## ğŸ§ª ì‹¤ìŠµ ê³¼ì œ

### ğŸ”¨ ê¸°ë³¸ ê³¼ì œ
1. **ì „ëµë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**: ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•œ 4ê°€ì§€ ì „ëµì˜ ì„±ëŠ¥ ì¸¡ì •
```python
# TODO: ë‹¤ì–‘í•œ íƒ€ì…ì˜ ì§ˆë¬¸ìœ¼ë¡œ ì „ëµë³„ ì„±ëŠ¥ ì¸¡ì •
question_types = {
    "factual": "Winston SmithëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
    "analytical": "1984ë…„ ì†Œì„¤ì˜ ê°ì‹œ ì‚¬íšŒë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    "comparative": "Ministry of Loveì™€ Truthë¥¼ ë¹„êµí•´ì£¼ì„¸ìš”",
    "summarization": "ì†Œì„¤ì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”"
}
# íŒíŠ¸: ì‹¤í–‰ ì‹œê°„, ë‹µë³€ í’ˆì§ˆ, ë¹„ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€
```

2. **Retriever ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ê²€ìƒ‰ ë§¤ê°œë³€ìˆ˜ì— ë”°ë¥¸ ê²°ê³¼ í’ˆì§ˆ ë³€í™” ë¶„ì„
```python
# TODO: kê°’, score_threshold ë“± ë§¤ê°œë³€ìˆ˜ ë³€ê²½ì— ë”°ë¥¸ ì˜í–¥ ë¶„ì„
retriever_configs = [
    {"k": 2}, {"k": 4}, {"k": 8},
    {"k": 4, "score_threshold": 0.7},
    {"k": 4, "score_threshold": 0.8}
]
```

### ğŸš€ ì‹¬í™” ê³¼ì œ
3. **ì ì‘í˜• ì „ëµ ì„ íƒê¸°**: ì§ˆë¬¸ ìœ í˜•ì„ ìë™ ë¶„ë¥˜í•˜ì—¬ ìµœì  ì „ëµ ì„ íƒ
```python
# TODO: ì§ˆë¬¸ ë¶„ì„ì„ í†µí•œ ì§€ëŠ¥í˜• ì „ëµ ì„ íƒ ì‹œìŠ¤í…œ
class IntelligentStrategySelector:
    def analyze_question_complexity(self, query): pass
    def detect_question_type(self, query): pass  
    def select_optimal_strategy(self, query): pass
```

4. **í•˜ì´ë¸Œë¦¬ë“œ ì²´ì¸**: ì—¬ëŸ¬ ì „ëµì˜ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ëŠ” ì•™ìƒë¸” ë°©ë²•
```python
# TODO: ë‹¤ì¤‘ ì „ëµ ê²°ê³¼ë¥¼ ì¡°í•©í•˜ì—¬ ë” ë‚˜ì€ ë‹µë³€ ìƒì„±
class EnsembleQAChain:
    def combine_results(self, results): pass
    def weighted_voting(self, answers, weights): pass
    def confidence_based_selection(self, results): pass
```

### ğŸ’¡ ì°½ì˜ ê³¼ì œ
5. **ë™ì  ë¬¸ì„œ í•„í„°ë§**: ì§ˆë¬¸ ê´€ë ¨ì„±ì— ë”°ë¥¸ ì‹¤ì‹œê°„ ë¬¸ì„œ í•„í„°ë§
```python
# TODO: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ ì¬í‰ê°€í•˜ì—¬ í’ˆì§ˆ í–¥ìƒ
class DynamicDocumentFilter:
    def calculate_relevance_score(self, doc, query): pass
    def filter_low_relevance_docs(self, docs, threshold): pass
    def rerank_documents(self, docs, query): pass
```

6. **ë¹„ìš© ìµœì í™” ì‹œìŠ¤í…œ**: ì˜ˆì‚° ì œí•œ ë‚´ì—ì„œ ìµœì  ì „ëµ ì„ íƒ
```python
# TODO: ì˜ˆì‚° ì œì•½ ì¡°ê±´ í•˜ì—ì„œ ìµœì ì˜ í’ˆì§ˆ/ë¹„ìš© ê· í˜•ì  ì°¾ê¸°
class CostOptimizedQA:
    def estimate_strategy_cost(self, strategy, docs): pass
    def select_within_budget(self, budget, strategies): pass
    def optimize_quality_per_dollar(self, strategies): pass
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### ë¹„ìš© ê´€ë¦¬
```python
# âŒ ë¹„íš¨ìœ¨ì ì¸ ë°©ë²•: ëª¨ë“  ì „ëµì„ ë§¤ë²ˆ ì‹œë„
def expensive_qa_approach(query):
    strategies = ["stuff", "refine", "map_reduce", "map_rerank"]
    results = []
    for strategy in strategies:  # 4ë°° ë¹„ìš©!
        chain = RetrievalQA.from_chain_type(llm=llm, chain_type=strategy, retriever=retriever)
        result = chain({"query": query})
        results.append(result)
    return results

# âœ… íš¨ìœ¨ì ì¸ ë°©ë²•: ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ì ì‘í˜• ì„ íƒ
def cost_effective_qa_approach(query):
    # ì§ˆë¬¸ ë¶„ì„
    if len(query.split()) > 20:
        strategy = "refine"  # ë³µì¡í•œ ì§ˆë¬¸ë§Œ refine ì‚¬ìš©
    else:
        strategy = "stuff"   # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ stuff
    
    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=strategy, retriever=retriever)
    return chain({"query": query})
```

### ì„±ëŠ¥ ìµœì í™”
- **í† í° ì œí•œ**: Stuff ì „ëµ ì‚¬ìš©ì‹œ ë¬¸ì„œ ìˆ˜ ì œí•œ í•„ìš”
- **ìºì‹œ í™œìš©**: ë™ì¼í•œ ì§ˆë¬¸ì˜ ë°˜ë³µ ì²˜ë¦¬ì‹œ ê²°ê³¼ ìºì‹±
- **ë³‘ë ¬ ì²˜ë¦¬**: Map-Reduceì—ì„œ ë¬¸ì„œë³„ ë³‘ë ¬ ì²˜ë¦¬ ê³ ë ¤

### ì „ëµë³„ íŠ¹ì„± ì´í•´
```python
# ì „ëµë³„ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê°€ì´ë“œ
strategy_use_cases = {
    "stuff": {
        "ì í•©í•œ_ìƒí™©": ["ê°„ë‹¨í•œ ì§ˆë¬¸", "ì ì€ ë¬¸ì„œ", "ë¹ ë¥¸ ì‘ë‹µ í•„ìš”"],
        "ë¶€ì í•©í•œ_ìƒí™©": ["ë³µì¡í•œ ë¶„ì„", "ë§ì€ ë¬¸ì„œ", "í† í° ì œí•œ ì´ˆê³¼"],
        "ë¹„ìš©": "ë‚®ìŒ",
        "ì •í™•ë„": "ë³´í†µ"
    },
    "refine": {
        "ì í•©í•œ_ìƒí™©": ["ë³µì¡í•œ ë¶„ì„", "ì ì§„ì  ê°œì„ ", "ë†’ì€ í’ˆì§ˆ"],
        "ë¶€ì í•©í•œ_ìƒí™©": ["ê°„ë‹¨í•œ ì§ˆë¬¸", "ë¹ ë¥¸ ì‘ë‹µ", "ë¹„ìš© ë¯¼ê°"],
        "ë¹„ìš©": "ë†’ìŒ",
        "ì •í™•ë„": "ë†’ìŒ"
    },
    "map_reduce": {
        "ì í•©í•œ_ìƒí™©": ["ë§ì€ ë¬¸ì„œ", "ìš”ì•½ ì‘ì—…", "ë³‘ë ¬ ì²˜ë¦¬"],
        "ë¶€ì í•©í•œ_ìƒí™©": ["ë¬¸ë§¥ ì—°ê²°", "ì„¸ë¶€ ë¶„ì„", "ì‹¤ì‹œê°„ ì²˜ë¦¬"],
        "ë¹„ìš©": "ë†’ìŒ",
        "ì •í™•ë„": "ë³´í†µ"
    },
    "map_rerank": {
        "ì í•©í•œ_ìƒí™©": ["ì‹ ë¢°ë„ ì¤‘ìš”", "ì—¬ëŸ¬ ë‹µë³€ ë¹„êµ", "í’ˆì§ˆ ê²€ì¦"],
        "ë¶€ì í•©í•œ_ìƒí™©": ["ë‹¨ìˆœ ì‚¬ì‹¤ ì§ˆë¬¸", "ë¹ ë¥¸ ì²˜ë¦¬", "ë¹„ìš© ì ˆì•½"],
        "ë¹„ìš©": "ë§¤ìš° ë†’ìŒ",
        "ì •í™•ë„": "ë§¤ìš° ë†’ìŒ"
    }
}
```

### í’ˆì§ˆ ë³´ì¥
- **"ëª¨ë¥´ê² ë‹¤" ë‹µë³€ í—ˆìš©**: ì˜ëª»ëœ ì •ë³´ë³´ë‹¤ ë¶ˆí™•ì‹¤ì„± í‘œí˜„ì´ ì¤‘ìš”
- **ì†ŒìŠ¤ ê²€ì¦**: ë°˜í™˜ëœ ë¬¸ì„œë“¤ì˜ ê´€ë ¨ì„± í™•ì¸
- **ë‹µë³€ ì¼ê´€ì„±**: ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•œ ì¼ê´€ëœ ë‹µë³€ í™•ë³´

## ğŸ”— ê´€ë ¨ ìë£Œ
- **ì´ì „ í•™ìŠµ**: [6.4 Vector Stores](./6.4_Vector_Stores.md)
- **ë‹¤ìŒ í•™ìŠµ**: [6.8 Stuff LCEL Chain](./6.8_Stuff_LCEL_Chain.md)
- **ì°¸ê³  ë¬¸ì„œ**: [LangChain RetrievalQA](https://python.langchain.com/docs/modules/chains/popular/vector_db_qa)
- **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: [LangSmith](https://docs.smith.langchain.com/)
- **ì‹¤ìŠµ íŒŒì¼**: [6.6 RetrievalQA.ipynb](../../00%20lecture/6.6%20RetrievalQA.ipynb)

---

ğŸ’¡ **í•µì‹¬ ì •ë¦¬**: RetrievalQA Chainì€ ë¬¸ì„œ ê²€ìƒ‰ê³¼ ì§ˆì˜ì‘ë‹µì„ ìë™í™”í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. 4ê°€ì§€ ë¬¸ì„œ ì²˜ë¦¬ ì „ëµ(Stuff, Refine, Map-Reduce, Map-Rerank)ì€ ê°ê° ë‹¤ë¥¸ ìƒí™©ì— ìµœì í™”ë˜ì–´ ìˆì–´, ì§ˆë¬¸ ìœ í˜•ê³¼ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ì ì ˆí•œ ì „ëµì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë¹„ìš©, ì„±ëŠ¥, ì •í™•ë„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì´í•´í•˜ê³  ìƒí™©ì— ë§ëŠ” ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.