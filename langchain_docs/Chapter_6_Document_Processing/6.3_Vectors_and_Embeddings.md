# ğŸ“– Section 6.3: Vectors and Embeddings - í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ë§ˆë²•

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- âœ… ì„ë² ë”©(Embedding)ì˜ ê°œë…ê³¼ ë²¡í„°í™”(Vectorization)ì˜ ì›ë¦¬ ì™„ì „ ì´í•´
- âœ… 3ì°¨ì› ë²¡í„° ì˜ˆì œë¥¼ í†µí•œ ì˜ë¯¸ì  ìœ ì‚¬ì„±ê³¼ ë²¡í„° ì—°ì‚° í•™ìŠµ
- âœ… OpenAI Embeddingsì˜ ê³ ì°¨ì› ë²¡í„° ê³µê°„ê³¼ ì‹¤ì œ í™œìš© ë°©ë²• ìŠµë“
- âœ… ë²¡í„° ê²€ìƒ‰ê³¼ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ë™ì‘ ì›ë¦¬ ì´í•´

## ğŸ§  í•µì‹¬ ê°œë…

### ì„ë² ë”©(Embedding)ì´ë€?
**ì„ë² ë”©**ì€ ì¸ê°„ì´ ì½ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ëŠ” ìˆ«ì(ë²¡í„°)ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜ë©ë‹ˆë‹¤.

```mermaid
graph LR
    A["í…ìŠ¤íŠ¸: 'King'"] --> B[ì„ë² ë”© ëª¨ë¸]
    B --> C["ë²¡í„°: [0.9, 0.1, 1.0]"]
    D["í…ìŠ¤íŠ¸: 'Queen'"] --> B
    B --> E["ë²¡í„°: [0.1, 0.9, 1.0]"]
    
    style A fill:#E6F3FF
    style C fill:#FFE6CC  
    style D fill:#E6F3FF
    style E fill:#FFE6CC
```

### ë²¡í„° ì°¨ì›(Dimensions)ì˜ ì´í•´

#### 3ì°¨ì› ì˜ˆì œ: ì™•êµ­ì˜ ì„¸ê³„
ê°•ì˜ì—ì„œ ì‚¬ìš©í•œ 3ì°¨ì› ë²¡í„° ì˜ˆì œë¡œ ê°œë…ì„ ì´í•´í•´ë³´ê² ìŠµë‹ˆë‹¤.

| ì°¨ì› | ì˜ë¯¸ | ì„¤ëª… |
|------|------|------|
| **Masculinity (ë‚¨ì„±ì„±)** | 0.0 ~ 1.0 | ë‹¨ì–´ê°€ ê°€ì§„ ë‚¨ì„±ì  íŠ¹ì„±ì˜ ì •ë„ |
| **Femininity (ì—¬ì„±ì„±)** | 0.0 ~ 1.0 | ë‹¨ì–´ê°€ ê°€ì§„ ì—¬ì„±ì  íŠ¹ì„±ì˜ ì •ë„ |
| **Royalty (ì™•ì¡±ì„±)** | 0.0 ~ 1.0 | ë‹¨ì–´ê°€ ê°€ì§„ ì™•ì¡±/ê·€ì¡±ì  íŠ¹ì„±ì˜ ì •ë„ |

#### ë‹¨ì–´ë³„ ë²¡í„° ê°’ ì˜ˆì‹œ
```python
# ğŸ§  ê°œë…: ì˜ë¯¸ì  íŠ¹ì„±ì„ ìˆ˜ì¹˜ë¡œ í‘œí˜„
word_vectors = {
    "King":   [0.9, 0.1, 1.0],  # ë†’ì€ ë‚¨ì„±ì„±, ë‚®ì€ ì—¬ì„±ì„±, ìµœê³  ì™•ì¡±ì„±
    "Queen":  [0.1, 0.9, 1.0],  # ë‚®ì€ ë‚¨ì„±ì„±, ë†’ì€ ì—¬ì„±ì„±, ìµœê³  ì™•ì¡±ì„±
    "Man":    [0.9, 0.1, 0.0],  # ë†’ì€ ë‚¨ì„±ì„±, ë‚®ì€ ì—¬ì„±ì„±, ì™•ì¡±ì„± ì—†ìŒ
    "Woman":  [0.1, 0.9, 0.0],  # ë‚®ì€ ë‚¨ì„±ì„±, ë†’ì€ ì—¬ì„±ì„±, ì™•ì¡±ì„± ì—†ìŒ
    "Royal":  [0.0, 0.0, 1.0]   # ì„±ë³„ ì¤‘ë¦½ì , ìˆœìˆ˜í•œ ì™•ì¡±ì„±
}
```

### ë²¡í„° ì—°ì‚°ì˜ ë§ˆë²•: ë‹¨ì–´ ëŒ€ìˆ˜í•™(Word Algebra)

#### ê¸°ë³¸ ì—°ì‚° ì˜ˆì œ
```python
# ğŸ§  ê°œë…: ë²¡í„° ì—°ì‚°ìœ¼ë¡œ ìƒˆë¡œìš´ ì˜ë¯¸ ë„ì¶œ

# King - Man = ?
king_vector = [0.9, 0.1, 1.0]
man_vector = [0.9, 0.1, 0.0]
result = [0.9-0.9, 0.1-0.1, 1.0-0.0]  # [0.0, 0.0, 1.0]
# ê²°ê³¼: Royal (ìˆœìˆ˜í•œ ì™•ì¡±ì„±ë§Œ ë‚¨ìŒ)

# Royal + Woman = ?
royal_vector = [0.0, 0.0, 1.0]
woman_vector = [0.1, 0.9, 0.0]
result = [0.0+0.1, 0.0+0.9, 1.0+0.0]  # [0.1, 0.9, 1.0]
# ê²°ê³¼: Queen (ì—¬ì„± + ì™•ì¡± = ì—¬ì™•)
```

### ê³ ì°¨ì› ë²¡í„°ì˜ ì‹¤ì œ

#### OpenAI Embeddings íŠ¹ì„±
- **ì°¨ì› ìˆ˜**: ~1,536ì°¨ì› (text-embedding-ada-002)
- **í‘œí˜„ ê°€ëŠ¥ ê°œë…**: ì •ì¹˜ì„±, ê°ì •, ì‹œê°„ì„±, ê¸°ìˆ ì„±, ë¬¸í™”ì„± ë“± ìˆ˜ì²œ ê°€ì§€
- **ì •í™•ë„**: ì¸ê°„ ìˆ˜ì¤€ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„± íŒë‹¨

```python
# ì‹¤ì œ OpenAI ì„ë² ë”© ì˜ˆì œ
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# ê° í…ìŠ¤íŠ¸ëŠ” 1,536ê°œ ìˆ«ìë¡œ ë³€í™˜ë¨
king_embedding = embeddings.embed_query("King")      # [0.234, -0.567, 0.123, ...]
queen_embedding = embeddings.embed_query("Queen")    # [0.189, -0.423, 0.145, ...]

print(f"King ë²¡í„° ì°¨ì›: {len(king_embedding)}")     # 1536
print(f"Queen ë²¡í„° ì°¨ì›: {len(queen_embedding)}")    # 1536
```

## ğŸ“‹ ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤

### OpenAIEmbeddings
```python
from langchain.embeddings import OpenAIEmbeddings

class OpenAIEmbeddings:
    def __init__(
        self,
        model: str = "text-embedding-ada-002",    # ğŸ“Œ ìš©ë„: ì„ë² ë”© ëª¨ë¸ ì„ íƒ
        openai_api_key: str = None,               # ğŸ“Œ ìš©ë„: OpenAI API í‚¤
        chunk_size: int = 1000                    # ğŸ“Œ ìš©ë„: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
    ):
        """
        OpenAIì˜ ì„ë² ë”© APIë¥¼ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ìƒì„±ê¸°
        
        ğŸ’° ë¹„ìš©: $0.0001 per 1K tokens (text-embedding-ada-002 ê¸°ì¤€)
        """
    
    def embed_query(self, text: str) -> List[float]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        ğŸ“¥ ì…ë ¥: ë¬¸ìì—´ í…ìŠ¤íŠ¸
        ğŸ“¤ ì¶œë ¥: 1,536ì°¨ì› ë²¡í„° (ë¦¬ìŠ¤íŠ¸)
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ì‚¬ìš©ì ì§ˆë¬¸ ì„ë² ë”©
        """
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ë²¡í„°í™”
        ğŸ“¥ ì…ë ¥: ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        ğŸ“¤ ì¶œë ¥: ë²¡í„° ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ë¬¸ì„œ ì»¬ë ‰ì…˜ ì„ë² ë”©
        """
```

### CacheBackedEmbeddings (ë¹„ìš© ì ˆì•½)
```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore

class CacheBackedEmbeddings:
    def __init__(
        self,
        underlying_embeddings,                     # ğŸ“Œ ìš©ë„: ì‹¤ì œ ì„ë² ë”© ëª¨ë¸
        document_embedding_cache,                  # ğŸ“Œ ìš©ë„: ìºì‹œ ì €ì¥ì†Œ
        namespace: str = ""                        # ğŸ“Œ ìš©ë„: ìºì‹œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤
    ):
        """
        ìºì‹±ì„ í†µí•œ ë¹„ìš© ìµœì í™” ì„ë² ë”© ë˜í¼
        
        ğŸ’° ì ˆì•½ íš¨ê³¼: ì¤‘ë³µ í…ìŠ¤íŠ¸ ì¬ê³„ì‚° ë°©ì§€ë¡œ ìµœëŒ€ 90% ë¹„ìš© ì ˆì•½
        """
```

## ğŸ”§ ë™ì‘ ê³¼ì • ìƒì„¸

### 1ë‹¨ê³„: ë²¡í„° ìœ ì‚¬ì„± ì´í•´
```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# === 3ì°¨ì› ë²¡í„°ë¡œ ìœ ì‚¬ì„± ê³„ì‚° ì‹¤ìŠµ ===
# ğŸ§  ê°œë…: ë²¡í„° ê°„ ê±°ë¦¬ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ì„± ì¸¡ì •

# ì™•êµ­ ë‹¨ì–´ë“¤ì˜ 3ì°¨ì› ë²¡í„°
vectors = {
    "King":   np.array([0.9, 0.1, 1.0]),
    "Queen":  np.array([0.1, 0.9, 1.0]), 
    "Man":    np.array([0.9, 0.1, 0.0]),
    "Woman":  np.array([0.1, 0.9, 0.0]),
    "Knight": np.array([0.9, 0.2, 0.7])  # ë‚¨ì„±ì ì´ê³  ì–´ëŠì •ë„ ì™•ì¡±ì 
}

print("ğŸ” ë²¡í„° ìœ ì‚¬ì„± ë¶„ì„:")
print("-" * 50)

# Kingê³¼ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ ìœ ì‚¬ì„± ê³„ì‚°
king_vector = vectors["King"].reshape(1, -1)

for word, vector in vectors.items():
    if word != "King":
        similarity = cosine_similarity(king_vector, vector.reshape(1, -1))[0][0]
        print(f"King â†” {word:6}: {similarity:.3f}")

# ì˜ˆìƒ ê²°ê³¼:
# King â†” Queen : 0.316 (ì™•ì¡±ì„±ì€ ê°™ì§€ë§Œ ì„±ë³„ì´ ë°˜ëŒ€)
# King â†” Man   : 0.900 (ì„±ë³„ì€ ê°™ì§€ë§Œ ì™•ì¡±ì„± ì°¨ì´)
# King â†” Woman : 0.100 (ì„±ë³„ê³¼ ì™•ì¡±ì„± ëª¨ë‘ ë‹¤ë¦„)
# King â†” Knight: 0.940 (ë§¤ìš° ìœ ì‚¬í•¨)
```

### 2ë‹¨ê³„: OpenAI ì„ë² ë”© ì‹¤ì „ ì‚¬ìš©
```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
import numpy as np

# === ì‹¤ì œ ë¬¸ì„œ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ===

# ğŸ”§ 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”© ë° ë¶„í• 
loader = UnstructuredFileLoader("./files/chapter_one.docx")
documents = loader.load()

splitter = CharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=600,
    chunk_overlap=100,
    model_name="gpt-3.5-turbo"
)

split_docs = splitter.split_documents(documents)
print(f"ğŸ“Š ë¶„í• ëœ ë¬¸ì„œ: {len(split_docs)}ê°œ ì²­í¬")

# ğŸ”§ 2ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002",
    chunk_size=1000  # ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ë¹„ìš© ìµœì í™”)
)

# ğŸ”§ 3ë‹¨ê³„: ë¬¸ì„œ ì„ë² ë”© ìƒì„±
print("ğŸš€ ì„ë² ë”© ìƒì„± ì‹œì‘...")

# í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
texts = [doc.page_content for doc in split_docs]

# ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„± (íš¨ìœ¨ì )
doc_embeddings = embeddings.embed_documents(texts)

print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
print(f"ğŸ“Š ìƒì„±ëœ ë²¡í„° ìˆ˜: {len(doc_embeddings)}")
print(f"ğŸ“Š ê° ë²¡í„° ì°¨ì›: {len(doc_embeddings[0])}")

# ğŸ”§ 4ë‹¨ê³„: ì„ë² ë”© í’ˆì§ˆ í™•ì¸
first_chunk = texts[0][:100] + "..."
first_embedding = doc_embeddings[0]

print(f"\nğŸ” ì²« ë²ˆì§¸ ì²­í¬ ì„ë² ë”© í™•ì¸:")
print(f"í…ìŠ¤íŠ¸: {first_chunk}")
print(f"ë²¡í„° ì˜ˆì‹œ: [{first_embedding[0]:.4f}, {first_embedding[1]:.4f}, {first_embedding[2]:.4f}, ...]")
print(f"ë²¡í„° í¬ê¸°: {np.linalg.norm(first_embedding):.4f}")
```

### 3ë‹¨ê³„: ë¹„ìš© ìµœì í™” ìºì‹±
```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
import os

# === ìºì‹±ì„ í†µí•œ ë¹„ìš© ì ˆì•½ ===
# ğŸ’° ì¤‘ìš”: ë™ì¼í•œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ë²ˆ ì„ë² ë”©í•˜ì§€ ì•Šë„ë¡ ìºì‹± ì‚¬ìš©

# ğŸ”§ 1ë‹¨ê³„: ìºì‹œ ì €ì¥ì†Œ ì„¤ì •
cache_dir = "./cache/embeddings"
os.makedirs(cache_dir, exist_ok=True)

file_store = LocalFileStore(cache_dir)

# ğŸ”§ 2ë‹¨ê³„: ìºì‹œ ì§€ì› ì„ë² ë”© ìƒì„±
underlying_embeddings = OpenAIEmbeddings(
    model="text-embedding-ada-002"
)

cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings=underlying_embeddings,
    document_embedding_cache=file_store,
    namespace="chapter_embeddings"  # í”„ë¡œì íŠ¸ë³„ êµ¬ë¶„
)

print("ğŸ’° ìºì‹± ì‹œìŠ¤í…œ í™œì„±í™”!")

# ğŸ”§ 3ë‹¨ê³„: ì²« ë²ˆì§¸ ì‹¤í–‰ (ì‹¤ì œ API í˜¸ì¶œ)
print("1ï¸âƒ£ ì²« ë²ˆì§¸ ì„ë² ë”© ìƒì„±...")
start_time = time.time()

embeddings_1 = cached_embeddings.embed_documents(texts[:5])  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
first_time = time.time() - start_time

print(f"â±ï¸ ì²« ë²ˆì§¸ ì‹¤í–‰ ì‹œê°„: {first_time:.2f}ì´ˆ")
print(f"ğŸ’¸ API í˜¸ì¶œ: {len(texts[:5])}ë²ˆ")

# ğŸ”§ 4ë‹¨ê³„: ë‘ ë²ˆì§¸ ì‹¤í–‰ (ìºì‹œì—ì„œ ë¡œë”©)
print("\n2ï¸âƒ£ ë™ì¼í•œ í…ìŠ¤íŠ¸ ì¬ì„ë² ë”©...")
start_time = time.time()

embeddings_2 = cached_embeddings.embed_documents(texts[:5])  # ê°™ì€ í…ìŠ¤íŠ¸
second_time = time.time() - start_time

print(f"â±ï¸ ë‘ ë²ˆì§¸ ì‹¤í–‰ ì‹œê°„: {second_time:.2f}ì´ˆ")
print(f"ğŸ’¸ API í˜¸ì¶œ: 0ë²ˆ (ìºì‹œ ì‚¬ìš©)")
print(f"ğŸ“ˆ ì†ë„ í–¥ìƒ: {(first_time / second_time):.1f}ë°° ë¹ ë¦„")

# ìºì‹œ ì •í™•ì„± í™•ì¸
vectors_match = np.array_equal(embeddings_1[0], embeddings_2[0])
print(f"ğŸ” ìºì‹œ ì •í™•ì„±: {'âœ… ì •í™•' if vectors_match else 'âŒ ì˜¤ë¥˜'}")
```

## ğŸ’» ì‹¤ì „ ì˜ˆì œ

### ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„
```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from langchain.embeddings import OpenAIEmbeddings
from typing import List, Tuple

class SemanticSearchEngine:
    """
    ğŸ¯ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ ì—”ì§„
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ë²¡í„° ìœ ì‚¬ì„± ê¸°ë°˜ ê²€ìƒ‰
    - ë‹¤ì¤‘ ê²€ìƒ‰ ê²°ê³¼ ë­í‚¹
    - ìœ ì‚¬ì„± ì ìˆ˜ ì œê³µ
    """
    
    def __init__(self, embeddings_model=None):
        self.embeddings_model = embeddings_model or OpenAIEmbeddings()
        self.documents = []
        self.document_embeddings = []
        
    def add_documents(self, documents: List[str]):
        """
        ğŸ“‹ ê¸°ëŠ¥: ë¬¸ì„œ ì»¬ë ‰ì…˜ì— ë¬¸ì„œë“¤ì„ ì¶”ê°€í•˜ê³  ì„ë² ë”© ìƒì„±
        ğŸ“¥ ì…ë ¥: ë¬¸ì„œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
        """
        print(f"ğŸ“š ë¬¸ì„œ {len(documents)}ê°œ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # ê¸°ì¡´ ë¬¸ì„œì— ì¶”ê°€
        self.documents.extend(documents)
        
        # ìƒˆ ë¬¸ì„œë“¤ì˜ ì„ë² ë”© ìƒì„±
        new_embeddings = self.embeddings_model.embed_documents(documents)
        self.document_embeddings.extend(new_embeddings)
        
        print(f"âœ… ì´ {len(self.documents)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ")
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, float]]:
        """
        ğŸ“‹ ê¸°ëŠ¥: ì§ˆì˜ì— ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ê²€ìƒ‰
        ğŸ“¥ ì…ë ¥: ê²€ìƒ‰ ì§ˆì˜, ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
        ğŸ“¤ ì¶œë ¥: (ë¬¸ì„œ_í…ìŠ¤íŠ¸, ìœ ì‚¬ì„±_ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.document_embeddings:
            return []
        
        # ì§ˆì˜ ì„ë² ë”© ìƒì„±
        query_embedding = self.embeddings_model.embed_query(query)
        query_vector = np.array(query_embedding).reshape(1, -1)
        
        # ëª¨ë“  ë¬¸ì„œì™€ì˜ ìœ ì‚¬ì„± ê³„ì‚°
        doc_vectors = np.array(self.document_embeddings)
        similarities = cosine_similarity(query_vector, doc_vectors)[0]
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ì„ íƒ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for i in top_indices:
            similarity_score = similarities[i]
            document_text = self.documents[i]
            results.append((document_text, similarity_score))
        
        return results
    
    def find_similar_documents(self, doc_index: int, top_k: int = 3) -> List[Tuple[str, float]]:
        """
        ğŸ“‹ ê¸°ëŠ¥: íŠ¹ì • ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë‹¤ë¥¸ ë¬¸ì„œë“¤ ì°¾ê¸°
        ğŸ“¥ ì…ë ¥: ê¸°ì¤€ ë¬¸ì„œ ì¸ë±ìŠ¤, ë°˜í™˜í•  ê²°ê³¼ ìˆ˜  
        ğŸ“¤ ì¶œë ¥: ìœ ì‚¬í•œ ë¬¸ì„œë“¤ê³¼ ìœ ì‚¬ì„± ì ìˆ˜
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ì¶”ì²œ ì‹œìŠ¤í…œ, ê´€ë ¨ ë¬¸ì„œ ì œì•ˆ
        """
        if doc_index >= len(self.document_embeddings):
            return []
        
        # ê¸°ì¤€ ë¬¸ì„œì˜ ë²¡í„°
        base_vector = np.array(self.document_embeddings[doc_index]).reshape(1, -1)
        
        # ëª¨ë“  ë¬¸ì„œì™€ì˜ ìœ ì‚¬ì„± ê³„ì‚°
        doc_vectors = np.array(self.document_embeddings)
        similarities = cosine_similarity(base_vector, doc_vectors)[0]
        
        # ìê¸° ìì‹  ì œì™¸í•˜ê³  ìƒìœ„ kê°œ ì„ íƒ
        similarities[doc_index] = -1  # ìê¸° ìì‹  ì œì™¸
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        for i in top_indices:
            if similarities[i] > 0:  # ìœ íš¨í•œ ìœ ì‚¬ì„± ì ìˆ˜ë§Œ
                similarity_score = similarities[i]
                document_text = self.documents[i]
                results.append((document_text, similarity_score))
        
        return results

# === ì‚¬ìš© ì˜ˆì‹œ ===
# ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
search_engine = SemanticSearchEngine()

# ìƒ˜í”Œ ë¬¸ì„œë“¤ (ì‹¤ì œë¡œëŠ” ì•ì—ì„œ ë¶„í• í•œ ë¬¸ì„œ ì‚¬ìš©)
sample_documents = [
    "The king ruled the kingdom with wisdom and justice.",
    "The queen was beloved by all her subjects.",
    "The knight defended the castle bravely.",
    "The princess lived in a beautiful palace.",
    "Democracy is a form of government by the people.",
    "The computer processes data using algorithms.",
    "Machine learning algorithms can predict outcomes."
]

# ë¬¸ì„œ ì¶”ê°€ ë° ì„ë² ë”©
search_engine.add_documents(sample_documents)

print("\nğŸ” ì˜ë¯¸ì  ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
print("=" * 50)

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
queries = [
    "royal family",
    "government system", 
    "artificial intelligence"
]

for query in queries:
    print(f"\nğŸ“‹ ì§ˆì˜: '{query}'")
    results = search_engine.search(query, top_k=3)
    
    for i, (doc, score) in enumerate(results):
        print(f"  {i+1}. [{score:.3f}] {doc[:50]}...")

# ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸° í…ŒìŠ¤íŠ¸
print(f"\nğŸ”— ì²« ë²ˆì§¸ ë¬¸ì„œì™€ ìœ ì‚¬í•œ ë¬¸ì„œë“¤:")
similar_docs = search_engine.find_similar_documents(0, top_k=2)
for doc, score in similar_docs:
    print(f"  [{score:.3f}] {doc[:50]}...")
```

## ğŸ” ë³€ìˆ˜/í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…

### ë²¡í„° ìœ ì‚¬ì„± ì¸¡ì • í•¨ìˆ˜ë“¤

#### ì½”ì‚¬ì¸ ìœ ì‚¬ì„± (Cosine Similarity)
```python
def calculate_cosine_similarity(vector1: List[float], vector2: List[float]) -> float:
    """
    ğŸ“‹ ê¸°ëŠ¥: ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ì„± ê³„ì‚°
    ğŸ“¥ ì…ë ¥: ë‘ ê°œì˜ ë²¡í„° (ê°™ì€ ì°¨ì›)
    ğŸ“¤ ì¶œë ¥: -1.0 ~ 1.0 ë²”ìœ„ì˜ ìœ ì‚¬ì„± ì ìˆ˜
    ğŸ’¡ í•´ì„: 1.0=ì™„ì „ ë™ì¼, 0.0=ë¬´ê´€ë ¨, -1.0=ì™„ì „ ë°˜ëŒ€
    """
    import math
    
    # ë‚´ì  ê³„ì‚°
    dot_product = sum(a * b for a, b in zip(vector1, vector2))
    
    # ë²¡í„° í¬ê¸° ê³„ì‚°
    magnitude1 = math.sqrt(sum(a * a for a in vector1))
    magnitude2 = math.sqrt(sum(b * b for b in vector2))
    
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ì„± = ë‚´ì  / (í¬ê¸°1 * í¬ê¸°2)
    cosine_sim = dot_product / (magnitude1 * magnitude2)
    return cosine_sim

# ì‚¬ìš© ì˜ˆì‹œ
king_vector = [0.9, 0.1, 1.0]
queen_vector = [0.1, 0.9, 1.0]
similarity = calculate_cosine_similarity(king_vector, queen_vector)
print(f"King â†” Queen ìœ ì‚¬ì„±: {similarity:.3f}")  # ì•½ 0.316
```

#### ìœ í´ë¦¬ë“œ ê±°ë¦¬ (Euclidean Distance)
```python
def calculate_euclidean_distance(vector1: List[float], vector2: List[float]) -> float:
    """
    ğŸ“‹ ê¸°ëŠ¥: ë‘ ë²¡í„° ê°„ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
    ğŸ“¥ ì…ë ¥: ë‘ ê°œì˜ ë²¡í„° (ê°™ì€ ì°¨ì›)
    ğŸ“¤ ì¶œë ¥: 0.0 ì´ìƒì˜ ê±°ë¦¬ ê°’
    ğŸ’¡ í•´ì„: 0.0=ì™„ì „ ë™ì¼, ê°’ì´ í´ìˆ˜ë¡ ë” ë‹¤ë¦„
    """
    import math
    
    # ì°¨ì´ì˜ ì œê³±ë“¤ì˜ í•©
    squared_diffs = [(a - b) ** 2 for a, b in zip(vector1, vector2)]
    
    # ì œê³±ê·¼ì„ ì·¨í•´ì„œ ê±°ë¦¬ ê³„ì‚°
    distance = math.sqrt(sum(squared_diffs))
    return distance

# ì‚¬ìš© ì˜ˆì‹œ
distance = calculate_euclidean_distance(king_vector, queen_vector)
print(f"King â†” Queen ê±°ë¦¬: {distance:.3f}")  # ì•½ 1.131
```

### ì„ë² ë”© í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜ë“¤
```python
def evaluate_embedding_quality(embeddings_list: List[List[float]], 
                             labels: List[str] = None) -> dict:
    """
    ğŸ“‹ ê¸°ëŠ¥: ì„ë² ë”©ì˜ í’ˆì§ˆì„ ë‹¤ì–‘í•œ ì§€í‘œë¡œ í‰ê°€
    ğŸ“¥ ì…ë ¥: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸, ì„ íƒì  ë¼ë²¨
    ğŸ“¤ ì¶œë ¥: í’ˆì§ˆ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ í‰ê°€, ì°¨ì› ìˆ˜ ìµœì í™”
    """
    import numpy as np
    from sklearn.decomposition import PCA
    from sklearn.cluster import KMeans
    
    vectors = np.array(embeddings_list)
    
    # ê¸°ë³¸ í†µê³„
    mean_vector = np.mean(vectors, axis=0)
    std_vector = np.std(vectors, axis=0)
    
    # ì°¨ì›ë³„ ë¶„ì‚° ë¶„ì„
    variance_per_dim = np.var(vectors, axis=0)
    effective_dimensions = np.sum(variance_per_dim > 0.01)  # ì‹¤ì§ˆì  ì°¨ì› ìˆ˜
    
    # PCAë¡œ ì£¼ìš” ì„±ë¶„ ë¶„ì„
    pca = PCA()
    pca.fit(vectors)
    explained_variance_ratio = pca.explained_variance_ratio_
    
    # 95% ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ì°¨ì› ìˆ˜
    cumsum = np.cumsum(explained_variance_ratio)
    dims_95_percent = np.argmax(cumsum >= 0.95) + 1
    
    # í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ (ë¼ë²¨ì´ ìˆì„ ë•Œ)
    clustering_score = None
    if labels and len(set(labels)) > 1:
        kmeans = KMeans(n_clusters=len(set(labels)), random_state=42)
        cluster_labels = kmeans.fit_predict(vectors)
        from sklearn.metrics import adjusted_rand_score
        clustering_score = adjusted_rand_score(labels, cluster_labels)
    
    return {
        "vector_count": len(vectors),
        "vector_dimension": vectors.shape[1],
        "effective_dimensions": effective_dimensions,
        "mean_magnitude": np.linalg.norm(mean_vector),
        "std_magnitude": np.mean(np.std(vectors, axis=0)),
        "dims_for_95_percent": dims_95_percent,
        "first_pc_variance": explained_variance_ratio[0],
        "clustering_quality": clustering_score
    }

# ì‚¬ìš© ì˜ˆì‹œ
quality_report = evaluate_embedding_quality(doc_embeddings[:10])
print("ğŸ“Š ì„ë² ë”© í’ˆì§ˆ ë¦¬í¬íŠ¸:")
for metric, value in quality_report.items():
    if isinstance(value, float):
        print(f"  {metric}: {value:.4f}")
    else:
        print(f"  {metric}: {value}")
```

## ğŸ§ª ì‹¤ìŠµ ê³¼ì œ

### ğŸ”¨ ê¸°ë³¸ ê³¼ì œ
1. **3ì°¨ì› ë²¡í„° ì‹¤ìŠµ**: ì¶”ê°€ ë‹¨ì–´ë“¤ì„ 3ì°¨ì›ìœ¼ë¡œ ì„ë² ë”©í•˜ê³  ìœ ì‚¬ì„± ê³„ì‚°
```python
# TODO: ë‹¤ìŒ ë‹¨ì–´ë“¤ì„ 3ì°¨ì›(Masculinity, Femininity, Royalty)ìœ¼ë¡œ ë²¡í„°í™”
words_to_embed = ["Prince", "Princess", "Duke", "Duchess", "Soldier", "Maid"]
# íŒíŠ¸: ê° ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë¶„ì„í•˜ì—¬ 0.0~1.0 ë²”ìœ„ë¡œ ì ìˆ˜ ë¶€ì—¬
```

2. **ë²¡í„° ì—°ì‚° ì‹¤í—˜**: ë‹¤ì–‘í•œ ë‹¨ì–´ ì¡°í•©ìœ¼ë¡œ ë²¡í„° ì—°ì‚° ìˆ˜í–‰
```python
# TODO: King - Man + Woman = Queen ì™¸ì— ë‹¤ë¥¸ ì¡°í•©ë“¤ ì‹œë„í•´ë³´ê¸°
# ì˜ˆì‹œ: Prince - Boy + Girl = ?
# íŒíŠ¸: ì„±ë³„ê³¼ ì™•ì¡±ì„±ì˜ ë³€í™˜ íŒ¨í„´ í™•ì¸
```

### ğŸš€ ì‹¬í™” ê³¼ì œ
3. **ì‹¤ì œ ì„ë² ë”© ìœ ì‚¬ì„± ë¶„ì„**: OpenAI ì„ë² ë”©ìœ¼ë¡œ ë‹¨ì–´ ê´€ê³„ ë¶„ì„
```python
# TODO: ì‹¤ì œ OpenAI ì„ë² ë”©ìœ¼ë¡œ ë‹¨ì–´ë“¤ì˜ ìœ ì‚¬ì„± ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
word_list = ["King", "Queen", "President", "Minister", "CEO", "Manager"]
# íŒíŠ¸: ëª¨ë“  ë‹¨ì–´ ìŒì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì„ ê³„ì‚°í•˜ì—¬ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
```

4. **ë¬¸ì„œ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§**: ë¬¸ì„œë“¤ì„ ë²¡í„° ìœ ì‚¬ì„±ìœ¼ë¡œ ê·¸ë£¹í™”
```python
# TODO: ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ë“¤ì„ K-meansë¡œ í´ëŸ¬ìŠ¤í„°ë§
from sklearn.cluster import KMeans
# íŒíŠ¸: ì„ë² ë”©ì„ ì°¨ì› ì¶•ì†Œ í›„ ì‹œê°í™”í•˜ì—¬ í´ëŸ¬ìŠ¤í„° í™•ì¸
```

### ğŸ’¡ ì°½ì˜ ê³¼ì œ
5. **ë§ì¶¤í˜• ìœ ì‚¬ì„± ì²™ë„**: íŠ¹ì • ë„ë©”ì¸ì— ìµœì í™”ëœ ìœ ì‚¬ì„± ì¸¡ì • ë°©ë²•
```python
# TODO: ê¸°ìˆ  ë¬¸ì„œì— íŠ¹í™”ëœ ìœ ì‚¬ì„± ì²™ë„ ê°œë°œ
def technical_similarity(embedding1, embedding2, domain_weights):
    """ê¸°ìˆ ì  ìœ ì‚¬ì„±ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘” ì»¤ìŠ¤í…€ ìœ ì‚¬ì„± í•¨ìˆ˜"""
    pass
```

6. **ì„ë² ë”© í’ˆì§ˆ ìµœì í™”**: ì²­í¬ í¬ê¸°ì— ë”°ë¥¸ ì„ë² ë”© í’ˆì§ˆ ë¶„ì„
```python
# TODO: ë‹¤ì–‘í•œ ì²­í¬ í¬ê¸°(200, 400, 600, 800ì)ë¡œ ì„ë² ë”© í’ˆì§ˆ ë¹„êµ
def optimize_chunk_size_for_embedding_quality(documents):
    """ì„ë² ë”© í’ˆì§ˆì„ ê·¹ëŒ€í™”í•˜ëŠ” ìµœì  ì²­í¬ í¬ê¸° ì°¾ê¸°"""
    pass
```

## âš ï¸ ì£¼ì˜ì‚¬í•­

### ë¹„ìš© ê´€ë¦¬
```python
# âŒ ë¹„íš¨ìœ¨ì ì¸ ë°©ë²•: ë§¤ë²ˆ ìƒˆë¡œ ì„ë² ë”© ìƒì„±
def inefficient_embedding(texts):
    embeddings = OpenAIEmbeddings()
    results = []
    for text in texts:
        result = embeddings.embed_query(text)  # ê°œë³„ API í˜¸ì¶œ
        results.append(result)
    return results

# âœ… íš¨ìœ¨ì ì¸ ë°©ë²•: ë°°ì¹˜ ì²˜ë¦¬ì™€ ìºì‹±
def efficient_embedding(texts):
    from langchain.embeddings import CacheBackedEmbeddings
    from langchain.storage import LocalFileStore
    
    # ìºì‹œ ì„¤ì •
    store = LocalFileStore("./cache/")
    underlying = OpenAIEmbeddings()
    cached = CacheBackedEmbeddings.from_bytes_store(underlying, store)
    
    # ë°°ì¹˜ë¡œ ì²˜ë¦¬
    return cached.embed_documents(texts)
```

### ë²¡í„° ì°¨ì›ê³¼ ì„±ëŠ¥
- **ê³ ì°¨ì›ì˜ ì €ì£¼**: ì°¨ì›ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ìœ ì‚¬ì„± êµ¬ë¶„ì´ ì–´ë ¤ì›Œì§
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 1,536ì°¨ì› ë²¡í„°ëŠ” ìƒë‹¹í•œ ë©”ëª¨ë¦¬ í•„ìš”
- **ê³„ì‚° ë³µì¡ë„**: ë²¡í„° ì—°ì‚° ì‹œê°„ì´ ì°¨ì›ì— ë¹„ë¡€

### ì„ë² ë”© ëª¨ë¸ ì„ íƒ
```python
# ëª¨ë¸ë³„ íŠ¹ì„± ë¹„êµ
embedding_models = {
    "text-embedding-ada-002": {
        "dimensions": 1536,
        "cost_per_1k_tokens": 0.0001,
        "quality": "ë†’ìŒ",
        "speed": "ë³´í†µ"
    },
    "text-embedding-3-small": {
        "dimensions": 1536,
        "cost_per_1k_tokens": 0.00002,
        "quality": "ë³´í†µ",
        "speed": "ë¹ ë¦„"
    },
    "text-embedding-3-large": {
        "dimensions": 3072,
        "cost_per_1k_tokens": 0.00013,
        "quality": "ë§¤ìš° ë†’ìŒ",
        "speed": "ëŠë¦¼"
    }
}
```

### ì–¸ì–´ë³„ ì„ë² ë”© í’ˆì§ˆ
- **ì˜ì–´**: ê°€ì¥ ë†’ì€ í’ˆì§ˆ (í›ˆë ¨ ë°ì´í„° í’ë¶€)
- **í•œêµ­ì–´**: ì–‘í˜¸í•œ í’ˆì§ˆ (ë©€í‹°ë§êµ¬ì–¼ ì§€ì›)
- **ì „ë¬¸ ìš©ì–´**: ë„ë©”ì¸ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ê³ ë ¤ í•„ìš”

## ğŸ”— ê´€ë ¨ ìë£Œ
- **ì´ì „ í•™ìŠµ**: [6.2 Tiktoken](./6.2_Tiktoken.md)
- **ë‹¤ìŒ í•™ìŠµ**: [6.4 Vector Stores](./6.4_Vector_Stores.md)
- **ì°¸ê³  ìë£Œ**: [Word2Vec Visualization](https://word2vec.mramohyeddin.me/)
- **ì¶”ì²œ ì˜ìƒ**: [Gustav SÃ¶derstrÃ¶m - Spotifyì˜ ë²¡í„° ê²€ìƒ‰](https://www.youtube.com/watch?v=example)
- **ì‹¤ìŠµ íŒŒì¼**: [6.3 Vectors and Embeddings.ipynb](../../00%20lecture/6.3%20Vectors.ipynb)

---

ğŸ’¡ **í•µì‹¬ ì •ë¦¬**: ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¥¼ ë³´ì¡´í•˜ëŠ” ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ë²¡í„° ê°„ ê±°ë¦¬ë¡œ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•  ìˆ˜ ìˆê³ , ë²¡í„° ì—°ì‚°ìœ¼ë¡œ ë‹¨ì–´ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œ OpenAI ì„ë² ë”©ì€ 1,536ì°¨ì›ì˜ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³µì¡í•œ ì˜ë¯¸ ê´€ê³„ë¥¼ í¬ì°©í•˜ë©°, RAG ì‹œìŠ¤í…œì—ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.