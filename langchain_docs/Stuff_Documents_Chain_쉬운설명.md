# ğŸ“– Stuff Documents Chain ì™„ë²½ ê°€ì´ë“œ

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- âœ… Stuff Documents Chainì˜ ê°œë…ê³¼ ì‘ë™ ì›ë¦¬ ì´í•´
- âœ… LCELì„ ì‚¬ìš©í•œ íˆ¬ëª…í•œ ì²´ì¸ êµ¬í˜„ ë°©ë²• ìŠµë“
- âœ… RunnablePassthroughì˜ ì—­í• ê³¼ í™œìš©ë²• ì´í•´
- âœ… ì–¸ì œ Stuff Chainì„ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨ ëŠ¥ë ¥ ìŠµë“

## ğŸ§  í•µì‹¬ ê°œë…

### Stuff Documents Chainì´ë€?
**Stuff Documents Chain**ì€ LangChainì—ì„œ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ **ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸** ë°©ë²•ì…ë‹ˆë‹¤.
- "Stuff" = ëª¨ë“  ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— **"ì‘¤ì…” ë„£ëŠ”ë‹¤(stuff)"**ëŠ” ì˜ë¯¸
- ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ **í•œêº¼ë²ˆì—** LLMì—ê²Œ ì „ë‹¬í•˜ëŠ” ë°©ì‹

### ê°œë… ê´€ê³„ë„
```mermaid
graph TD
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B[Retriever<br/>ë¬¸ì„œ ê²€ìƒ‰]
    B --> C[ê²€ìƒ‰ëœ ë¬¸ì„œë“¤<br/>Doc1, Doc2, Doc3...]
    C --> D[ëª¨ë“  ë¬¸ì„œë¥¼<br/>í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©]
    D --> E[í”„ë¡¬í”„íŠ¸ì—<br/>ì»¨í…ìŠ¤íŠ¸ + ì§ˆë¬¸ ì‚½ì…]
    E --> F[LLMì—<br/>ì „ì²´ ì „ë‹¬]
    F --> G[ìµœì¢… ë‹µë³€]
    
    style A fill:#E6F3FF
    style D fill:#FFE6CC
    style G fill:#E6FFE6
```

### ì–¸ì œ ì‚¬ìš©í•˜ëŠ”ê°€?
1. **ë¬¸ì„œê°€ ì ì„ ë•Œ** (ë³´í†µ 3-5ê°œ)
2. **ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘**ì´ í•„ìš”í•  ë•Œ
3. **ì¢…í•©ì ì¸ ì´í•´**ê°€ í•„ìš”í•œ ì§ˆë¬¸ì¼ ë•Œ
4. **ë‹¨ìˆœí•œ ì§ˆë¬¸-ë‹µë³€** ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œ

## ğŸ“‹ ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤

### RunnablePassthrough
```python
from langchain.schema.runnable import RunnablePassthrough

class RunnablePassthrough:
    def invoke(self, input_data):
        """
        ì…ë ¥ ë°ì´í„°ë¥¼ ìˆ˜ì • ì—†ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤
        
        Args:
            input_data: ì „ë‹¬í•  ë°ì´í„° (ë³´í†µ ì§ˆë¬¸ í…ìŠ¤íŠ¸)
        
        Returns:
            ë™ì¼í•œ ì…ë ¥ ë°ì´í„°
        """
        return input_data
```

**ğŸ“Œ ë§¤ê°œë³€ìˆ˜ ìƒì„¸**:
- `input_data` (required): ì „ë‹¬í•  ë°ì´í„°, ì£¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ í…ìŠ¤íŠ¸

**ğŸ“Œ ì£¼ìš” ìš©ë„**:
- ì²´ì¸ì—ì„œ ë™ì¼í•œ ì…ë ¥ì„ ì—¬ëŸ¬ ê³³ì—ì„œ ì‚¬ìš©í•  ë•Œ
- ì§ˆë¬¸ì„ ìˆ˜ì • ì—†ì´ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬í•  ë•Œ

### LCEL Chain êµ¬ì„±
```python
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI

# LCEL ë°©ì‹ì˜ ì²´ì¸ êµ¬ì„±
chain = (
    {
        "context": retriever,              # ë¬¸ì„œ ê²€ìƒ‰
        "question": RunnablePassthrough()  # ì§ˆë¬¸ ì „ë‹¬
    }
    | prompt    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
    | llm       # LLM í˜¸ì¶œ
)
```

## ğŸ”§ ë™ì‘ ê³¼ì • ìƒì„¸

### Step 1: í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ ì¤€ë¹„
```python
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(temperature=0.1)

# Vector Storeì™€ Retriever ìƒì„±
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(docs, embeddings)
retriever = vectorstore.as_retriever()
```

### Step 2: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
```python
prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        "You are a helpful assistant. Answer questions using only the following context. "
        "If you don't know the answer just say you don't know, don't make it up:\n\n{context}"
    ),
    ("human", "{question}"),
])
```

### Step 3: LCEL ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
```python
# ì²´ì¸ êµ¬ì„±
chain = (
    {
        "context": retriever,               # ğŸ“Œ ê²€ìƒ‰ê¸°ê°€ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        "question": RunnablePassthrough(),  # ğŸ“Œ ì§ˆë¬¸ì€ ê·¸ëŒ€ë¡œ ì „ë‹¬
    }
    | prompt   # ğŸ“Œ í”„ë¡¬í”„íŠ¸ì— contextì™€ question ì‚½ì…
    | llm      # ğŸ“Œ LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
)

# ì‹¤í–‰
response = chain.invoke("Describe Victory Mansions")
```

## ğŸ’» ì‹¤ì „ ì˜ˆì œ

### ì™„ì „í•œ Stuff Documents Chain êµ¬í˜„
```python
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough

# 1. LLM ì„¤ì •
llm = ChatOpenAI(
    temperature=0.1,  # ğŸ“Œ ë‚®ì€ temperatureë¡œ ì¼ê´€ëœ ë‹µë³€
)

# 2. ìºì‹œ ì„¤ì • (ì„ë² ë”© ì¬ì‚¬ìš©)
cache_dir = LocalFileStore("./.cache/")

# 3. ë¬¸ì„œ ë¶„í• ê¸° ì„¤ì •
splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n",
    chunk_size=600,      # ğŸ“Œ ì²­í¬ í¬ê¸°
    chunk_overlap=100,   # ğŸ“Œ ì²­í¬ ê°„ ê²¹ì¹¨
)

# 4. ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
loader = UnstructuredFileLoader("./files/chapter_one.txt")
docs = loader.load_and_split(text_splitter=splitter)

# 5. ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
embeddings = OpenAIEmbeddings()
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
vectorstore = FAISS.from_documents(docs, cached_embeddings)

# 6. Retriever ìƒì„±
retriever = vectorstore.as_retriever()

# 7. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant. Answer questions using only the following context. "
            "If you don't know the answer just say you don't know, don't make it up:\n\n{context}",
        ),
        ("human", "{question}"),
    ]
)

# 8. LCEL ì²´ì¸ êµ¬ì„±
chain = (
    {
        "context": retriever,
        "question": RunnablePassthrough(),
    }
    | prompt
    | llm
)

# 9. ì‚¬ìš© ì˜ˆì‹œ
result = chain.invoke("Describe Victory Mansions")
print(result.content)
```

## ğŸ” ë³€ìˆ˜/í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…

### ì£¼ìš” ë³€ìˆ˜ ì„¤ëª…
```python
temperature=0.1          # ğŸ“Œ ìš©ë„: ì°½ì˜ì„± ì œì–´, íƒ€ì…: float, ì˜ˆì‹œ: 0.0-2.0
chunk_size=600          # ğŸ“Œ ìš©ë„: ë¬¸ì„œ ë¶„í•  í¬ê¸°, íƒ€ì…: int, ì˜ˆì‹œ: 500-1000
chunk_overlap=100       # ğŸ“Œ ìš©ë„: ì²­í¬ ê°„ ê²¹ì¹¨, íƒ€ì…: int, ì˜ˆì‹œ: 50-200
```

### ì²´ì¸ ì‹¤í–‰ ê³¼ì • ìƒì„¸
```python
def explain_chain_execution(question: str):
    """
    ğŸ“‹ ê¸°ëŠ¥: Stuff Chainì˜ ì‹¤í–‰ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…
    ğŸ“¥ ì…ë ¥: ì‚¬ìš©ì ì§ˆë¬¸
    ğŸ“¤ ì¶œë ¥: ê° ë‹¨ê³„ë³„ ì²˜ë¦¬ ê²°ê³¼
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ë””ë²„ê¹… ë° ì´í•´ë„ í–¥ìƒ
    """
    # Step 1: Retrieverê°€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    relevant_docs = retriever.get_relevant_documents(question)
    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(relevant_docs)}")
    
    # Step 2: ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    context = "\n\n".join([doc.page_content for doc in relevant_docs])
    print(f"ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´: {len(context)} ë¬¸ì")
    
    # Step 3: í”„ë¡¬í”„íŠ¸ ìƒì„±
    formatted_prompt = prompt.format(context=context, question=question)
    print(f"ìµœì¢… í”„ë¡¬í”„íŠ¸:\n{formatted_prompt[:200]}...")
    
    return formatted_prompt
```

## ğŸ§ª ì‹¤ìŠµ ê³¼ì œ

### ğŸ”¨ ê¸°ë³¸ ê³¼ì œ
1. ì£¼ì–´ì§„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ë‹¤ì–‘í•œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
2. chunk_sizeì™€ chunk_overlap ê°’ì„ ë³€ê²½í•˜ë©° ê²°ê³¼ ë¹„êµ
3. ë‹¤ë¥¸ ë¬¸ì„œë¡œ ë²¡í„° ì €ì¥ì†Œ ìƒì„±í•´ë³´ê¸°

### ğŸš€ ì‹¬í™” ê³¼ì œ
1. ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ë¥¼ ì œí•œí•˜ëŠ” ê¸°ëŠ¥ ì¶”ê°€
```python
# íŒíŠ¸: retriever ì„¤ì • ë³€ê²½
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
```

2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì»¤ìŠ¤í„°ë§ˆì´ì§•
```python
# íŒíŠ¸: í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ ë˜ëŠ” íŠ¹ì • ë„ë©”ì¸ ì „ìš© í”„ë¡¬í”„íŠ¸ ì‘ì„±
```

### ğŸ’¡ ì°½ì˜ ê³¼ì œ
1. ë¬¸ì„œì˜ ê´€ë ¨ì„± ì ìˆ˜ë¥¼ í‘œì‹œí•˜ëŠ” ê¸°ëŠ¥ êµ¬í˜„
2. ë‹µë³€ê³¼ í•¨ê»˜ ì¶œì²˜ ë¬¸ì„œë¥¼ í‘œì‹œí•˜ëŠ” ì²´ì¸ êµ¬ì„±

## âš ï¸ ì£¼ì˜ì‚¬í•­

### í† í° ì œí•œ ë¬¸ì œ
- OpenAI ëª¨ë¸ì˜ í† í° ì œí•œ (gpt-3.5-turbo: 4,096 í† í°)
- ë¬¸ì„œê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥
- í•´ê²°ì±…: retrieverì˜ kê°’ ì¡°ì • ë˜ëŠ” Map Reduce Chain ì‚¬ìš©

### ë¹„ìš© ê³ ë ¤ì‚¬í•­
```python
# ë¹„ìš© ì ˆê° íŒ
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}  # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ ì œí•œ
)
```

### ì„±ëŠ¥ ìµœì í™”
- ìºì‹± í™œìš©ìœ¼ë¡œ ë°˜ë³µ ì„ë² ë”© ë°©ì§€
- ì ì ˆí•œ chunk_size ì„ íƒ (ë„ˆë¬´ í¬ë©´ í† í° ì´ˆê³¼, ë„ˆë¬´ ì‘ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±)

## ğŸ”— ê´€ë ¨ ìë£Œ
- **ì´ì „ í•™ìŠµ**: [6.6 RetrievalQA](./6.6_RetrievalQA.md)
- **ë‹¤ìŒ í•™ìŠµ**: [6.9 Map Reduce LCEL Chain](./6.9_Map_Reduce_LCEL_Chain.md) 
- **ì‹¤ìŠµ íŒŒì¼**: [6.8 Stuff LCEL Chain.ipynb](../00%20lecture/6.8%20Stuff%20LCEL%20Chain.ipynb)
- **ê³µì‹ ë¬¸ì„œ**: [LangChain LCEL Documentation](https://python.langchain.com/docs/expression_language/)

---

ğŸ’¡ **í•µì‹¬ ì •ë¦¬**: Stuff Documents Chainì€ ëª¨ë“  ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— ë„£ì–´ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹ì…ë‹ˆë‹¤. LCELì„ ì‚¬ìš©í•˜ë©´ íˆ¬ëª…í•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥í•œ ì²´ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì†Œê·œëª¨ ë¬¸ì„œ ì„¸íŠ¸ì—ì„œ ì¢…í•©ì ì¸ ë‹µë³€ì´ í•„ìš”í•  ë•Œ ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤.