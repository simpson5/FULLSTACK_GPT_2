# ğŸ“– Section 5.4: ConversationSummaryBufferMemory - í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬

## ğŸ¯ í•™ìŠµ ëª©í‘œ
- âœ… ConversationSummaryBufferMemoryì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ë°©ì‹ ì´í•´
- âœ… max_token_limit íŒŒë¼ë¯¸í„°ë¥¼ í†µí•œ í† í° ì„ê³„ê°’ ì œì–´ í•™ìŠµ
- âœ… ìµœê·¼ ë©”ì‹œì§€ëŠ” ë³´ì¡´, ì˜¤ë˜ëœ ë©”ì‹œì§€ëŠ” ìš”ì•½í•˜ëŠ” ì „ëµ í™œìš©
- âœ… ì‹¤ë¬´ì—ì„œ ìµœì ì˜ í† í° ì„ê³„ê°’ ì„¤ì • ë°©ë²• ìŠµë“

## ğŸ§  í•µì‹¬ ê°œë…

### ConversationSummaryBufferMemoryë€?
**ConversationSummaryBufferMemory**ëŠ” **BufferMemory**ì™€ **SummaryMemory**ì˜ ì¥ì ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ì…ë‹ˆë‹¤. ìµœê·¼ ëŒ€í™”ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , í† í° í•œê³„ë¥¼ ì´ˆê³¼í•˜ëŠ” ì˜¤ë˜ëœ ëŒ€í™”ëŠ” ìš”ì•½í•©ë‹ˆë‹¤.

```mermaid
graph TD
    A[ìƒˆë¡œìš´ ëŒ€í™”] --> B{í† í° í•œê³„ í™•ì¸}
    B -->|í•œê³„ ë‚´| C[Bufferì— ê·¸ëŒ€ë¡œ ì €ì¥]
    B -->|í•œê³„ ì´ˆê³¼| D[ì˜¤ë˜ëœ ë©”ì‹œì§€ ìš”ì•½ ìƒì„±]
    
    D --> E[Summary Section]
    C --> F[Buffer Section]
    
    subgraph "ë©”ëª¨ë¦¬ êµ¬ì¡°"
        E --> G[ì‹œìŠ¤í…œ: ì´ì „ ëŒ€í™” ìš”ì•½...]
        F --> H[Human: ìµœê·¼ ì§ˆë¬¸]
        F --> I[AI: ìµœê·¼ ë‹µë³€]
    end
    
    style E fill:#FFB6C1,stroke:#FF69B4,stroke-width:2px
    style F fill:#90EE90,stroke:#228B22,stroke-width:2px
```

### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ì˜ ì¥ì 

| íŠ¹ì§• | BufferMemory | SummaryMemory | SummaryBufferMemory |
|------|--------------|---------------|---------------------|
| **ìµœê·¼ ëŒ€í™”** | âœ… ì™„ì „ ë³´ì¡´ | âŒ ìš”ì•½ë§Œ | âœ… ì™„ì „ ë³´ì¡´ |
| **ì˜¤ë˜ëœ ëŒ€í™”** | âœ… ì™„ì „ ë³´ì¡´ | âœ… ìš”ì•½ ë³´ì¡´ | âœ… ìš”ì•½ ë³´ì¡´ |
| **í† í° íš¨ìœ¨ì„±** | âŒ ê³„ì† ì¦ê°€ | âœ… ì¼ì • ìœ ì§€ | âœ… ì œí•œëœ ì¦ê°€ |
| **ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ** | âœ… ìµœê³  | â­ ì¤‘ê°„ | ğŸ”¥ ìµœì  ê· í˜• |
| **ë¹„ìš© íš¨ìœ¨ì„±** | âŒ ë†’ì€ ë¹„ìš© | âœ… ì˜ˆì¸¡ ê°€ëŠ¥ | âœ… ì¤‘ê°„ ë¹„ìš© |

## ğŸ“‹ ì£¼ìš” í´ë˜ìŠ¤/í•¨ìˆ˜ ë ˆí¼ëŸ°ìŠ¤

### ConversationSummaryBufferMemory í´ë˜ìŠ¤
```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI

class ConversationSummaryBufferMemory:
    def __init__(
        self,
        llm: BaseLanguageModel,              # ğŸ“Œ ìš©ë„: ìš”ì•½ìš© LLM ëª¨ë¸, íƒ€ì…: Required
        max_token_limit: int = 2000,         # ğŸ“Œ ìš©ë„: í† í° ì„ê³„ê°’, íƒ€ì…: int, í•µì‹¬ íŒŒë¼ë¯¸í„°!
        return_messages: bool = False,       # ğŸ“Œ ìš©ë„: ë©”ì‹œì§€ ê°ì²´ ë°˜í™˜ ì—¬ë¶€
        memory_key: str = "history",         # ğŸ“Œ ìš©ë„: ë©”ëª¨ë¦¬ í‚¤ ì´ë¦„
        summarize_step: int = 2,            # ğŸ“Œ ìš©ë„: Nê°œì”© ë¬¶ì–´ì„œ ìš”ì•½
        moving_summary_buffer: str = ""      # ğŸ“Œ ìš©ë„: ì§„í–‰ ì¤‘ì¸ ìš”ì•½ ì €ì¥
    ):
        """
        ğŸ“‹ ê¸°ëŠ¥: í† í° ì„ê³„ê°’ ê¸°ë°˜ìœ¼ë¡œ ë²„í¼ì™€ ìš”ì•½ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬
        ğŸ“¥ ì…ë ¥: LLM ëª¨ë¸ê³¼ í† í° ì„ê³„ê°’ ì„¤ì •
        ğŸ“¤ ì¶œë ¥: ConversationSummaryBufferMemory ì¸ìŠ¤í„´ìŠ¤
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ì¤‘ê°„ ê¸¸ì´ ëŒ€í™”, ìµœê·¼ ì»¨í…ìŠ¤íŠ¸ê°€ ì¤‘ìš”í•œ ê²½ìš°
        ğŸ”— ê´€ë ¨ ê°œë…: Token Management, Hybrid Memory Architecture
        """
```

### í•µì‹¬ íŒŒë¼ë¯¸í„°: max_token_limit

```python
# ğŸ§  ê°œë…: max_token_limitì€ í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ì˜ í•µì‹¬ ì œì–´ ë§¤ê°œë³€ìˆ˜

# ğŸ’¡ ì‹¤ë¬´ ê¶Œì¥ ì„¤ì •ê°’:
token_limit_guide = {
    "ê°„ë‹¨í•œ_ëŒ€í™”": 500,     # ì¼ë°˜ì ì¸ Q&A, ê°„ë‹¨í•œ ìƒë‹´
    "ì¼ë°˜_ëŒ€í™”": 1000,      # ê¸°ë³¸ ê³ ê° ì„œë¹„ìŠ¤, ì¼ë°˜ ì±—ë´‡
    "ë³µì¡í•œ_ìƒë‹´": 2000,    # ê¸°ìˆ  ì§€ì›, ë³µì¡í•œ ë¬¸ì œ í•´ê²°
    "ì „ë¬¸_ì»¨ì„¤íŒ…": 4000,    # ì „ë¬¸ì  ìƒë‹´, ì¥ì‹œê°„ ì„¸ì…˜
    "êµìœ¡_ì„¸ì…˜": 6000       # íŠœí„°ë§, ì¥ì‹œê°„ êµìœ¡ ëŒ€í™”
}

# âš ï¸ ì£¼ì˜: ë„ˆë¬´ ë†’ìœ¼ë©´ BufferMemoryì™€ ì°¨ì´ ì—†ìŒ
# âš ï¸ ì£¼ì˜: ë„ˆë¬´ ë‚®ìœ¼ë©´ SummaryMemoryì™€ ìœ ì‚¬í•¨
```

## ğŸ”§ ë™ì‘ ê³¼ì • ìƒì„¸

### 1. ê¸°ë³¸ í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì„¤ì •
```python
# === Step 1: LLMê³¼ í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ===
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI

# LLM ì„¤ì • (ìš”ì•½ìš©)
llm = ChatOpenAI(
    temperature=0.1,  # ğŸ“Œ ì¼ê´€ëœ ìš”ì•½ì„ ìœ„í•œ ë‚®ì€ ì˜¨ë„
    model="gpt-3.5-turbo"
)

# í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=150,  # ğŸ“Œ í•µì‹¬: í† í° ì„ê³„ê°’ (í…ŒìŠ¤íŠ¸ìš© ë‚®ì€ ê°’)
    return_messages=True  # ğŸ“Œ ChatModelìš© ë©”ì‹œì§€ ê°ì²´ ë°˜í™˜
)

# === Step 2: í† í° í•œê³„ ì´í•˜ ëŒ€í™” (ë²„í¼ ëª¨ë“œ) ===
def add_message(human_input: str, ai_output: str):
    """ëŒ€í™” ì¶”ê°€ í—¬í¼ í•¨ìˆ˜"""
    memory.save_context({"input": human_input}, {"output": ai_output})

def get_current_state():
    """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ì¡°íšŒ ë° ë¶„ì„"""
    history = memory.load_memory_variables({})
    messages = history.get("history", [])
    
    # ë©”ì‹œì§€ íƒ€ì… ë¶„ì„
    summary_messages = [msg for msg in messages if msg.type == "system"]
    regular_messages = [msg for msg in messages if msg.type != "system"]
    
    return {
        "total_messages": len(messages),
        "summary_count": len(summary_messages),
        "buffer_count": len(regular_messages),
        "has_summary": len(summary_messages) > 0
    }

# ì²« ë²ˆì§¸ ëŒ€í™” (í† í° í•œê³„ ì´í•˜)
print("=== 1ë‹¨ê³„: í† í° í•œê³„ ì´í•˜ ëŒ€í™” (ë²„í¼ ëª¨ë“œ) ===")
add_message(
    "ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤. ì„œìš¸ì— ì‚´ê³  ìˆì–´ìš”.",
    "ì•ˆë…•í•˜ì„¸ìš” ê¹€ì² ìˆ˜ë‹˜! ì„œìš¸ì€ ì •ë§ ë©‹ì§„ ë„ì‹œë„¤ìš”."
)

state1 = get_current_state()
print(f"ë©”ëª¨ë¦¬ ìƒíƒœ: ì „ì²´ {state1['total_messages']}ê°œ ë©”ì‹œì§€")
print(f"ìš”ì•½ ìˆìŒ: {state1['has_summary']}")

# ë‘ ë²ˆì§¸ ëŒ€í™” (ì—¬ì „íˆ ë²„í¼ ëª¨ë“œ)
add_message("ì„œìš¸ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ì•„ìš”!", "ì •ë§ ê·¸ë ‡ë„¤ìš”! ë´„ ë‚ ì”¨ê°€ ì™„ë²½í•©ë‹ˆë‹¤.")

state2 = get_current_state()
print(f"\në‘ ë²ˆì§¸ ëŒ€í™” í›„: {state2['total_messages']}ê°œ ë©”ì‹œì§€, ìš”ì•½: {state2['has_summary']}")

# === Step 3: í† í° í•œê³„ ì´ˆê³¼ ì‹œ ìš”ì•½ ìƒì„± ===
print("\n=== 2ë‹¨ê³„: í† í° í•œê³„ ì´ˆê³¼ ì‹œ ìš”ì•½ ëª¨ë“œ ì „í™˜ ===")

# ì¶”ê°€ ëŒ€í™”ë¥¼ ê³„ì† ì§„í–‰í•˜ì—¬ í† í° í•œê³„ ì´ˆê³¼ ìœ ë„
additional_conversations = [
    ("í•œêµ­ì—ì„œ ì œì¼ ì¢‹ì•„í•˜ëŠ” ìŒì‹ì´ ë­ì˜ˆìš”?", "ê¹€ì¹˜ì°Œê°œë¥¼ ì •ë§ ì¢‹ì•„í•´ìš”!"),
    ("í•œêµ­ ë¬¸í™” ì¤‘ ì¸ìƒ ê¹Šì€ ê²Œ ìˆë‚˜ìš”?", "ì „í†µ ìŒì•…ì´ ì•„ë¦„ë‹¤ì›Œìš”."),
    ("ì„œìš¸ì—ì„œ ê°€ë³¼ ë§Œí•œ ê³³ ì¶”ì²œí•´ ì£¼ì„¸ìš”.", "ê²½ë³µê¶ê³¼ ë‚¨ì‚°íƒ€ì›Œë¥¼ ì¶”ì²œë“œë ¤ìš”."),
    ("í•œêµ­ì–´ ë°°ìš°ê¸° ì–´ë µë‚˜ìš”?", "ì²˜ìŒì—” ì–´ë µì§€ë§Œ ì¬ë¯¸ìˆì–´ìš”!")
]

for i, (human, ai) in enumerate(additional_conversations, 3):
    add_message(human, ai)
    state = get_current_state()
    
    print(f"{i}ë²ˆì§¸ ëŒ€í™” í›„ - ë©”ì‹œì§€: {state['total_messages']}ê°œ, ìš”ì•½ ì¡´ì¬: {state['has_summary']}")
    
    # ìš”ì•½ì´ ìƒì„±ë˜ë©´ ìƒì„¸ ë¶„ì„
    if state['has_summary']:
        print("ğŸ‰ í† í° í•œê³„ ì´ˆê³¼! í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ í™œì„±í™”")
        messages = memory.load_memory_variables({})["history"]
        
        for msg in messages:
            if msg.type == "system":
                print(f"ğŸ“‹ ìš”ì•½: {msg.content[:100]}...")
            else:
                role = "Human" if msg.type == "human" else "AI"
                print(f"{role}: {msg.content}")
        break
```

### 2. í† í° ì„ê³„ê°’ë³„ ë™ì‘ ë¹„êµ
```python
# === í† í° ì„ê³„ê°’ì´ ë©”ëª¨ë¦¬ ë™ì‘ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„ ===
def test_token_limits(conversations, token_limits=[100, 500, 1000, 2000]):
    """
    ğŸ“‹ ê¸°ëŠ¥: ë‹¤ì–‘í•œ í† í° ì„ê³„ê°’ì—ì„œ ë©”ëª¨ë¦¬ ë™ì‘ ë¹„êµ
    ğŸ“¥ ì…ë ¥: í…ŒìŠ¤íŠ¸ ëŒ€í™”ì™€ ì„ê³„ê°’ ë¦¬ìŠ¤íŠ¸
    ğŸ“¤ ì¶œë ¥: ê° ì„ê³„ê°’ë³„ ë©”ëª¨ë¦¬ ë™ì‘ ë¶„ì„
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ìµœì  ì„ê³„ê°’ ê²°ì •
    """
    results = {}
    
    for limit in token_limits:
        print(f"\nğŸ”¬ í† í° í•œê³„ {limit} í…ŒìŠ¤íŠ¸")
        
        # ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        test_memory = ConversationSummaryBufferMemory(
            llm=ChatOpenAI(temperature=0.1),
            max_token_limit=limit,
            return_messages=True
        )
        
        # ëª¨ë“  ëŒ€í™” ì¶”ê°€
        summary_triggered_at = None
        for i, (human, ai) in enumerate(conversations):
            test_memory.save_context({"input": human}, {"output": ai})
            
            # ìš”ì•½ì´ ì²˜ìŒ ìƒì„±ëœ ì‹œì  ê¸°ë¡
            messages = test_memory.load_memory_variables({})["history"]
            has_summary = any(msg.type == "system" for msg in messages)
            
            if has_summary and summary_triggered_at is None:
                summary_triggered_at = i + 1
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ ë¶„ì„
        final_messages = test_memory.load_memory_variables({})["history"]
        summary_msgs = [msg for msg in final_messages if msg.type == "system"]
        buffer_msgs = [msg for msg in final_messages if msg.type != "system"]
        
        results[limit] = {
            "summary_triggered_at": summary_triggered_at,
            "final_summary_count": len(summary_msgs),
            "final_buffer_count": len(buffer_msgs),
            "total_final_messages": len(final_messages)
        }
        
        print(f"   ìš”ì•½ ì‹œì‘: {summary_triggered_at}ë²ˆì§¸ ëŒ€í™”")
        print(f"   ìµœì¢… êµ¬ì„±: ìš”ì•½ {len(summary_msgs)}ê°œ + ë²„í¼ {len(buffer_msgs)}ê°œ")
    
    return results

# í…ŒìŠ¤íŠ¸ ëŒ€í™” ì¤€ë¹„
test_conversations = [
    ("ì•ˆë…•í•˜ì„¸ìš”, ì €ëŠ” ì´ì˜í¬ì…ë‹ˆë‹¤.", "ì•ˆë…•í•˜ì„¸ìš” ì˜í¬ë‹˜!"),
    ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.", "ì •ë§ ë§‘ê³  í™”ì°½í•´ìš”."),
    ("ì£¼ë§ ê³„íšì´ ìˆìœ¼ì‹ ê°€ìš”?", "ê³µì›ì— ì‚°ì±…í•˜ëŸ¬ ê°ˆ ì˜ˆì •ì´ì—ìš”."),
    ("ì–´ë–¤ ìŒì‹ì„ ì¢‹ì•„í•˜ì„¸ìš”?", "í•œì‹ì„ ê°€ì¥ ì¢‹ì•„í•©ë‹ˆë‹¤."),
    ("ì·¨ë¯¸ê°€ ë¬´ì—‡ì¸ê°€ìš”?", "ì±… ì½ê¸°ì™€ ì˜í™” ê°ìƒì´ì—ìš”."),
    ("ìµœê·¼ì— ë³¸ ì˜í™” ì¤‘ ê¸°ì–µë‚˜ëŠ” ê²Œ ìˆë‚˜ìš”?", "ì•¡ì…˜ ì˜í™”ë¥¼ í•˜ë‚˜ ë´¤ëŠ”ë° ì •ë§ ì¬ë¯¸ìˆì—ˆì–´ìš”."),
    ("ì±…ì€ ì£¼ë¡œ ì–´ë–¤ ì¥ë¥´ë¥¼ ì½ìœ¼ì‹œë‚˜ìš”?", "ì†Œì„¤ê³¼ ì—ì„¸ì´ë¥¼ ì£¼ë¡œ ì½ì–´ìš”."),
    ("ì¶”ì²œí•˜ê³  ì‹¶ì€ ì±…ì´ ìˆë‚˜ìš”?", "ìµœê·¼ì— ì½ì€ ì†Œì„¤ í•œ ê¶Œì„ ì¶”ì²œë“œë¦¬ê³  ì‹¶ì–´ìš”.")
]

# í† í° ì„ê³„ê°’ë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
comparison_results = test_token_limits(test_conversations)

# ê²°ê³¼ ë¶„ì„
print("\nğŸ“Š í† í° ì„ê³„ê°’ë³„ ë™ì‘ ë¶„ì„ ê²°ê³¼:")
print("=" * 60)
for limit, result in comparison_results.items():
    efficiency = "ë†’ìŒ" if result["summary_triggered_at"] and result["summary_triggered_at"] <= 4 else "ë³´í†µ" if result["summary_triggered_at"] else "ë‚®ìŒ"
    print(f"í•œê³„ {limit}: ìš”ì•½ì‹œì  {result['summary_triggered_at']}, íš¨ìœ¨ì„± {efficiency}")
```

## ğŸ’» ì‹¤ì „ ì˜ˆì œ

### ì ì‘í˜• ê³ ê° ì„œë¹„ìŠ¤ ì‹œìŠ¤í…œ
```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema.runnable import RunnablePassthrough
import tiktoken
from datetime import datetime, timedelta

class AdaptiveCustomerServiceSystem:
    """
    ğŸ¯ ëª©ì : í† í° íš¨ìœ¨ì„±ê³¼ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆì„ ê· í˜•ì¡ì€ ê³ ê° ì„œë¹„ìŠ¤
    ğŸ’¡ íŠ¹ì§•: ë™ì  í† í° ì„ê³„ê°’ ì¡°ì •, ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë©”ëª¨ë¦¬ ê´€ë¦¬
    """
    
    def __init__(self, service_type: str = "general"):
        self.service_type = service_type
        
        # ì„œë¹„ìŠ¤ íƒ€ì…ë³„ í† í° í•œê³„ ì„¤ì •
        self.token_limits = {
            "simple": 500,      # ê°„ë‹¨í•œ FAQ
            "general": 1500,    # ì¼ë°˜ ê³ ê° ì„œë¹„ìŠ¤
            "technical": 3000,  # ê¸°ìˆ  ì§€ì›
            "premium": 5000     # í”„ë¦¬ë¯¸ì—„ ì„œë¹„ìŠ¤
        }
        
        # LLM ì„¤ì •
        self.llm = ChatOpenAI(temperature=0.2, model="gpt-3.5-turbo")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
        self.memory = ConversationSummaryBufferMemory(
            llm=self.llm,
            max_token_limit=self.token_limits.get(service_type, 1500),
            return_messages=True,
            memory_key="conversation_history"
        )
        
        # ê³ ê° ì„œë¹„ìŠ¤ìš© í”„ë¡¬í”„íŠ¸
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì „ë¬¸ì ì´ê³  íš¨ìœ¨ì ì¸ ê³ ê° ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì´ì „ ëŒ€í™” ë‚´ìš©:
{conversation_history}

ìƒë‹´ ì›ì¹™:
1. ìµœê·¼ ëŒ€í™”ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ì¡°í•˜ì—¬ ë§¥ë½ì„ ìœ ì§€í•˜ì„¸ìš”
2. ìš”ì•½ëœ ì´ì „ ë‚´ìš©ë„ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”
3. ê³ ê°ì˜ ê°ì • ìƒíƒœë¥¼ ê³ ë ¤í•˜ì—¬ ì‘ë‹µí•˜ì„¸ìš”
4. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…ì„ ì œì‹œí•˜ì„¸ìš”

í˜„ì¬ ì„œë¹„ìŠ¤ ë ˆë²¨: {service_level}"""),
            ("human", "{input}")
        ])
        
        # ì²´ì¸ êµ¬ì„±
        self.chain = (
            RunnablePassthrough.assign(
                conversation_history=lambda _: self._get_memory_context(),
                service_level=lambda _: self.service_type.upper()
            )
            | self.prompt
            | self.llm
        )
        
        # ì„¸ì…˜ í†µê³„
        self.session_stats = {
            "start_time": datetime.now(),
            "total_interactions": 0,
            "summary_generations": 0,
            "token_usage_estimate": 0,
            "escalation_triggers": 0
        }
    
    def _get_memory_context(self) -> str:
        """ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        memory_vars = self.memory.load_memory_variables({})
        history = memory_vars.get("conversation_history", [])
        
        if not history:
            return "ìƒˆë¡œìš´ ìƒë‹´ ì„¸ì…˜ì…ë‹ˆë‹¤."
        
        # ë©”ì‹œì§€ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        context_parts = []
        for msg in history:
            if msg.type == "system":
                context_parts.append(f"[ì´ì „ ëŒ€í™” ìš”ì•½] {msg.content}")
            elif msg.type == "human":
                context_parts.append(f"ê³ ê°: {msg.content}")
            elif msg.type == "ai":
                context_parts.append(f"ìƒë‹´ì‚¬: {msg.content}")
        
        return "\n".join(context_parts)
    
    def chat(self, customer_input: str) -> dict:
        """
        ğŸ“‹ ê¸°ëŠ¥: ê³ ê° ì…ë ¥ ì²˜ë¦¬ ë° í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ê´€ë¦¬
        ğŸ“¥ ì…ë ¥: ê³ ê° ë©”ì‹œì§€
        ğŸ“¤ ì¶œë ¥: ì‘ë‹µê³¼ ë©”ëª¨ë¦¬ ìƒíƒœ ì •ë³´
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ì‹¤ì‹œê°„ ê³ ê° ìƒë‹´
        """
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ (ì´ì „)
        pre_messages = self.memory.load_memory_variables({})["conversation_history"]
        pre_summary_count = sum(1 for msg in pre_messages if msg.type == "system")
        
        # ì‘ë‹µ ìƒì„±
        response = self.chain.invoke({"input": customer_input})
        
        # ë©”ëª¨ë¦¬ì— ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ ë™ì‘ íŠ¸ë¦¬ê±° ê°€ëŠ¥)
        self.memory.save_context(
            {"input": customer_input},
            {"output": response.content}
        )
        
        # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸ (ì´í›„)
        post_messages = self.memory.load_memory_variables({})["conversation_history"]
        post_summary_count = sum(1 for msg in post_messages if msg.type == "system")
        
        # ìš”ì•½ ìƒì„± ì—¬ë¶€ í™•ì¸
        summary_generated = post_summary_count > pre_summary_count
        if summary_generated:
            self.session_stats["summary_generations"] += 1
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.session_stats["total_interactions"] += 1
        
        # ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”ì„± ê²€í† 
        escalation_needed = self._check_escalation_triggers(customer_input, response.content)
        
        return {
            "response": response.content,
            "memory_info": {
                "total_messages": len(post_messages),
                "has_summary": post_summary_count > 0,
                "summary_generated_now": summary_generated,
                "memory_mode": "hybrid" if post_summary_count > 0 else "buffer"
            },
            "escalation_needed": escalation_needed,
            "session_stats": self.session_stats.copy()
        }
    
    def _check_escalation_triggers(self, customer_input: str, ai_response: str) -> bool:
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”ì„± ê²€í† """
        escalation_keywords = [
            "ë§¤ë‹ˆì €", "ë¶ˆë§Œ", "í™”ê°€", "ì·¨ì†Œ", "í™˜ë¶ˆ", "ì†Œì†¡", "í•­ì˜", 
            "ì‹¤ë§", "ë¬¸ì œ", "í•´ê²°", "ì±…ì„ì", "ìƒê¸‰ì"
        ]
        
        if any(keyword in customer_input for keyword in escalation_keywords):
            self.session_stats["escalation_triggers"] += 1
            return True
        
        return False
    
    def get_memory_analysis(self) -> dict:
        """
        ğŸ“‹ ê¸°ëŠ¥: í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ìƒì„¸ ë¶„ì„
        ğŸ“¤ ì¶œë ¥: ë©”ëª¨ë¦¬ êµ¬ì„±ê³¼ íš¨ìœ¨ì„± ë¶„ì„
        ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ë©”ëª¨ë¦¬ ìµœì í™” ë° ë””ë²„ê¹…
        """
        messages = self.memory.load_memory_variables({})["conversation_history"]
        
        # ë©”ì‹œì§€ íƒ€ì…ë³„ ë¶„ë¥˜
        summary_messages = [msg for msg in messages if msg.type == "system"]
        human_messages = [msg for msg in messages if msg.type == "human"]
        ai_messages = [msg for msg in messages if msg.type == "ai"]
        
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
        total_content = " ".join(msg.content for msg in messages)
        encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
        estimated_tokens = len(encoding.encode(total_content))
        
        return {
            "memory_composition": {
                "summary_messages": len(summary_messages),
                "recent_human_messages": len(human_messages),
                "recent_ai_messages": len(ai_messages),
                "total_messages": len(messages)
            },
            "token_analysis": {
                "estimated_current_tokens": estimated_tokens,
                "token_limit": self.memory.max_token_limit,
                "utilization_rate": estimated_tokens / self.memory.max_token_limit,
                "efficiency_status": self._get_efficiency_status(estimated_tokens)
            },
            "memory_mode": "hybrid" if summary_messages else "buffer_only",
            "session_duration": datetime.now() - self.session_stats["start_time"]
        }
    
    def _get_efficiency_status(self, current_tokens: int) -> str:
        """í† í° íš¨ìœ¨ì„± ìƒíƒœ íŒë‹¨"""
        utilization = current_tokens / self.memory.max_token_limit
        
        if utilization < 0.3:
            return "ì—¬ìœ "
        elif utilization < 0.7:
            return "ì ì •"
        elif utilization < 0.9:
            return "ì£¼ì˜"
        else:
            return "ìœ„í—˜"
    
    def adjust_token_limit(self, new_limit: int):
        """ë™ì  í† í° í•œê³„ ì¡°ì •"""
        old_limit = self.memory.max_token_limit
        self.memory.max_token_limit = new_limit
        
        print(f"í† í° í•œê³„ ì¡°ì •: {old_limit} â†’ {new_limit}")
        
        # í˜„ì¬ í† í° ì‚¬ìš©ëŸ‰ì´ ìƒˆ í•œê³„ë¥¼ ì´ˆê³¼í•˜ë©´ ìš”ì•½ ê°•ì œ ì‹¤í–‰
        current_analysis = self.get_memory_analysis()
        if current_analysis["token_analysis"]["estimated_current_tokens"] > new_limit:
            print("ìƒˆ í•œê³„ ì´ˆê³¼ë¡œ ì¸í•œ ìš”ì•½ ì¬ìƒì„± í•„ìš”")

# === ì‹¤ì œ ê³ ê° ì„œë¹„ìŠ¤ ì‹œë®¬ë ˆì´ì…˜ ===
print("ğŸ¢ ì ì‘í˜• ê³ ê° ì„œë¹„ìŠ¤ ì‹œìŠ¤í…œ ì‹œì‘")
print("=" * 50)

# ê¸°ìˆ  ì§€ì› ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
service = AdaptiveCustomerServiceSystem("technical")

# ë³µì¡í•œ ê¸°ìˆ  ì§€ì› ì‹œë‚˜ë¦¬ì˜¤
tech_support_scenario = [
    "ì•ˆë…•í•˜ì„¸ìš”, ë…¸íŠ¸ë¶ì— ë¬¸ì œê°€ ìˆì–´ì„œ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤.",
    "ë¶€íŒ…í•  ë•Œ ë¸”ë£¨ìŠ¤í¬ë¦°ì´ ë‚˜íƒ€ë‚˜ìš”. ì—ëŸ¬ ì½”ë“œëŠ” 0x0000007Bì…ë‹ˆë‹¤.",
    "ì–´ì œê¹Œì§€ëŠ” ì˜ ëëŠ”ë° ì˜¤ëŠ˜ ê°‘ìê¸° ì´ëŸ° ë¬¸ì œê°€ ìƒê²¼ì–´ìš”.",
    "ì¤‘ìš”í•œ í”„ë ˆì  í…Œì´ì…˜ íŒŒì¼ì´ ìˆëŠ”ë° ì ‘ê·¼í•  ìˆ˜ ì—†ì–´ì„œ ì •ë§ ê¸‰í•©ë‹ˆë‹¤.",
    "í˜¹ì‹œ ë°ì´í„° ë³µêµ¬ê°€ ê°€ëŠ¥í•œê°€ìš”? ë°±ì—…ì´ ì—†ì–´ì„œ ê±±ì •ë¼ìš”.",
    "ì´ì „ì—ë„ ê°€ë” ëŠë ¤ì§€ëŠ” í˜„ìƒì´ ìˆì—ˆì–´ìš”. ê´€ë ¨ì´ ìˆì„ê¹Œìš”?",
    "í•˜ë“œì›¨ì–´ ë¬¸ì œì¸ì§€ ì†Œí”„íŠ¸ì›¨ì–´ ë¬¸ì œì¸ì§€ ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆë‚˜ìš”?",
    "ìˆ˜ë¦¬ê°€ í•„ìš”í•˜ë‹¤ë©´ ë¹„ìš©ì´ ì–¼ë§ˆë‚˜ ë“¤ê¹Œìš”?",
    "ì„ì‹œë¡œë¼ë„ ë°ì´í„°ì— ì ‘ê·¼í•  ë°©ë²•ì´ ìˆì„ê¹Œìš”?"
]

for i, customer_message in enumerate(tech_support_scenario, 1):
    print(f"\n--- {i}ë²ˆì§¸ ìƒë‹´ ---")
    print(f"ğŸ‘¤ ê³ ê°: {customer_message}")
    
    # ìƒë‹´ ì²˜ë¦¬
    result = service.chat(customer_message)
    
    print(f"ğŸ§ ìƒë‹´ì‚¬: {result['response'][:200]}...")
    
    # ë©”ëª¨ë¦¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§
    memory_info = result['memory_info']
    print(f"ğŸ“Š ë©”ëª¨ë¦¬: {memory_info['memory_mode']} ëª¨ë“œ, "
          f"{memory_info['total_messages']}ê°œ ë©”ì‹œì§€")
    
    if memory_info['summary_generated_now']:
        print("ğŸ”„ ìš”ì•½ ìƒì„±ë¨ - í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ í™œì„±í™”!")
    
    if result['escalation_needed']:
        print("âš¡ ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ê°ì§€!")

# ìµœì¢… ë¶„ì„
print("\n" + "=" * 60)
print("ğŸ“ˆ ìµœì¢… ì„¸ì…˜ ë¶„ì„")
print("=" * 60)

final_analysis = service.get_memory_analysis()
session_stats = service.session_stats

print(f"ğŸ“Š ë©”ëª¨ë¦¬ êµ¬ì„±:")
for key, value in final_analysis['memory_composition'].items():
    print(f"   {key}: {value}")

print(f"\nğŸ’° í† í° íš¨ìœ¨ì„±:")
token_info = final_analysis['token_analysis']
print(f"   í˜„ì¬ í† í°: {token_info['estimated_current_tokens']}")
print(f"   í† í° í•œê³„: {token_info['token_limit']}")
print(f"   ì‚¬ìš©ë¥ : {token_info['utilization_rate']:.1%}")
print(f"   ìƒíƒœ: {token_info['efficiency_status']}")

print(f"\nğŸ¯ ì„¸ì…˜ í†µê³„:")
print(f"   ì´ ìƒë‹´: {session_stats['total_interactions']}íšŒ")
print(f"   ìš”ì•½ ìƒì„±: {session_stats['summary_generations']}íšŒ")
print(f"   ì—ìŠ¤ì»¬ë ˆì´ì…˜: {session_stats['escalation_triggers']}íšŒ")
print(f"   ì„¸ì…˜ ì‹œê°„: {final_analysis['session_duration']}")
```

## ğŸ” ë³€ìˆ˜/í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…

### í•µì‹¬ ë³€ìˆ˜ë“¤
```python
# í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì œì–´ ë³€ìˆ˜
max_token_limit = 2000         # ğŸ“Œ ìš©ë„: ìš”ì•½ íŠ¸ë¦¬ê±° ì„ê³„ê°’, íƒ€ì…: int, í•µì‹¬!
moving_summary_buffer = ""     # ğŸ“Œ ìš©ë„: ì§„í–‰ ì¤‘ì¸ ìš”ì•½, íƒ€ì…: str
chat_memory = []               # ğŸ“Œ ìš©ë„: ìµœê·¼ ë©”ì‹œì§€ ë²„í¼, íƒ€ì…: List[BaseMessage]

# í† í° ê³„ì‚° ê´€ë ¨
token_counter = 0              # ğŸ“Œ ìš©ë„: í˜„ì¬ í† í° ì‚¬ìš©ëŸ‰, íƒ€ì…: int
summarize_step = 2             # ğŸ“Œ ìš©ë„: Nê°œì”© ë¬¶ì–´ì„œ ìš”ì•½, íƒ€ì…: int
```

### í•µì‹¬ ë©”ì„œë“œë“¤
```python
def _get_num_tokens_from_messages(messages: List[BaseMessage]) -> int:
    """
    ğŸ“‹ ê¸°ëŠ¥: ë©”ì‹œì§€ë“¤ì˜ í† í° ìˆ˜ ê³„ì‚°
    ğŸ“¥ ì…ë ¥: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    ğŸ“¤ ì¶œë ¥: ì´ í† í° ìˆ˜
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: í† í° ì„ê³„ê°’ í™•ì¸
    """

def prune(self) -> None:
    """
    ğŸ“‹ ê¸°ëŠ¥: í† í° í•œê³„ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ë©”ì‹œì§€ë¥¼ ìš”ì•½ìœ¼ë¡œ ë³€í™˜
    ğŸ“¤ ì¶œë ¥: None (ë‚´ë¶€ ë©”ëª¨ë¦¬ ìƒíƒœ ë³€ê²½)
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ìë™ ë©”ëª¨ë¦¬ ìµœì í™”
    """

def clear(self) -> None:
    """
    ğŸ“‹ ê¸°ëŠ¥: ë²„í¼ì™€ ìš”ì•½ ëª¨ë‘ ì´ˆê¸°í™”
    ğŸ“¤ ì¶œë ¥: None
    ğŸ’¡ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: ìƒˆ ì„¸ì…˜ ì‹œì‘
    """
```

## ğŸ§ª ì‹¤ìŠµ ê³¼ì œ

### ğŸ”¨ ê¸°ë³¸ ê³¼ì œ
1. **í† í° ì„ê³„ê°’ ì‹¤í—˜**: ë‹¤ì–‘í•œ max_token_limitì—ì„œ ë©”ëª¨ë¦¬ ë™ì‘ ê´€ì°°
```python
# TODO: 100, 500, 1000, 2000 í† í° í•œê³„ë¡œ ê°™ì€ ëŒ€í™” í…ŒìŠ¤íŠ¸
def experiment_token_limits():
    limits = [100, 500, 1000, 2000]
    # ê° í•œê³„ì—ì„œ ì–¸ì œ ìš”ì•½ì´ ì‹œì‘ë˜ëŠ”ì§€ ë¶„ì„
    pass
```

2. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¸¡ì •**: BufferMemory vs SummaryBufferMemory ë¹„êµ
```python
# TODO: ê°™ì€ ëŒ€í™”ì—ì„œ ë©”ëª¨ë¦¬ íƒ€ì…ë³„ í† í° ì‚¬ìš©ëŸ‰ê³¼ í’ˆì§ˆ ë¹„êµ
def compare_memory_efficiency():
    # êµ¬í˜„í•˜ê¸°
    pass
```

### ğŸš€ ì‹¬í™” ê³¼ì œ
3. **ì ì‘í˜• í† í° ê´€ë¦¬**: ëŒ€í™” íŒ¨í„´ì— ë”°ë¥¸ ë™ì  ì„ê³„ê°’ ì¡°ì •
```python
# TODO: ë³µì¡í•œ ëŒ€í™”ëŠ” ë†’ì€ ì„ê³„ê°’, ê°„ë‹¨í•œ ëŒ€í™”ëŠ” ë‚®ì€ ì„ê³„ê°’
class AdaptiveTokenManager:
    def __init__(self):
        self.complexity_history = []
        self.base_limit = 1000
    
    def adjust_limit_based_on_complexity(self, conversation_complexity: float):
        # êµ¬í˜„í•˜ê¸°
        pass
```

4. **ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë©”ëª¨ë¦¬**: ì¤‘ìš”í•œ ì •ë³´ëŠ” ë” ì˜¤ë˜ ë²„í¼ì— ìœ ì§€
```python
# TODO: ì¤‘ìš”ë„ ì ìˆ˜ê°€ ë†’ì€ ë©”ì‹œì§€ëŠ” ìš”ì•½ì—ì„œ ì œì™¸
class PriorityBasedSummaryBufferMemory(ConversationSummaryBufferMemory):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.importance_scores = []
    
    def save_context_with_priority(self, inputs, outputs, importance: float):
        # ì¤‘ìš”ë„ ê¸°ë°˜ ì €ì¥ ë¡œì§ êµ¬í˜„
        pass
```

### ğŸ’¡ ì°½ì˜ ê³¼ì œ
5. **ë©€í‹°ë ˆë²¨ ìš”ì•½**: ë‹¨ê¸° ìš”ì•½ + ì¤‘ê¸° ìš”ì•½ + ì¥ê¸° ìš”ì•½ ê³„ì¸µ êµ¬ì¡°
6. **ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´**: ìš”ì•½ ì‹œ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ ê°•ì œ ë³´ì¡´ ë©”ì»¤ë‹ˆì¦˜
7. **ì‹¤ì‹œê°„ ìµœì í™”**: ëŒ€í™” ì§„í–‰ ì¤‘ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ìµœì í™”

## âš ï¸ ì£¼ì˜ì‚¬í•­

### í† í° ì„ê³„ê°’ ì„¤ì • ê°€ì´ë“œë¼ì¸
```python
# ğŸ¯ ì„œë¹„ìŠ¤ë³„ ê¶Œì¥ max_token_limit ì„¤ì •
service_recommendations = {
    "FAQ_ë´‡": 300,           # ê°„ë‹¨í•œ ì§ˆë‹µ, ë¹ ë¥¸ ì‘ë‹µ í•„ìš”
    "ì¼ë°˜_ê³ ê°ì„œë¹„ìŠ¤": 1500,  # ê· í˜•ì¡íŒ í’ˆì§ˆê³¼ íš¨ìœ¨ì„±
    "ê¸°ìˆ _ì§€ì›": 3000,       # ë³µì¡í•œ ë¬¸ì œ, ê¸´ ì„¤ëª… í•„ìš”
    "êµìœ¡_íŠœí„°": 4000,       # í•™ìŠµ ë§¥ë½ ìœ ì§€ ì¤‘ìš”
    "ì „ë¬¸_ìƒë‹´": 6000,       # ì¥ì‹œê°„ ê¹Šì´ ìˆëŠ” ëŒ€í™”
    "ì˜ë£Œ_ìƒë‹´": 8000        # í™˜ì íˆìŠ¤í† ë¦¬ ë³´ì¡´ í•„ìˆ˜
}

# âš ï¸ ë„ˆë¬´ ë‚®ì€ ì„¤ì •ì˜ ìœ„í—˜ì„±
if max_token_limit < 500:
    print("ê²½ê³ : ë„ˆë¬´ ë‚®ì€ ì„ê³„ê°’ì€ ë¹ˆë²ˆí•œ ìš”ì•½ìœ¼ë¡œ ì¸í•œ ì •ë³´ ì†ì‹¤ ìœ„í—˜")

# âš ï¸ ë„ˆë¬´ ë†’ì€ ì„¤ì •ì˜ ë¬¸ì œì   
if max_token_limit > 5000:
    print("ì£¼ì˜: ë†’ì€ ì„ê³„ê°’ì€ BufferMemoryì™€ ìœ ì‚¬í•œ ë¹„ìš© ì¦ê°€")
```

### ìš”ì•½ í’ˆì§ˆ ê´€ë¦¬
1. **ì¤‘ìš” ì •ë³´ ì†ì‹¤ ë°©ì§€**: ê³ ê° ì •ë³´, ì£¼ë¬¸ ë²ˆí˜¸ ë“± í•µì‹¬ ë°ì´í„° ë³„ë„ ì €ì¥
2. **ìš”ì•½ ê²€ì¦**: ìš”ì•½ëœ ë‚´ìš©ì´ í•µì‹¬ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
3. **ì—ëŸ¬ ë³µêµ¬**: ìš”ì•½ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë©”ì‹œì§€ë¡œ í´ë°±

### ë¹„ìš© ìµœì í™”
- **ìš”ì•½ ëª¨ë¸ ì„ íƒ**: ì €ë ´í•œ ëª¨ë¸ë¡œ ìš”ì•½, ë©”ì¸ ëŒ€í™”ëŠ” ê³ ê¸‰ ëª¨ë¸
- **ë°°ì¹˜ ìš”ì•½**: ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ í•œ ë²ˆì— ìš”ì•½í•˜ì—¬ API í˜¸ì¶œ ì¤„ì´ê¸°
- **ìºì‹œ í™œìš©**: ìœ ì‚¬í•œ íŒ¨í„´ì˜ ìš”ì•½ ê²°ê³¼ ì¬ì‚¬ìš©

## ğŸ”— ê´€ë ¨ ìë£Œ
- **ì´ì „ í•™ìŠµ**: [5.3 ConversationSummaryMemory](./5.3_ConversationSummaryMemory.md)
- **ë‹¤ìŒ í•™ìŠµ**: [5.5 ConversationKGMemory](./5.5_ConversationKGMemory.md)
- **ì„±ëŠ¥ ë¹„êµ**: [Memory Types Performance Analysis](./5.9_Recap.md#performance-comparison)
- **í† í° ê´€ë¦¬**: [Token Optimization Strategies](../Advanced_Topics/Token_Management.md)

---

ğŸ’¡ **í•µì‹¬ ì •ë¦¬**: ConversationSummaryBufferMemoryëŠ” **ìµœê·¼ ëŒ€í™”ì˜ ì™„ì „ì„±**ê³¼ **í† í° íš¨ìœ¨ì„±**ì„ ëª¨ë‘ í™•ë³´í•˜ëŠ” ìµœê³ ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì†”ë£¨ì…˜ì…ë‹ˆë‹¤. `max_token_limit` ì„¤ì •ì´ ì„±ëŠ¥ì˜ í•µì‹¬ì´ë©°, ëŒ€ë¶€ë¶„ì˜ ì‹¤ë¬´ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ **ê°€ì¥ ê· í˜•ì¡íŒ ì„ íƒ**ì…ë‹ˆë‹¤. í† í° ë¹„ìš©ê³¼ ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì‚¬ì´ì˜ ìµœì  ê· í˜•ì ì„ ì œê³µí•©ë‹ˆë‹¤.