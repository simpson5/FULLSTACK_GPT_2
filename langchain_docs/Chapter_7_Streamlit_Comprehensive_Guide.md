# Chapter 7: Streamlit ì™„ë²½ ê°€ì´ë“œ

## ëª©ì°¨
1. [ì†Œê°œ](#ì†Œê°œ)
2. [Streamlit ê¸°ë³¸ ê°œë…](#streamlit-ê¸°ë³¸-ê°œë…)
3. [ë°ì´í„° íë¦„ê³¼ ìƒíƒœ ê´€ë¦¬](#ë°ì´í„°-íë¦„ê³¼-ìƒíƒœ-ê´€ë¦¬)
4. [ë©€í‹°í˜ì´ì§€ ì• í”Œë¦¬ì¼€ì´ì…˜](#ë©€í‹°í˜ì´ì§€-ì• í”Œë¦¬ì¼€ì´ì…˜)
5. [ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„](#ì±„íŒ…-ì¸í„°í˜ì´ìŠ¤-êµ¬í˜„)
6. [íŒŒì¼ ì—…ë¡œë“œì™€ ì²˜ë¦¬](#íŒŒì¼-ì—…ë¡œë“œì™€-ì²˜ë¦¬)
7. [ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬](#ëŒ€í™”-íˆìŠ¤í† ë¦¬-ê´€ë¦¬)
8. [LangChain ì²´ì¸ í†µí•©](#langchain-ì²´ì¸-í†µí•©)
9. [ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„](#ìŠ¤íŠ¸ë¦¬ë°-ì‘ë‹µ-êµ¬í˜„)
10. [ì‹¤ìŠµ ì½”ë“œ ì˜ˆì œ](#ì‹¤ìŠµ-ì½”ë“œ-ì˜ˆì œ)

## ì†Œê°œ

Streamlitì€ Pythonìœ¼ë¡œ ë°ì´í„° ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ì´ ì±•í„°ì—ì„œëŠ” Streamlitê³¼ LangChainì„ ê²°í•©í•˜ì—¬ DocumentGPTë¼ëŠ” ì±—ë´‡ì„ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## Streamlit ê¸°ë³¸ ê°œë…

### 1. st.write() - ë§ŒëŠ¥ ì¶œë ¥ í•¨ìˆ˜

`st.write()`ëŠ” Streamlitì˜ "Swiss army knife"ë¡œ, ê±°ì˜ ëª¨ë“  ê²ƒì„ í™”ë©´ì— ì¶œë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import streamlit as st
from langchain.prompts import PromptTemplate

# í…ìŠ¤íŠ¸ ì¶œë ¥
st.write("Hello, Streamlit!")

# ë³€ìˆ˜ ì¶œë ¥
prompt_template = PromptTemplate.from_template("Tell me about {topic}")
st.write(prompt_template)  # í´ë˜ìŠ¤ ì •ì˜ì™€ ë¬¸ì„œí™”ê¹Œì§€ í‘œì‹œ
```

### 2. Streamlit Magic

ë³€ìˆ˜ë¥¼ ë‹¨ë…ìœ¼ë¡œ ì‘ì„±í•˜ë©´ ìë™ìœ¼ë¡œ í™”ë©´ì— ì¶œë ¥ë©ë‹ˆë‹¤.

```python
# st.write() ì—†ì´ë„ ì¶œë ¥ë¨
"This is magic!"
prompt_template  # ë³€ìˆ˜ëª…ë§Œ ì‘ì„±í•´ë„ ì¶œë ¥
```

### 3. ì£¼ìš” ìœ„ì ¯ë“¤

```python
# ì„ íƒ ë°•ìŠ¤
model = st.selectbox(
    "Choose your model",
    ["GPT-3", "GPT-4"]
)

# í…ìŠ¤íŠ¸ ì…ë ¥
name = st.text_input("What is your name?")

# ìŠ¬ë¼ì´ë”
temperature = st.slider("Temperature", min_value=0.1, max_value=1.0)

# ì²´í¬ë°•ìŠ¤
is_expensive = st.checkbox("Not cheap")
```

## ë°ì´í„° íë¦„ê³¼ ìƒíƒœ ê´€ë¦¬

### í•µì‹¬ ê°œë…: ì „ì²´ ì¬ì‹¤í–‰

**Streamlitì˜ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§•**: ë°ì´í„°ê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ **ì „ì²´ Python íŒŒì¼ì´ ìœ„ì—ì„œ ì•„ë˜ë¡œ ë‹¤ì‹œ ì‹¤í–‰**ë©ë‹ˆë‹¤.

```python
from datetime import datetime

# ì‹œê°„ì„ í‘œì‹œí•˜ë©´ ë°ì´í„° ë³€ê²½ ì‹œë§ˆë‹¤ ì—…ë°ì´íŠ¸ë¨
st.title(datetime.now().strftime("%H:%M:%S"))

model = st.selectbox("Model", ["GPT-3", "GPT-4"])
st.write(f"You chose: {model}")

# ì‚¬ìš©ìê°€ ì„ íƒì„ ë³€ê²½í•  ë•Œë§ˆë‹¤ ì „ì²´ ì½”ë“œê°€ ì¬ì‹¤í–‰ë˜ì–´
# ì‹œê°„ì´ ì—…ë°ì´íŠ¸ë˜ê³  ì„ íƒ ê²°ê³¼ê°€ í‘œì‹œë¨
```

### ì¡°ê±´ë¶€ ìœ„ì ¯ í‘œì‹œ

```python
is_expensive = st.checkbox("Not cheap")

if is_expensive:
    st.write("Expensive mode")
else:
    st.write("Cheap mode")
    # ë‹¤ë¥¸ ìœ„ì ¯ë“¤ì„ ì¡°ê±´ë¶€ë¡œ í‘œì‹œ
    st.slider("Temperature", 0.1, 1.0)
```

## ë©€í‹°í˜ì´ì§€ ì• í”Œë¦¬ì¼€ì´ì…˜

### 1. í˜ì´ì§€ ì„¤ì •

```python
# Home.py (ë©”ì¸ íŒŒì¼)
st.set_page_config(
    page_title="FullstackGPT Home",
    page_icon="ğŸ¤–",
)
```

### 2. í˜ì´ì§€ êµ¬ì¡°

```
project/
â”œâ”€â”€ Home.py              # ë©”ì¸ í˜ì´ì§€
â””â”€â”€ pages/              # ì´ í´ë”ëª…ì€ ë°˜ë“œì‹œ "pages"ì—¬ì•¼ í•¨
    â”œâ”€â”€ 01_DocumentGPT.py
    â”œâ”€â”€ 02_PrivateGPT.py
    â””â”€â”€ 03_QuizGPT.py
```

íŒŒì¼ëª… ì•ì˜ ìˆ«ì(01, 02, 03)ëŠ” ì •ë ¬ìš©ì´ë©° UIì—ëŠ” í‘œì‹œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### 3. ì‚¬ì´ë“œë°” í™œìš©

```python
# with íŒ¨í„´ì„ ì‚¬ìš©í•œ ì‚¬ì´ë“œë°” êµ¬ì„±
with st.sidebar:
    st.title("Sidebar Title")
    st.text_input("Input in sidebar")
    
# ë˜ëŠ”
st.sidebar.title("Sidebar Title")
st.sidebar.text_input("Input in sidebar")
```

## ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„

### 1. ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ

```python
# ì‚¬ìš©ì ë©”ì‹œì§€
with st.chat_message("human"):
    st.write("Hello, AI!")

# AI ë©”ì‹œì§€
with st.chat_message("ai"):
    st.write("Hello, Human!")
```

### 2. ì±„íŒ… ì…ë ¥

```python
message = st.chat_input("Send a message to the AI")
if message:
    # ë©”ì‹œì§€ ì²˜ë¦¬
    st.write(f"You said: {message}")
```

### 3. ìƒíƒœ í‘œì‹œ

```python
# ì‘ì—… ì§„í–‰ ìƒíƒœ í‘œì‹œ
with st.status("Embedding file...", expanded=True) as status:
    time.sleep(2)
    st.write("Getting the file")
    time.sleep(2)
    st.write("Embedding the file")
    time.sleep(2)
    st.write("Caching the file")
    status.update(label="Complete!", state="complete", expanded=False)
```

## íŒŒì¼ ì—…ë¡œë“œì™€ ì²˜ë¦¬

### íŒŒì¼ ì—…ë¡œë” êµ¬í˜„

```python
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )

if file:
    # íŒŒì¼ ë‚´ìš© ì½ê¸°
    file_content = file.read()
    
    # íŒŒì¼ ì €ì¥
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
```

### ì„ë² ë”© í•¨ìˆ˜ with ìºì‹±

```python
@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./.cache/files", exist_ok=True)
    
    with open(file_path, "wb") as f:
        f.write(file_content)
    
    # ì„ë² ë”© ìºì‹œ ë””ë ‰í† ë¦¬
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    
    # í…ìŠ¤íŠ¸ ë¶„í• 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    
    # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    
    # ì„ë² ë”© ìƒì„± ë° ìºì‹±
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    
    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    
    return retriever
```

**`@st.cache_data` ë°ì½”ë ˆì´í„°ì˜ ì¤‘ìš”ì„±**:
- ì…ë ¥(íŒŒì¼)ì´ ë™ì¼í•˜ë©´ í•¨ìˆ˜ë¥¼ ì¬ì‹¤í–‰í•˜ì§€ ì•Šê³  ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
- ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì—°ì‚°(ì„ë² ë”© ìƒì„±)ì„ í•œ ë²ˆë§Œ ìˆ˜í–‰

## ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬

### Session State í™œìš©

```python
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ë©”ì‹œì§€ ì €ì¥ í•¨ìˆ˜
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

# ë©”ì‹œì§€ ì „ì†¡ í•¨ìˆ˜
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

# ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,  # ì´ë¯¸ ì €ì¥ëœ ë©”ì‹œì§€ëŠ” ë‹¤ì‹œ ì €ì¥í•˜ì§€ ì•ŠìŒ
        )
```

### íŒŒì¼ ë³€ê²½ ì‹œ ëŒ€í™” ì´ˆê¸°í™”

```python
if file:
    retriever = embed_file(file)
    send_message("I'm ready! Ask away!", "ai", save=False)
    paint_history()
    # ... ì±„íŒ… ë¡œì§
else:
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ëŒ€í™” ì´ˆê¸°í™”
    st.session_state["messages"] = []
```

## LangChain ì²´ì¸ í†µí•©

### ë¬¸ì„œ í¬ë§· í•¨ìˆ˜

```python
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)
```

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context. 
            If you don't know the answer just say you don't know. 
            DON'T make anything up.
            
            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)
```

### LCEL ì²´ì¸ êµ¬ì„±

```python
llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
    callbacks=[ChatCallbackHandler()],
)

# ì²´ì¸ êµ¬ì„±
chain = (
    {
        "context": retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough(),
    }
    | prompt
    | llm
)

# ì²´ì¸ ì‹¤í–‰
if message:
    send_message(message, "human")
    with st.chat_message("ai"):
        chain.invoke(message)
```

## ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„

### ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬

```python
from langchain.callbacks.base import BaseCallbackHandler

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""
    
    def on_llm_start(self, *args, **kwargs):
        # LLM ì‹œì‘ ì‹œ ë¹ˆ ë°•ìŠ¤ ìƒì„±
        self.message_box = st.empty()
    
    def on_llm_end(self, *args, **kwargs):
        # LLM ì¢…ë£Œ ì‹œ ë©”ì‹œì§€ ì €ì¥
        save_message(self.message, "ai")
    
    def on_llm_new_token(self, token, *args, **kwargs):
        # ìƒˆ í† í° ìˆ˜ì‹  ì‹œ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        self.message += token
        self.message_box.markdown(self.message)
```

**ì‘ë™ ì›ë¦¬**:
1. `on_llm_start`: AI ë©”ì‹œì§€ ì˜ì—­ì— ë¹ˆ ë°•ìŠ¤ ìƒì„±
2. `on_llm_new_token`: ê° í† í°ì„ ë°›ì„ ë•Œë§ˆë‹¤ ë°•ìŠ¤ ë‚´ìš© ì—…ë°ì´íŠ¸
3. `on_llm_end`: ì™„ì„±ëœ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥

## ì‹¤ìŠµ ì½”ë“œ ì˜ˆì œ

### ì™„ì „í•œ DocumentGPT êµ¬í˜„

```python
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st
import os

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[ChatCallbackHandler()],
)

@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./.cache/files", exist_ok=True)
    
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
        
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=splitter)
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )

def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context. 
            If you don't know the answer just say you don't know. 
            DON'T make anything up.
            
            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)

st.title("DocumentGPT")

st.markdown(
    """
Welcome!
            
Use this chatbot to ask questions to an AI about your files!

Upload your files on the sidebar.
"""
)

with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )

if file:
    retriever = embed_file(file)
    send_message("I'm ready! Ask away!", "ai", save=False)
    paint_history()
    message = st.chat_input("Ask anything about your file...")
    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            chain.invoke(message)
else:
    st.session_state["messages"] = []
```

## ì¤‘ìš” í¬ì¸íŠ¸ ì •ë¦¬

1. **ë°ì´í„° íë¦„**: Streamlitì€ ë°ì´í„° ë³€ê²½ ì‹œ ì „ì²´ ì½”ë“œë¥¼ ì¬ì‹¤í–‰
2. **Session State**: ì¬ì‹¤í–‰ ê°„ì— ë°ì´í„°ë¥¼ ìœ ì§€í•˜ëŠ” ìœ ì¼í•œ ë°©ë²•
3. **ìºì‹±**: `@st.cache_data`ë¡œ ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì—°ì‚° ìµœì í™”
4. **ìŠ¤íŠ¸ë¦¬ë°**: ì½œë°± í•¸ë“¤ëŸ¬ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ êµ¬í˜„
5. **íŒŒì¼ êµ¬ì¡°**: `pages/` í´ë”ë¡œ ë©€í‹°í˜ì´ì§€ ì•± êµ¬ì„±

## ì¶”ê°€ ê°œì„  ì‚¬í•­

### ë³´ì•ˆ ê°•í™”
```python
import re

def sanitize_filename(filename):
    # íŒŒì¼ëª…ì—ì„œ ìœ„í—˜í•œ ë¬¸ì ì œê±°
    return re.sub(r'[^\w\s.-]', '', filename)
```

### í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
`.streamlit/secrets.toml` íŒŒì¼ ìƒì„±:
```toml
OPENAI_API_KEY = "your-api-key"
```

**.gitignoreì— ì¶”ê°€**:
```
.streamlit/
.cache/
```

## ì½”ë“œ ì±Œë¦°ì§€

DocumentGPTì— ë©”ëª¨ë¦¬ ê¸°ëŠ¥ì„ ì¶”ê°€í•´ë³´ì„¸ìš”:
- ConversationBufferMemory í™œìš©
- ì´ì „ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€
- ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•œ ì—°ì†ì ì¸ ë‹µë³€ ê°€ëŠ¥

ì´ê²ƒìœ¼ë¡œ Streamlitê³¼ LangChainì„ í™œìš©í•œ DocumentGPT êµ¬ì¶• ê°€ì´ë“œë¥¼ ë§ˆì¹©ë‹ˆë‹¤.