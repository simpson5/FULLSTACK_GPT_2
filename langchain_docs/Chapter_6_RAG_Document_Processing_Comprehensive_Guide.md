# Chapter 6: RAG & Document Processing ì™„ë²½ ê°€ì´ë“œ

## ëª©ì°¨
1. [RAG ê°œìš”](#rag-ê°œìš”)
2. [ë°ì´í„° ë¡œë”ì™€ ë¬¸ì„œ ë¶„í• ](#ë°ì´í„°-ë¡œë”ì™€-ë¬¸ì„œ-ë¶„í• )
3. [Tiktokenê³¼ í† í° ê³„ì‚°](#tiktokenê³¼-í† í°-ê³„ì‚°)
4. [ë²¡í„°ì™€ ì„ë² ë”©](#ë²¡í„°ì™€-ì„ë² ë”©)
5. [ë²¡í„° ìŠ¤í† ì–´](#ë²¡í„°-ìŠ¤í† ì–´)
6. [LangSmith ëª¨ë‹ˆí„°ë§](#langsmith-ëª¨ë‹ˆí„°ë§)
7. [RetrievalQA](#retrievalqa)
8. [Stuff LCEL Chain](#stuff-lcel-chain)
9. [Map Reduce LCEL Chain](#map-reduce-lcel-chain)
10. [ì‹¤ìŠµ ì½”ë“œ ì˜ˆì œ](#ì‹¤ìŠµ-ì½”ë“œ-ì˜ˆì œ)

## RAG ê°œìš”

### RAG(Retrieval Augmented Generation)ë€?

**RAG**ëŠ” ì™¸ë¶€ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ì‘ë‹µì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

```mermaid
graph LR
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B[ë¬¸ì„œ ê²€ìƒ‰]
    B --> C[ê´€ë ¨ ë¬¸ì„œ]
    C --> D[í”„ë¡¬í”„íŠ¸ êµ¬ì„±]
    A --> D
    D --> E[LLM ì‘ë‹µ]
```

### RAGì˜ í•µì‹¬ ë‹¨ê³„

1. **Load**: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ
2. **Transform**: ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
3. **Embed**: í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
4. **Store**: ë²¡í„°ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
5. **Retrieve**: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰
6. **Generate**: ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±

### RAGì˜ ì¥ì 

- âœ… **ìµœì‹  ì •ë³´**: ì‹¤ì‹œê°„ìœ¼ë¡œ ì™¸ë¶€ ë¬¸ì„œ í™œìš©
- âœ… **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ë¶„ì•¼ì˜ ì „ë¬¸ ì§€ì‹ í™œìš©
- âœ… **íˆ¬ëª…ì„±**: ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œ í™•ì¸ ê°€ëŠ¥
- âœ… **ë¹„ìš© íš¨ìœ¨**: ëª¨ë¸ ì¬í›ˆë ¨ ì—†ì´ ì§€ì‹ í™•ì¥

## ë°ì´í„° ë¡œë”ì™€ ë¬¸ì„œ ë¶„í• 

### Document Loaders

LangChainì€ **50ê°€ì§€ ì´ìƒì˜ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤**ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

#### ê¸°ë³¸ ë¡œë”ë“¤

```python
# í…ìŠ¤íŠ¸ íŒŒì¼
from langchain.document_loaders import TextLoader
loader = TextLoader("document.txt")

# PDF íŒŒì¼
from langchain.document_loaders import PyPDFLoader
loader = PyPDFLoader("document.pdf")

# CSV íŒŒì¼
from langchain.document_loaders import CSVLoader
loader = CSVLoader("data.csv")
```

#### UnstructuredFileLoader (ê¶Œì¥)

**ë§ŒëŠ¥ ë¡œë”**ë¡œ ëŒ€ë¶€ë¶„ì˜ íŒŒì¼ í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤.

```python
from langchain.document_loaders import UnstructuredFileLoader

# ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›: PDF, DOCX, TXT, HTML, ì´ë¯¸ì§€ ë“±
loader = UnstructuredFileLoader("./files/chapter_one.docx")
documents = loader.load()

print(f"ë¬¸ì„œ ìˆ˜: {len(documents)}")
print(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ê¸¸ì´: {len(documents[0].page_content)}")
```

#### í†µí•© ë¡œë” ì˜ˆì œ

```python
from langchain.document_loaders import UnstructuredFileLoader
import os

def load_documents_from_directory(directory_path):
    """ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ë¡œë“œ"""
    documents = []
    supported_extensions = ['.txt', '.pdf', '.docx', '.html']
    
    for filename in os.listdir(directory_path):
        file_path = os.path.join(directory_path, filename)
        _, ext = os.path.splitext(filename)
        
        if ext.lower() in supported_extensions:
            loader = UnstructuredFileLoader(file_path)
            docs = loader.load()
            documents.extend(docs)
    
    return documents

# ì‚¬ìš© ì˜ˆì œ
documents = load_documents_from_directory("./documents/")
print(f"ì´ {len(documents)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")
```

### í…ìŠ¤íŠ¸ ë¶„í•  (Text Splitting)

#### ë¶„í• ì´ í•„ìš”í•œ ì´ìœ 

1. **í† í° ì œí•œ**: LLMì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ì œí•œ
2. **ê²€ìƒ‰ íš¨ìœ¨ì„±**: ì‘ì€ ì²­í¬ê°€ ë” ì •í™•í•œ ê²€ìƒ‰ ì œê³µ
3. **ë¹„ìš© ì ˆì•½**: ê´€ë ¨ ë¶€ë¶„ë§Œ LLMì— ì „ë‹¬

#### RecursiveCharacterTextSplitter

**ê°€ì¥ ì¼ë°˜ì ì¸ ë¶„í• ê¸°**ë¡œ ë¬¸ì¥ê³¼ ë‹¨ë½ì„ ë³´ì¡´í•©ë‹ˆë‹¤.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,    # ì²­í¬ ìµœëŒ€ í¬ê¸°
    chunk_overlap=100,  # ì²­í¬ ê°„ ì¤‘ë³µ
    length_function=len # ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜
)

# ë¬¸ì„œ ë¡œë“œ í›„ ë¶„í• 
loader = UnstructuredFileLoader("./files/document.txt")
documents = loader.load_and_split(text_splitter=splitter)

print(f"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
```

#### CharacterTextSplitter

**íŠ¹ì • êµ¬ë¶„ì**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

```python
from langchain.text_splitter import CharacterTextSplitter

# ë‹¨ë½ë³„ ë¶„í•  (ê¶Œì¥ ì„¤ì •)
splitter = CharacterTextSplitter(
    separator="\n",      # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„í• 
    chunk_size=600,      # ì ë‹¹í•œ í¬ê¸°
    chunk_overlap=100,   # ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ìš© ì¤‘ë³µ
)

documents = loader.load_and_split(text_splitter=splitter)
```

#### ì²­í¬ í¬ê¸° ìµœì í™”

```python
def analyze_document_stats(documents):
    """ë¬¸ì„œ í†µê³„ ë¶„ì„"""
    lengths = [len(doc.page_content) for doc in documents]
    
    return {
        "total_docs": len(documents),
        "avg_length": sum(lengths) / len(lengths),
        "min_length": min(lengths),
        "max_length": max(lengths),
        "total_chars": sum(lengths)
    }

# ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
configs = [
    {"chunk_size": 300, "chunk_overlap": 50},
    {"chunk_size": 600, "chunk_overlap": 100},
    {"chunk_size": 1000, "chunk_overlap": 150},
]

for config in configs:
    splitter = RecursiveCharacterTextSplitter(**config)
    docs = loader.load_and_split(text_splitter=splitter)
    stats = analyze_document_stats(docs)
    print(f"ì„¤ì • {config}: {stats}")
```

## Tiktokenê³¼ í† í° ê³„ì‚°

### Tiktoken ê°œìš”

**OpenAIì˜ ê³µì‹ í† í¬ë‚˜ì´ì €**ë¡œ ì •í™•í•œ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í† í° ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„í• 

```python
from langchain.text_splitter import CharacterTextSplitter

# Tiktoken ê¸°ë°˜ ë¶„í• ê¸°
splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n",
    chunk_size=600,     # í† í° ìˆ˜ ê¸°ì¤€
    chunk_overlap=100,
)

documents = loader.load_and_split(text_splitter=splitter)
```

### í† í° ìˆ˜ ê³„ì‚°

```python
import tiktoken

def count_tokens(text, model="gpt-3.5-turbo"):
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def analyze_token_usage(documents):
    """ë¬¸ì„œë“¤ì˜ í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„"""
    total_tokens = 0
    token_counts = []
    
    for doc in documents:
        tokens = count_tokens(doc.page_content)
        token_counts.append(tokens)
        total_tokens += tokens
    
    return {
        "total_tokens": total_tokens,
        "avg_tokens_per_doc": total_tokens / len(documents),
        "max_tokens": max(token_counts),
        "estimated_cost": total_tokens * 0.002 / 1000  # GPT-3.5 ê¸°ì¤€
    }

# í† í° ë¶„ì„
token_stats = analyze_token_usage(documents)
print(f"ì˜ˆìƒ ë¹„ìš©: ${token_stats['estimated_cost']:.4f}")
```

### ëª¨ë¸ë³„ ìµœì í™”

```python
def get_optimal_chunk_size(model_name):
    """ëª¨ë¸ë³„ ìµœì  ì²­í¬ í¬ê¸° ë°˜í™˜"""
    model_limits = {
        "gpt-3.5-turbo": 4096,
        "gpt-4": 8192,
        "gpt-4-turbo": 128000,
        "claude-2": 100000
    }
    
    limit = model_limits.get(model_name, 4096)
    # í”„ë¡¬í”„íŠ¸ ì˜¤ë²„í—¤ë“œë¥¼ ê³ ë ¤í•˜ì—¬ 70% ì‚¬ìš©
    return int(limit * 0.7)

# ëª¨ë¸ì— ë§ëŠ” ë¶„í• ê¸° ìƒì„±
def create_model_optimized_splitter(model_name="gpt-3.5-turbo"):
    chunk_size = get_optimal_chunk_size(model_name)
    overlap = min(100, chunk_size // 10)  # 10% ì¤‘ë³µ
    
    return CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=chunk_size,
        chunk_overlap=overlap,
    )
```

## ë²¡í„°ì™€ ì„ë² ë”©

### ì„ë² ë”© ê°œë…

**ì„ë² ë”©**ì€ í…ìŠ¤íŠ¸ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ìˆ˜ì¹˜í™”í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

# í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
text = "LangChain is a framework for developing applications powered by language models."
vector = embeddings.embed_query(text)

print(f"ë²¡í„° ì°¨ì›: {len(vector)}")
print(f"ë²¡í„° ì¼ë¶€: {vector[:5]}")
```

### ìœ ì‚¬ë„ ê³„ì‚°

```python
import numpy as np
from scipy.spatial.distance import cosine

def calculate_similarity(text1, text2, embeddings_model):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    vector1 = embeddings_model.embed_query(text1)
    vector2 = embeddings_model.embed_query(text2)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (1 - ì½”ì‚¬ì¸ ê±°ë¦¬)
    similarity = 1 - cosine(vector1, vector2)
    return similarity

# ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
texts = [
    "Python is a programming language",
    "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤",
    "The weather is sunny today",
    "Machine learning uses Python"
]

embeddings = OpenAIEmbeddings()
for i, text1 in enumerate(texts):
    for j, text2 in enumerate(texts[i+1:], i+1):
        sim = calculate_similarity(text1, text2, embeddings)
        print(f"'{text1}' vs '{text2}': {sim:.3f}")
```

### ì„ë² ë”© ìºì‹±

**ë¹„ìš© ì ˆì•½**ì„ ìœ„í•´ ì„ë² ë”© ê²°ê³¼ë¥¼ ìºì‹œí•©ë‹ˆë‹¤.

```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore

# ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
cache_dir = LocalFileStore("./.cache/embeddings/")

# ìºì‹œê°€ ì ìš©ëœ ì„ë² ë”©
embeddings = OpenAIEmbeddings()
cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings=embeddings,
    document_embedding_cache=cache_dir,
)

# ì²˜ìŒ í˜¸ì¶œ: API ìš”ì²­ ë°œìƒ
vector1 = cached_embeddings.embed_query("First query")

# ë‘ ë²ˆì§¸ í˜¸ì¶œ: ìºì‹œì—ì„œ ë°˜í™˜ (ë¹ ë¦„, ë¬´ë£Œ)
vector2 = cached_embeddings.embed_query("First query")
```

### ì„ë² ë”© ëª¨ë¸ ë¹„êµ

```python
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings

def compare_embedding_models():
    """ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
    models = {
        "OpenAI": OpenAIEmbeddings(),
        "HuggingFace": HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    }
    
    test_queries = [
        "What is machine learning?",
        "How does Python work?",
        "Explain neural networks"
    ]
    
    test_docs = [
        "Machine learning is a subset of AI that enables computers to learn without being explicitly programmed.",
        "Python is a high-level programming language known for its simplicity and versatility.",
        "Neural networks are computing systems inspired by biological neural networks."
    ]
    
    results = {}
    
    for model_name, model in models.items():
        print(f"\n{model_name} ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
        
        for query in test_queries:
            query_vector = model.embed_query(query)
            
            similarities = []
            for doc in test_docs:
                doc_vector = model.embed_query(doc)
                similarity = 1 - cosine(query_vector, doc_vector)
                similarities.append(similarity)
            
            best_match_idx = np.argmax(similarities)
            print(f"'{query}' -> '{test_docs[best_match_idx]}' (ìœ ì‚¬ë„: {similarities[best_match_idx]:.3f})")

# ëª¨ë¸ ë¹„êµ ì‹¤í–‰
compare_embedding_models()
```

## ë²¡í„° ìŠ¤í† ì–´

### ë²¡í„° ìŠ¤í† ì–´ ê°œìš”

**ë²¡í„° ìŠ¤í† ì–´**ëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.

### Chroma (ë¡œì»¬ ë²¡í„° ìŠ¤í† ì–´)

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings

# ë¬¸ì„œ ì¤€ë¹„
loader = UnstructuredFileLoader("./files/document.txt")
documents = loader.load_and_split(text_splitter=splitter)

# ì„ë² ë”© ëª¨ë¸
embeddings = OpenAIEmbeddings()

# Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory="./.cache/chroma"  # ì˜êµ¬ ì €ì¥
)

# ìœ ì‚¬ë„ ê²€ìƒ‰
query = "What is the main topic of this document?"
similar_docs = vectorstore.similarity_search(query, k=3)

for i, doc in enumerate(similar_docs):
    print(f"ë¬¸ì„œ {i+1}: {doc.page_content[:100]}...")
```

### FAISS (ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰)

```python
from langchain.vectorstores import FAISS

# FAISS ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ì¸ë©”ëª¨ë¦¬)
vectorstore = FAISS.from_documents(documents, embeddings)

# ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
vectorstore.save_local("./vectorstore")

# ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
new_vectorstore = FAISS.load_local("./vectorstore", embeddings)

# ê²€ìƒ‰ ê²°ê³¼ì™€ ì ìˆ˜
results = vectorstore.similarity_search_with_score(query, k=3)
for doc, score in results:
    print(f"ì ìˆ˜: {score:.3f} | ë‚´ìš©: {doc.page_content[:50]}...")
```

### Retriever ìƒì„±

```python
# ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰ê¸° ìƒì„±
retriever = vectorstore.as_retriever(
    search_type="similarity",    # ê²€ìƒ‰ ìœ í˜•
    search_kwargs={"k": 3}      # ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
)

# ê²€ìƒ‰ ì‹¤í–‰
retrieved_docs = retriever.get_relevant_documents(query)
print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(retrieved_docs)}")
```

### ê³ ê¸‰ ê²€ìƒ‰ ì˜µì…˜

```python
# MMR (Maximal Marginal Relevance) ê²€ìƒ‰
retriever_mmr = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20,      # ì´ˆê¸° í›„ë³´ ë¬¸ì„œ ìˆ˜
        "lambda_mult": 0.7  # ë‹¤ì–‘ì„± vs ê´€ë ¨ì„± ê· í˜•
    }
)

# ì„ê³„ê°’ ê¸°ë°˜ ê²€ìƒ‰
retriever_threshold = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.8,  # ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜
        "k": 10
    }
)
```

## LangSmith ëª¨ë‹ˆí„°ë§

### LangSmith ì„¤ì •

```python
import os

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-api-key"
os.environ["LANGCHAIN_PROJECT"] = "RAG-Project"
```

### ì¶”ì  ê°€ëŠ¥í•œ RAG ì²´ì¸

```python
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate

def create_traced_rag_chain(retriever, llm):
    """ì¶”ì  ê°€ëŠ¥í•œ RAG ì²´ì¸ ìƒì„±"""
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    prompt = ChatPromptTemplate.from_template("""
    ë‹µë³€í•  ë•Œ ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ëª¨ë¥´ëŠ” ê²½ìš° ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.

    ì»¨í…ìŠ¤íŠ¸: {context}

    ì§ˆë¬¸: {question}
    
    ë‹µë³€:
    """)
    
    chain = (
        {
            "context": retriever | RunnableLambda(format_docs),
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
    )
    
    return chain

# ì¶”ì ì´ í™œì„±í™”ëœ ì²´ì¸ ì‚¬ìš©
chain = create_traced_rag_chain(retriever, ChatOpenAI())
response = chain.invoke("ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?")
```

## RetrievalQA

### ê¸°ë³¸ RetrievalQA

```python
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# RetrievalQA ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    chain_type="stuff",      # ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
    retriever=retriever,
    return_source_documents=True  # ì†ŒìŠ¤ ë¬¸ì„œ ë°˜í™˜
)

# ì§ˆë¬¸-ë‹µë³€
result = qa_chain({"query": "ë¬¸ì„œì˜ í•µì‹¬ ì£¼ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?"})

print("ë‹µë³€:", result["result"])
print("\nì†ŒìŠ¤ ë¬¸ì„œ:")
for i, doc in enumerate(result["source_documents"]):
    print(f"{i+1}. {doc.page_content[:100]}...")
```

### ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸

```python
from langchain.prompts import PromptTemplate

# í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
korean_template = """
ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. 
ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³  "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

ë¬¸ë§¥:
{context}

ì§ˆë¬¸: {question}

í•œêµ­ì–´ë¡œ ìì„¸íˆ ë‹µë³€í•˜ì„¸ìš”:
"""

PROMPT = PromptTemplate(
    template=korean_template,
    input_variables=["context", "question"]
)

# ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ì ìš©
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(),
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True
)
```

## Stuff LCEL Chain

### Stuff ì „ëµ

**ëª¨ë“  ê²€ìƒ‰ëœ ë¬¸ì„œ**ë¥¼ í•˜ë‚˜ì˜ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```python
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.prompts import ChatPromptTemplate

def create_stuff_chain(retriever, llm):
    """Stuff ì „ëµì„ ì‚¬ìš©í•œ LCEL ì²´ì¸"""
    
    def format_docs(docs):
        """ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©"""
        return "\n\n".join([
            f"ë¬¸ì„œ {i+1}:\n{doc.page_content}"
            for i, doc in enumerate(docs)
        ])
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
        ("human", """
        ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:
        
        {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€ ì‹œ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:
        1. í•µì‹¬ ë‹µë³€
        2. ê·¼ê±° ë¬¸ì„œ ë²ˆí˜¸
        3. ì‹ ë¢°ë„ (1-10)
        """)
    ])
    
    chain = (
        {
            "context": retriever | RunnableLambda(format_docs),
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
    )
    
    return chain

# Stuff ì²´ì¸ ì‚¬ìš©
stuff_chain = create_stuff_chain(retriever, ChatOpenAI())
response = stuff_chain.invoke("ë¬¸ì„œì˜ ì£¼ìš” ê²°ë¡ ì€ ë¬´ì—‡ì¸ê°€ìš”?")
print(response.content)
```

### Stuff ì „ëµì˜ ì¥ë‹¨ì 

**ì¥ì **:
- ê°„ë‹¨í•œ êµ¬í˜„
- ëª¨ë“  ì»¨í…ìŠ¤íŠ¸ í™œìš©
- ì¼ê´€ëœ ë‹µë³€

**ë‹¨ì **:
- í† í° ì œí•œì— ì·¨ì•½
- ê¸´ ë¬¸ì„œ ì²˜ë¦¬ ì–´ë ¤ì›€
- ë¹„ìš©ì´ ë§ì´ ë“¦

## Map Reduce LCEL Chain

### Map-Reduce ì „ëµ

**í° ë¬¸ì„œë¥¼ ì²˜ë¦¬**í•˜ê¸° ìœ„í•´ ë¬¸ì„œë¥¼ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í•œ í›„ ê²°ê³¼ë¥¼ ê²°í•©í•©ë‹ˆë‹¤.

```python
from langchain.schema.runnable import RunnableParallel, RunnableLambda

def create_map_reduce_chain(retriever, llm):
    """Map-Reduce ì „ëµì„ ì‚¬ìš©í•œ LCEL ì²´ì¸"""
    
    # Map ë‹¨ê³„: ê° ë¬¸ì„œ ê°œë³„ ë¶„ì„
    map_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”."),
        ("human", "ë¬¸ì„œ: {doc}\n\nì§ˆë¬¸: {question}\n\nì´ ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”:")
    ])
    
    # Reduce ë‹¨ê³„: ìš”ì•½ë“¤ì„ ê²°í•©
    reduce_prompt = ChatPromptTemplate.from_messages([
        ("system", "ì—¬ëŸ¬ ë¬¸ì„œ ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”."),
        ("human", """
        ì§ˆë¬¸: {question}
        
        ë¬¸ì„œë³„ ìš”ì•½ë“¤:
        {summaries}
        
        ìœ„ ìš”ì•½ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”:
        """)
    ])
    
    def map_docs(inputs):
        """ê° ë¬¸ì„œì— ëŒ€í•´ Map ë‹¨ê³„ ì‹¤í–‰"""
        docs = inputs["docs"]
        question = inputs["question"]
        
        map_chain = map_prompt | llm
        
        summaries = []
        for doc in docs:
            summary = map_chain.invoke({
                "doc": doc.page_content,
                "question": question
            })
            summaries.append(summary.content)
        
        return {
            "summaries": "\n\n".join(summaries),
            "question": question
        }
    
    # ì „ì²´ ì²´ì¸ êµ¬ì„±
    chain = (
        RunnableParallel({
            "docs": retriever,
            "question": RunnablePassthrough()
        })
        | RunnableLambda(map_docs)
        | reduce_prompt
        | llm
    )
    
    return chain

# Map-Reduce ì²´ì¸ ì‚¬ìš©
map_reduce_chain = create_map_reduce_chain(retriever, ChatOpenAI())
response = map_reduce_chain.invoke("ë¬¸ì„œë“¤ì—ì„œ ì–¸ê¸‰ëœ ì£¼ìš” ê¸°ìˆ ë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?")
print(response.content)
```

### ì „ëµ ë¹„êµ

| ì „ëµ | ì¥ì  | ë‹¨ì  | ì ìš© ìƒí™© |
|------|------|------|-----------|
| **Stuff** | ê°„ë‹¨, ë¹ ë¦„, ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´ | í† í° ì œí•œ, ë¹„ìš© | ì§§ì€ ë¬¸ì„œ |
| **Map-Reduce** | ê¸´ ë¬¸ì„œ ì²˜ë¦¬, ë³‘ë ¬ ì²˜ë¦¬ | ë³µì¡, ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤ ê°€ëŠ¥ | ê¸´ ë¬¸ì„œ |
| **Refine** | ì ì§„ì  ê°œì„  | ìˆœì°¨ ì²˜ë¦¬ë¡œ ëŠë¦¼ | ì •í™•ë„ ì¤‘ìš” |
| **Map-Rerank** | ìµœì  ë‹µë³€ ì„ íƒ | ì¶”ê°€ ìˆœìœ„ ëª¨ë¸ í•„ìš” | ë‹¤ì–‘í•œ ê´€ì  |

## ì‹¤ìŠµ ì½”ë“œ ì˜ˆì œ

### ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•

```python
import os
from typing import List, Dict, Any
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores import FAISS
from langchain.storage import LocalFileStore
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.prompts import ChatPromptTemplate

class RAGSystem:
    """ì™„ì „í•œ RAG ì‹œìŠ¤í…œ êµ¬í˜„"""
    
    def __init__(self, 
                 chunk_size: int = 600,
                 chunk_overlap: int = 100,
                 cache_dir: str = "./.cache/"):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.cache_dir = cache_dir
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.text_splitter = self._create_text_splitter()
        self.embeddings = self._create_embeddings()
        self.llm = ChatOpenAI(temperature=0)
        self.vectorstore = None
        self.retriever = None
        
    def _create_text_splitter(self):
        """í…ìŠ¤íŠ¸ ë¶„í• ê¸° ìƒì„±"""
        return CharacterTextSplitter.from_tiktoken_encoder(
            separator="\n",
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
        )
    
    def _create_embeddings(self):
        """ìºì‹œëœ ì„ë² ë”© ëª¨ë¸ ìƒì„±"""
        base_embeddings = OpenAIEmbeddings()
        cache_store = LocalFileStore(os.path.join(self.cache_dir, "embeddings"))
        
        return CacheBackedEmbeddings.from_bytes_store(
            underlying_embeddings=base_embeddings,
            document_embedding_cache=cache_store,
        )
    
    def load_documents(self, file_paths: List[str]) -> List[Dict]:
        """ë¬¸ì„œë“¤ì„ ë¡œë“œí•˜ê³  ë¶„í• """
        all_documents = []
        
        for file_path in file_paths:
            if not os.path.exists(file_path):
                print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                continue
                
            try:
                loader = UnstructuredFileLoader(file_path)
                documents = loader.load_and_split(text_splitter=self.text_splitter)
                
                # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ ì •ë³´ ì¶”ê°€
                for doc in documents:
                    doc.metadata["source_file"] = os.path.basename(file_path)
                    doc.metadata["file_path"] = file_path
                
                all_documents.extend(documents)
                print(f"âœ… {file_path}: {len(documents)}ê°œ ì²­í¬ ë¡œë“œ")
                
            except Exception as e:
                print(f"âŒ {file_path} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return all_documents
    
    def create_vectorstore(self, documents: List[Dict]):
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
        if not documents:
            raise ValueError("ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ’¾ {len(documents)}ê°œ ë¬¸ì„œë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
        
        self.vectorstore = FAISS.from_documents(
            documents=documents,
            embedding=self.embeddings
        )
        
        # ê²€ìƒ‰ê¸° ìƒì„±
        self.retriever = self.vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 5, "fetch_k": 10}
        )
        
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
    
    def create_qa_chain(self):
        """QA ì²´ì¸ ìƒì„±"""
        if not self.retriever:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        def format_docs_with_sources(docs):
            """ì†ŒìŠ¤ ì •ë³´ë¥¼ í¬í•¨í•œ ë¬¸ì„œ í¬ë§·íŒ…"""
            formatted = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get("source_file", "Unknown")
                content = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content
                formatted.append(f"[ë¬¸ì„œ {i}] ({source})\n{content}")
            
            return "\n\n".join(formatted)
        
        prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ê·œì¹™:
1. ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ìƒì‹ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”  
3. ë‹µë³€ì— ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
4. í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”"""),
            
            ("human", """ë¬¸ì„œë“¤:
{context}

ì§ˆë¬¸: {question}

ìœ„ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. ë‹µë³€ ëì— ì°¸ê³ í•œ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ [ë¬¸ì„œ X] í˜•ì‹ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”:""")
        ])
        
        chain = (
            {
                "context": self.retriever | RunnableLambda(format_docs_with_sources),
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
        )
        
        return chain
    
    def query(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        if not hasattr(self, 'qa_chain'):
            self.qa_chain = self.create_qa_chain()
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.retriever.get_relevant_documents(question)
        
        # ë‹µë³€ ìƒì„±
        response = self.qa_chain.invoke(question)
        
        return {
            "question": question,
            "answer": response.content,
            "source_documents": relevant_docs,
            "num_sources": len(relevant_docs)
        }
    
    def save_vectorstore(self, path: str):
        """ë²¡í„° ìŠ¤í† ì–´ ì €ì¥"""
        if self.vectorstore:
            self.vectorstore.save_local(path)
            print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ë¨: {path}")
    
    def load_vectorstore(self, path: str):
        """ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            self.vectorstore = FAISS.load_local(path, self.embeddings)
            self.retriever = self.vectorstore.as_retriever(
                search_type="mmr",
                search_kwargs={"k": 5, "fetch_k": 10}
            )
            print(f"âœ… ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œë¨: {path}")
        except Exception as e:
            print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì œ
def main():
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = RAGSystem()
    
    # ë¬¸ì„œ ë¡œë“œ
    document_files = [
        "./files/chapter_one.docx",
        "./files/technical_doc.pdf",
        "./files/manual.txt"
    ]
    
    documents = rag.load_documents(document_files)
    
    if documents:
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        rag.create_vectorstore(documents)
        
        # ì§ˆë¬¸-ë‹µë³€ í…ŒìŠ¤íŠ¸
        questions = [
            "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì–¸ê¸‰ëœ ê¸°ìˆ ë“¤ì˜ íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "ê°€ì¥ ì¤‘ìš”í•œ ê²°ë¡ ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        for question in questions:
            print(f"\nğŸ¤” ì§ˆë¬¸: {question}")
            result = rag.query(question)
            print(f"ğŸ’¡ ë‹µë³€: {result['answer']}")
            print(f"ğŸ“š ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {result['num_sources']}")
        
        # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        rag.save_vectorstore("./vectorstore")

if __name__ == "__main__":
    main()
```

### í‰ê°€ ë° ìµœì í™”

```python
def evaluate_rag_performance():
    """RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€"""
    test_questions = [
        {
            "question": "ë¬¸ì„œì˜ ì£¼ì œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "expected_topics": ["machine learning", "AI", "technology"]
        },
        {
            "question": "ì–¸ê¸‰ëœ ì¥ì ë“¤ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "expected_topics": ["efficiency", "accuracy", "speed"]
        }
    ]
    
    rag = RAGSystem()
    # ... (ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±)
    
    results = []
    for test_case in test_questions:
        result = rag.query(test_case["question"])
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€
        answer_lower = result["answer"].lower()
        topic_matches = sum(1 for topic in test_case["expected_topics"] 
                          if topic in answer_lower)
        
        score = topic_matches / len(test_case["expected_topics"])
        
        results.append({
            "question": test_case["question"],
            "score": score,
            "answer_length": len(result["answer"]),
            "num_sources": result["num_sources"]
        })
    
    # í‰ê·  ì„±ëŠ¥
    avg_score = sum(r["score"] for r in results) / len(results)
    print(f"í‰ê·  ì ìˆ˜: {avg_score:.2f}")
    
    return results
```

## í•µì‹¬ í¬ì¸íŠ¸ ì •ë¦¬

### RAG ì‹œìŠ¤í…œ ì„¤ê³„ ì›ì¹™

1. **ë¬¸ì„œ í’ˆì§ˆ**: ì¢‹ì€ RAGëŠ” ì¢‹ì€ ë¬¸ì„œì—ì„œ ì‹œì‘
2. **ì²­í¬ í¬ê¸°**: ë„ˆë¬´ ì‘ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±, ë„ˆë¬´ í¬ë©´ ë…¸ì´ì¦ˆ ì¦ê°€
3. **ì„ë² ë”© ì„ íƒ**: ë„ë©”ì¸ì— ë§ëŠ” ì„ë² ë”© ëª¨ë¸ ì„ íƒ
4. **ê²€ìƒ‰ ì „ëµ**: MMR, ì„ê³„ê°’ ê¸°ë°˜ ë“± ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ë²• í™œìš©

### ì„±ëŠ¥ ìµœì í™” íŒ

1. **ìºì‹± í™œìš©**: ì„ë² ë”©ê³¼ ê²€ìƒ‰ ê²°ê³¼ ìºì‹±ìœ¼ë¡œ ë¹„ìš© ì ˆì•½
2. **ì²­í¬ ìµœì í™”**: ë„ë©”ì¸ë³„ ìµœì  ì²­í¬ í¬ê¸° ì‹¤í—˜
3. **ë©”íƒ€ë°ì´í„° í™œìš©**: ì†ŒìŠ¤, ë‚ ì§œ ë“±ìœ¼ë¡œ í•„í„°ë§ ê°œì„ 
4. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: í‚¤ì›Œë“œ + ë²¡í„° ê²€ìƒ‰ ì¡°í•©

### ì‹¤ë¬´ ì ìš© ê³ ë ¤ì‚¬í•­

1. **í™•ì¥ì„±**: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¶„ì‚° ë²¡í„° DB ê³ ë ¤
2. **ì‹¤ì‹œê°„ì„±**: ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹œ ë²¡í„° ìŠ¤í† ì–´ ë™ê¸°í™”
3. **í‰ê°€ ë©”íŠ¸ë¦­**: ì •í™•ë„, ê´€ë ¨ì„±, ì‘ë‹µ ì‹œê°„ ë“± ì¢…í•© í‰ê°€
4. **ì‚¬ìš©ì í”¼ë“œë°±**: ì‹¤ì œ ì‚¬ìš©ì í‰ê°€ë¥¼ í†µí•œ ì§€ì†ì  ê°œì„ 

ì´ê²ƒìœ¼ë¡œ LangChain RAG & Document Processingì˜ ì™„ë²½ ê°€ì´ë“œë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ë‹¤ìŒ ì¥ì—ì„œëŠ” Streamlitì„ í™œìš©í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•ì„ í•™ìŠµí•˜ê² ìŠµë‹ˆë‹¤.