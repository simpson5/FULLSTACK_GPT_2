###
# 05. Streamlit is ğŸ”¥
# ì˜¤ëŠ˜ì˜ ê°•ì˜: í’€ìŠ¤íƒ GPT: #7.0ë¶€í„° ~ #7.10ê¹Œì§€
# ì´ì „ ê³¼ì œì—ì„œ êµ¬í˜„í•œ RAG íŒŒì´í”„ë¼ì¸ì„ Streamlitìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
# íŒŒì¼ ì—…ë¡œë“œ, ì±„íŒ… ê¸°ë¡, ê·¸ë¦¬ê³  ConversationBufferMemoryë¥¼ í†µí•œ ëŒ€í™” ê¸°ì–µ ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
# st.sidebarë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ë¦¿ ì•±ì˜ ì½”ë“œì™€ í•¨ê»˜ ê¹ƒí—ˆë¸Œ ë¦¬í¬ì§€í† ë¦¬ì— ë§í¬ë¥¼ ë„£ìŠµë‹ˆë‹¤.
###

# LangChain ê´€ë ¨ imports - ê° ì»´í¬ë„ŒíŠ¸ì˜ ì—­í• ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder  # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±ìš©
from langchain.document_loaders import UnstructuredFileLoader  # ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ë¡œë“œ
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings  # ì„ë² ë”© ìƒì„± ë° ìºì‹±
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough  # LCEL ì²´ì¸ êµ¬ì„±ìš”ì†Œ
from langchain.storage import LocalFileStore  # ë¡œì»¬ íŒŒì¼ ê¸°ë°˜ ìºì‹œ ì €ì¥ì†Œ
from langchain.text_splitter import CharacterTextSplitter  # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
from langchain.vectorstores.faiss import FAISS  # ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ FAISS ë²¡í„°ìŠ¤í† ì–´
from langchain.chat_models import ChatOpenAI  # OpenAI ì±„íŒ… ëª¨ë¸
from langchain.callbacks.base import BaseCallbackHandler  # ì½œë°± í•¸ë“¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤
from langchain.memory import ConversationBufferMemory  # ëŒ€í™” ê¸°ë¡ ë©”ëª¨ë¦¬
import streamlit as st  # Streamlit ì›¹ ì•± í”„ë ˆì„ì›Œí¬
import os  # íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…ìš©

# Streamlit í˜ì´ì§€ ì„¤ì • - ë¸Œë¼ìš°ì € íƒ­ì— í‘œì‹œë  ì œëª©ê³¼ ì•„ì´ì½˜ ì„¤ì •
st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)


# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ êµ¬í˜„í•˜ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ (ë©”ëª¨ë¦¬ í†µí•©)
# LLMì´ ìƒì„±í•˜ëŠ” ê° í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— í‘œì‹œí•˜ê³  LangChain ë©”ëª¨ë¦¬ì— ì €ì¥
class ChatCallbackHandler(BaseCallbackHandler):
    def __init__(self, question=""):
        self.message = ""  # ëˆ„ì ë  ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ëŠ” ë³€ìˆ˜
        self.question = question  # í˜„ì¬ ì§ˆë¬¸ ì €ì¥

    def on_llm_start(self, *args, **kwargs):
        # LLMì´ ì‘ë‹µ ìƒì„±ì„ ì‹œì‘í•  ë•Œ í˜¸ì¶œ
        # st.empty()ë¡œ ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ìƒì„±í•˜ì—¬ ë‚˜ì¤‘ì— ë‚´ìš©ì„ ì—…ë°ì´íŠ¸
        self.message_box = st.empty()
        self.message = ""  # ë©”ì‹œì§€ ì´ˆê¸°í™”

    def on_llm_end(self, *args, **kwargs):
        # LLMì´ ì‘ë‹µ ìƒì„±ì„ ì™„ë£Œí–ˆì„ ë•Œ í˜¸ì¶œ
        # Streamlit ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        save_message(self.message, "ai")
        # LangChain ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
        if self.question and self.message:
            memory.save_context(
                {"input": self.question},      # ì‚¬ìš©ì ì…ë ¥
                {"output": self.message}       # AI ì‘ë‹µ
            )

    def on_llm_new_token(self, token, *args, **kwargs):
        # ìƒˆë¡œìš´ í† í°(ê¸€ì)ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ í˜¸ì¶œ
        self.message += token  # ê¸°ì¡´ ë©”ì‹œì§€ì— ìƒˆ í† í° ì¶”ê°€
        self.message_box.markdown(self.message)  # í™”ë©´ì˜ ë©”ì‹œì§€ ë°•ìŠ¤ ì—…ë°ì´íŠ¸


# ChatOpenAI ëª¨ë¸ì€ ì´ì œ ê° í•¨ìˆ˜ì—ì„œ í•„ìš”ì‹œ ë™ì ìœ¼ë¡œ ìƒì„±
# (ì„¸ì…˜ì˜ API í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±)


# ConversationBufferMemory ì´ˆê¸°í™” - LangChain ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
# return_messages: Trueë¡œ ì„¤ì •í•˜ì—¬ HumanMessage/AIMessage ê°ì²´ë¡œ ì €ì¥ (ì±„íŒ… ëª¨ë¸ í˜¸í™˜)
# memory_key: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì˜ ë³€ìˆ˜ëª…ê³¼ ì¼ì¹˜í•´ì•¼ í•¨
memory = ConversationBufferMemory(
    return_messages=True,      # ë©”ì‹œì§€ í˜•íƒœ ë°˜í™˜
    memory_key="chat_history"  # í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜ëª…
)


# @st.cache_data ë°ì½”ë ˆì´í„°: ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì—°ì‚°ì„ ìºì‹±
# ë™ì¼í•œ íŒŒì¼ì— ëŒ€í•´ì„œëŠ” í•¨ìˆ˜ë¥¼ ì¬ì‹¤í–‰í•˜ì§€ ì•Šê³  ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
# show_spinner: ì²˜ë¦¬ ì¤‘ í‘œì‹œí•  ë©”ì‹œì§€
@st.cache_data(show_spinner="Embedding file...")
def embed_file(file, api_key):
    """
    íŒŒì¼ì„ ì„ë² ë”©í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    - file: ì—…ë¡œë“œëœ íŒŒì¼ ê°ì²´
    - api_key: OpenAI API í‚¤ (ìºì‹œ ë¬´íš¨í™”ë¥¼ ìœ„í•´ ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬)
    
    Returns:
    - retriever: FAISS ê²€ìƒ‰ê¸°
    - docs: ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸  
    - file_info: íŒŒì¼ ì²˜ë¦¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    # API í‚¤ ìœ íš¨ì„± ì‚¬ì „ ê²€ì¦
    if not api_key or not api_key.startswith('sk-'):
        raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ OpenAI API í‚¤ì…ë‹ˆë‹¤. 'sk-'ë¡œ ì‹œì‘í•˜ëŠ” í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    # ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¡œì»¬ì— ì €ì¥í•˜ëŠ” ê³¼ì •
    file_content = file.read()  # íŒŒì¼ ë‚´ìš©ì„ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
    file_size_kb = len(file_content) / 1024  # KB ë‹¨ìœ„ë¡œ íŒŒì¼ í¬ê¸° ê³„ì‚°
    
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„± (Streamlit Cloud í˜¸í™˜)
    os.makedirs("./.cache/files", exist_ok=True)
    os.makedirs(f"./.cache/embeddings/{file.name}", exist_ok=True)
    
    file_path = f"./.cache/files/{file.name}"  # ì €ì¥í•  ê²½ë¡œ ì„¤ì •
    # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ ì“°ê¸° ëª¨ë“œë¡œ ì—´ì–´ ì €ì¥
    with open(file_path, "wb") as f:
        f.write(file_content)
    # ì„ë² ë”© ìºì‹œë¥¼ ìœ„í•œ ë¡œì»¬ íŒŒì¼ ì €ì¥ì†Œ ì„¤ì •
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    
    # ë¬¸ì„œ í¬ê¸°ì— ë”°ë¥¸ ì ì‘í˜• ì²­í‚¹ ì „ëµ
    if file_size_kb <= 10:  # 10KB ì´í•˜ - ì‘ì€ ë¬¸ì„œ
        chunk_size, chunk_overlap = 2000, 200
    elif file_size_kb <= 50:  # 10-50KB - ì¤‘ê°„ ë¬¸ì„œ (25KB ì†Œì„¤ í¬í•¨)
        chunk_size, chunk_overlap = 4000, 400
    elif file_size_kb <= 200:  # 50-200KB - í° ë¬¸ì„œ
        chunk_size, chunk_overlap = 6000, 600
    else:  # 200KB ì´ˆê³¼ - ë§¤ìš° í° ë¬¸ì„œ
        chunk_size, chunk_overlap = 8000, 800
    
    # ì ì‘í˜• í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
    )
    # UnstructuredFileLoader: PDF, TXT, DOCX ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
    loader = UnstructuredFileLoader(file_path)
    # ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì„¤ì •í•œ splitterë¡œ ë¶„í• 
    docs = loader.load_and_split(text_splitter=splitter)
    
    # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ëª…ì‹œì  API í‚¤ ì‚¬ìš©)
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=api_key)
        # ìºì‹œ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ì„ë² ë”© ìƒì„± - ë™ì¼í•œ í…ìŠ¤íŠ¸ëŠ” ì¬ê³„ì‚°í•˜ì§€ ì•ŠìŒ
        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    except Exception as e:
        st.error(f"âŒ OpenAI API ì¸ì¦ ì‹¤íŒ¨: {str(e)}")
        st.error("ğŸ”‘ API í‚¤ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        raise e
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± - ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„° DB
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    
    # ë¬¸ì„œ í¬ê¸°ì— ë”°ë¥¸ ì ì‘í˜• ê²€ìƒ‰ ì „ëµ
    num_chunks = len(docs)
    if num_chunks <= 3:  # ì ì€ ì²­í¬ - ëª¨ë“  ì²­í¬ ê²€ìƒ‰
        k = num_chunks
    elif num_chunks <= 10:  # ì¤‘ê°„ ì²­í¬ - ëŒ€ë¶€ë¶„ ê²€ìƒ‰
        k = max(8, num_chunks - 2)
    else:  # ë§ì€ ì²­í¬ - ìƒìœ„ 15ê°œ ê²€ìƒ‰
        k = 15
    
    # ê²€ìƒ‰ê¸°(retriever) ìƒì„± - ì ì‘í˜• ê²€ìƒ‰
    retriever = vectorstore.as_retriever(
        search_kwargs={"k": k}
    )
    
    # íŒŒì¼ ì •ë³´ ë°˜í™˜ (UI í‘œì‹œìš©)
    file_info = {
        "size_kb": round(file_size_kb, 1),
        "chunk_size": chunk_size,
        "chunk_overlap": chunk_overlap,
        "num_chunks": num_chunks,
        "search_k": k
    }
    
    return retriever, docs, file_info


# ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
# Streamlitì€ ì¬ì‹¤í–‰ ì‹œ ëª¨ë“  ë³€ìˆ˜ê°€ ì´ˆê¸°í™”ë˜ë¯€ë¡œ st.session_state ì‚¬ìš©
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


# ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ê³  ì„ íƒì ìœ¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def send_message(message, role, save=True):
    # st.chat_messageë¡œ ì—­í• (human/ai)ì— ë§ëŠ” ì±„íŒ… UI ìƒì„±
    with st.chat_message(role):
        st.markdown(message)  # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ í‘œì‹œ
    # save=Trueì¼ ë•Œë§Œ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    if save:
        save_message(message, role)


# ì €ì¥ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™”ë©´ì— ë‹¤ì‹œ ê·¸ë¦¬ëŠ” í•¨ìˆ˜
# Streamlitì´ ì¬ì‹¤í–‰ë  ë•Œë§ˆë‹¤ ì´ì „ ëŒ€í™”ë¥¼ ë³µì›
def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,  # ì´ë¯¸ ì €ì¥ëœ ë©”ì‹œì§€ì´ë¯€ë¡œ ë‹¤ì‹œ ì €ì¥í•˜ì§€ ì•ŠìŒ
        )


# ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
# ê° ë¬¸ì„œ ë‚´ìš©ì„ ë‘ ì¤„ ë„ì›€(\n\n)ìœ¼ë¡œ êµ¬ë¶„
def format_docs(docs):
    return "\n\n".join(document.page_content for document in docs)


# ë¬¸ì„œ ì „ì²´ ìš”ì•½ì„ ìœ„í•œ í•¨ìˆ˜
def summarize_document(docs):
    """
    ë¬¸ì„œ ì „ì²´ë¥¼ ìš”ì•½í•˜ì—¬ ì‚¬ìš©ìê°€ ì „ì²´ ë‚´ìš©ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ í•¨
    Map-Reduce ë°©ì‹ìœ¼ë¡œ ê¸´ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìš”ì•½
    """
    if not docs:
        return "ìš”ì•½í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    summary_prompt = ChatPromptTemplate.from_template(
        """ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”. 
        ì£¼ìš” ë‚´ìš©ê³¼ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ í¬í•¨í•´ì„œ 3-5ê°œì˜ ë¬¸ë‹¨ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
        
        ë¬¸ì„œ ë‚´ìš©:
        {text}
        
        ìš”ì•½:"""
    )
    
    # ê° ì²­í¬ë³„ ìš”ì•½ ìƒì„±
    chunk_summaries = []
    # API í‚¤ê°€ ì„¸ì…˜ì— ìˆì„ ë•Œë§Œ ìš”ì•½ ê°€ëŠ¥
    if not st.session_state.get("api_key"):
        return "âŒ API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    summary_llm = ChatOpenAI(
        temperature=0.1,
        openai_api_key=st.session_state["api_key"]
    )
    
    # ì²­í¬ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ì²˜ë¦¬ (5ê°œì”©)
    for i in range(0, len(docs), 5):
        chunk_group = docs[i:i+5]
        combined_text = "\n\n".join([doc.page_content for doc in chunk_group])
        
        try:
            summary_chain = summary_prompt | summary_llm
            summary = summary_chain.invoke({"text": combined_text})
            chunk_summaries.append(summary.content)
        except Exception as e:
            chunk_summaries.append(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    # ëª¨ë“  ì²­í¬ ìš”ì•½ì„ ìµœì¢… ìš”ì•½ìœ¼ë¡œ í†µí•©
    if len(chunk_summaries) > 1:
        final_text = "\n\n".join(chunk_summaries)
        final_summary_chain = summary_prompt | summary_llm
        try:
            final_summary = final_summary_chain.invoke({"text": final_text})
            return final_summary.content
        except Exception as e:
            return f"ìµœì¢… ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n\nê°œë³„ ìš”ì•½ë“¤:\n" + "\n\n".join(chunk_summaries)
    else:
        return chunk_summaries[0] if chunk_summaries else "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


# LangChain ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¡œë“œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
# LCEL ì²´ì¸ì—ì„œ ì‚¬ìš©ë˜ë©°, ì²´ì¸ ì‹¤í–‰ ì‹œë§ˆë‹¤ ìë™ìœ¼ë¡œ í˜¸ì¶œë¨
def load_memory(_):
    """
    ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¡œë“œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    ì²´ì¸ ì…ë ¥ ë”•ì…”ë„ˆë¦¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (_ë¡œ í‘œì‹œ)
    """
    memory_vars = memory.load_memory_variables({})
    return memory_vars["chat_history"]


# RAG(Retrieval Augmented Generation)ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ë©”ëª¨ë¦¬ í¬í•¨)
# system: AIì˜ í–‰ë™ ì§€ì¹¨ ì„¤ì •
# MessagesPlaceholder: ëŒ€í™” ê¸°ë¡ì´ ì‚½ì…ë  ìœ„ì¹˜
# human: ì‚¬ìš©ìì˜ ì§ˆë¬¸
# {context}, {chat_history}, {question}ì€ ëŸ°íƒ€ì„ì— ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜ë¨
prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        """
        ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
        ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ê³  ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
        
        ì»¨í…ìŠ¤íŠ¸:
        {context}
        """
    ),
    MessagesPlaceholder(variable_name="chat_history"),  # ëŒ€í™” ê¸°ë¡ì´ ì‚½ì…ë  ìœ„ì¹˜
    ("human", "{question}")  # ì‚¬ìš©ì ì§ˆë¬¸ì´ ì „ë‹¬ë  í”Œë ˆì´ìŠ¤í™€ë”
])


# í˜ì´ì§€ ì œëª© ì„¤ì •
st.title("DocumentGPT")

# í™˜ì˜ ë©”ì‹œì§€ì™€ ì‚¬ìš© ì•ˆë‚´ (íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ì„ ë•Œë§Œ í‘œì‹œ)
if "file_uploaded" not in st.session_state or not st.session_state.file_uploaded:
    if not st.session_state.get("api_key"):
        st.markdown(
            """
            ## ğŸš€ ì‹œì‘í•˜ê¸°
            
            1. ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ **OpenAI API Key**ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”
            2. API Key ë°œê¸‰: [OpenAI Platform](https://platform.openai.com/api-keys)
            3. API Key ì…ë ¥ í›„ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”
            """
        )
    else:
        st.markdown(
            """
            ## ğŸ“„ í™˜ì˜í•©ë‹ˆë‹¤!

            ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì—¬ ì‹œì‘í•˜ì„¸ìš”.

            ì—…ë¡œë“œí•œ ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ë³´ì„¸ìš”!
            """
        )

# ì‚¬ì´ë“œë°”ì— API í‚¤ ì…ë ¥ ë° íŒŒì¼ ì—…ë¡œë” ìœ„ì ¯ ìƒì„±
with st.sidebar:
    st.markdown("## ğŸ”‘ API Configuration")
    
    # API í‚¤ ì…ë ¥ (ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ì—¬ ìœ ì§€)
    api_key = st.text_input(
        "OpenAI API Key",
        value=st.session_state.get("api_key", ""),
        type="password",
        placeholder="sk-...",
        help="https://platform.openai.com/api-keys ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    # API í‚¤ ì €ì¥ ë° ëª¨ë¸ ì´ˆê¸°í™”
    if api_key:
        st.session_state["api_key"] = api_key
        import os
        os.environ["OPENAI_API_KEY"] = api_key
        
        # API í‚¤ ìœ íš¨ì„± ê²€ì¦
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ë¡œ API í‚¤ í™•ì¸
            test_llm = ChatOpenAI(
                temperature=0.1,
                openai_api_key=api_key
            )
            st.success("âœ… API Key ì„¤ì • ì™„ë£Œ!")
        except Exception as e:
            st.error(f"âŒ API Key ì˜¤ë¥˜: ìœ íš¨í•˜ì§€ ì•Šì€ API Keyì…ë‹ˆë‹¤.")
            api_key = None
            if "api_key" in st.session_state:
                del st.session_state["api_key"]
    else:
        if "api_key" in st.session_state:
            del st.session_state["api_key"]
    
    st.markdown("---")
    st.markdown("## ğŸ“„ Document Upload")
    
    # API í‚¤ê°€ ì—†ìœ¼ë©´ íŒŒì¼ ì—…ë¡œë“œ ë¹„í™œì„±í™”
    if not api_key:
        st.warning("âš ï¸ ë¨¼ì € OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        file = None
    else:
        file = st.file_uploader(
            "Upload a .txt .pdf or .docx file",
            type=["pdf", "txt", "docx"],  # í—ˆìš©ëœ íŒŒì¼ í˜•ì‹ ì§€ì •
        )
    
    # ë¬¸ì„œê°€ ì—…ë¡œë“œëœ ê²½ìš° ìš”ì•½ ë²„íŠ¼ í‘œì‹œ
    if file:
        st.markdown("---")
        st.markdown("## Document Analysis")
        if st.button("ğŸ“„ ë¬¸ì„œ ì „ì²´ ìš”ì•½"):
            st.session_state["show_summary"] = True
        
        if st.button("ğŸ” ê²€ìƒ‰ ì •ë³´"):
            st.session_state["show_search_info"] = True
    
    st.markdown("---")
    st.markdown("## About")
    st.markdown("DocumentGPT with Memory - Enhanced with conversation history")
    st.markdown("**ì ì‘í˜• ì „ëµ**: ë¬¸ì„œ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ì²­í‚¹ìœ¼ë¡œ ì „ì²´ ë¬¸ì„œ ì™„ë²½ ë¶„ì„!")
    st.markdown("â€¢ 10KB â†“: 2Kí† í° â€¢ 25KB: 4Kí† í° â€¢ 200KB: 6Kí† í° â€¢ 200KB+: 8Kí† í°")
    st.markdown("[GitHub Repository](https://github.com/your-repo)")  # 7ê°• ìš”êµ¬ì‚¬í•­: ê¹ƒí—ˆë¸Œ ë§í¬

# íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš°ì˜ ë©”ì¸ ë¡œì§
if file:
    # íŒŒì¼ ì—…ë¡œë“œ ìƒíƒœ ì„¤ì • (í™˜ì˜ ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° ìœ„í•¨)
    st.session_state.file_uploaded = True
    
    # íŒŒì¼ì„ ì„ë² ë”©í•˜ê³  ê²€ìƒ‰ê¸°, ë¬¸ì„œ ì²­í¬, íŒŒì¼ ì •ë³´ ìƒì„± (ìºì‹œë¨)
    try:
        retriever, docs, file_info = embed_file(file, st.session_state["api_key"])
    except Exception as e:
        st.error("ğŸ’¥ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        st.error("ğŸ”§ í•´ê²° ë°©ë²•:")
        st.error("1ï¸âƒ£ API í‚¤ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        st.error("2ï¸âƒ£ OpenAI ê³„ì •ì— ì¶©ë¶„í•œ í¬ë ˆë”§ì´ ìˆëŠ”ì§€ í™•ì¸")
        st.error("3ï¸âƒ£ ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸")
        st.stop()
    
    # ë¬¸ì„œ ì •ë³´ í‘œì‹œ
    st.success(f"ğŸ“„ **{file.name}** ({file_info['size_kb']}KB) ì—…ë¡œë“œ ì™„ë£Œ!")
    
    # ë¬¸ì„œ ìš”ì•½ í‘œì‹œ
    if st.session_state.get("show_summary", False):
        st.markdown("## ğŸ“„ ë¬¸ì„œ ì „ì²´ ìš”ì•½")
        with st.spinner("ë¬¸ì„œë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘..."):
            summary = summarize_document(docs)
            st.markdown(summary)
            st.markdown("---")
        st.session_state["show_summary"] = False
    
    # ê²€ìƒ‰ ì •ë³´ í‘œì‹œ
    if st.session_state.get("show_search_info", False):
        st.markdown("## ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì„¤ì • ì •ë³´")
        st.info(f"""
        **ë¬¸ì„œ ì²˜ë¦¬ ì •ë³´:**
        - íŒŒì¼ í¬ê¸°: {file_info['size_kb']}KB
        - ì´ ì²­í¬ ìˆ˜: {file_info['num_chunks']}ê°œ
        - ì²­í¬ í¬ê¸°: {file_info['chunk_size']:,} í† í° (ì ì‘í˜•)
        - ì²­í¬ ì¤‘ë³µ: {file_info['chunk_overlap']:,} í† í°
        - ê²€ìƒ‰ ë°©ì‹: ë‹¨ìˆœ ìœ ì‚¬ë„ ê²€ìƒ‰
        - ê²€ìƒ‰ ì²­í¬ ìˆ˜: {file_info['search_k']}ê°œ
        
        **ì ì‘í˜• ì „ëµ:**
        - ë¬¸ì„œ í¬ê¸°ì— ë”°ë¥¸ ìµœì  ì²­í¬ í¬ê¸° ìë™ ì¡°ì •
        - ì²­í¬ ìˆ˜ì— ë”°ë¥¸ ê²€ìƒ‰ ë²”ìœ„ ìµœì í™”
        - ë¬¸ì„œ ì „ì²´ ì»¤ë²„ë¦¬ì§€ ë³´ì¥ìœ¼ë¡œ ê²°ë§ê¹Œì§€ ì •í™•í•œ ê²€ìƒ‰
        """)
        st.session_state["show_search_info"] = False
    
    # AI ì¤€ë¹„ ì™„ë£Œ ë©”ì‹œì§€ (ì €ì¥í•˜ì§€ ì•ŠìŒ)
    send_message("ì¤€ë¹„ì™„ë£Œ! ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!", "ai", save=False)
    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    paint_history()

    # ì±„íŒ… ì…ë ¥ ìœ„ì ¯ ìƒì„±
    message = st.chat_input("Ask anything about your file...")
    
    if message:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
        send_message(message, "human")
        
        # í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„± (ë©”ëª¨ë¦¬ ì €ì¥ í¬í•¨)
        callback_handler = ChatCallbackHandler(question=message)
        
        # API í‚¤ í™•ì¸ í›„ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        if not st.session_state.get("api_key"):
            st.error("âŒ API Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            st.stop()
        
        # ì§ˆë¬¸ë³„ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì½œë°± í•¸ë“¤ëŸ¬ í¬í•¨)
        llm_with_callback = ChatOpenAI(
            temperature=0.1,
            streaming=True,
            callbacks=[callback_handler],
            openai_api_key=st.session_state["api_key"],  # ì„¸ì…˜ì˜ API í‚¤ ì‚¬ìš©
        )
        
        # LCEL(LangChain Expression Language) ì²´ì¸ êµ¬ì„± (ë©”ëª¨ë¦¬ í¬í•¨)
        # 1. RunnablePassthrough.assignìœ¼ë¡œ ë™ì  ë³€ìˆ˜ ì£¼ì…
        # 2. chat_history: ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ë¡œë“œ
        # 3. context: ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ í›„ í¬ë§·
        # 4. prompt: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë³€ìˆ˜ë“¤ ì ìš©
        # 5. llm: ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ì‘ë‹µ ìƒì„±
        chain = (
            RunnablePassthrough.assign(
                chat_history=load_memory,    # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ë¡œë“œ
                context=lambda x: format_docs(retriever.get_relevant_documents(x["question"]))
            )
            | prompt
            | llm_with_callback
        )
        # AI ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì²´ì¸ ì‹¤í–‰
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì´ ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ í†µí•´ ì‹¤ì‹œê°„ í‘œì‹œë¨
        with st.chat_message("ai"):
            chain.invoke({"question": message})

# íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
else:
    # íŒŒì¼ ì—…ë¡œë“œ ìƒíƒœ í•´ì œ (í™˜ì˜ ë©”ì‹œì§€ ë‹¤ì‹œ í‘œì‹œ)
    st.session_state.file_uploaded = False
    st.session_state["messages"] = []
    # LangChain ë©”ëª¨ë¦¬ë„ ì´ˆê¸°í™”
    memory.clear()