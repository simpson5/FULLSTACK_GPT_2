###
# ì˜¤ëŠ˜ì˜ ê°•ì˜: í’€ìŠ¤íƒ GPT: #7.0ë¶€í„° ~ #7.10ê¹Œì§€
# ì´ì „ ê³¼ì œì—ì„œ êµ¬í˜„í•œ RAG íŒŒì´í”„ë¼ì¸ì„ Streamlitìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•©ë‹ˆë‹¤.
# íŒŒì¼ ì—…ë¡œë“œ ë° ì±„íŒ… ê¸°ë¡ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
# ì‚¬ìš©ìê°€ ìì²´ OpenAI API í‚¤ë¥¼ ì‚¬ìš©í•˜ë„ë¡ í—ˆìš©í•˜ê³ , st.sidebar ë‚´ë¶€ì˜ st.inputì—ì„œ ì´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
# st.sidebarë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¼ë¦¿ ì•±ì˜ ì½”ë“œì™€ í•¨ê»˜ ê¹ƒí—ˆë¸Œ ë¦¬í¬ì§€í† ë¦¬ì— ë§í¬ë¥¼ ë„£ìŠµë‹ˆë‹¤.
###

# LangChain ê´€ë ¨ imports - ê° ì»´í¬ë„ŒíŠ¸ì˜ ì—­í• ì„ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”
from langchain.prompts import ChatPromptTemplate  # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±ìš©
from langchain.document_loaders import UnstructuredFileLoader  # ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ë¡œë“œ
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings  # ì„ë² ë”© ìƒì„± ë° ìºì‹±
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough  # LCEL ì²´ì¸ êµ¬ì„±ìš”ì†Œ
from langchain.storage import LocalFileStore  # ë¡œì»¬ íŒŒì¼ ê¸°ë°˜ ìºì‹œ ì €ì¥ì†Œ
from langchain.text_splitter import CharacterTextSplitter  # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
from langchain.vectorstores.faiss import FAISS  # ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ FAISS ë²¡í„°ìŠ¤í† ì–´
from langchain.chat_models import ChatOpenAI  # OpenAI ì±„íŒ… ëª¨ë¸
from langchain.callbacks.base import BaseCallbackHandler  # ì½œë°± í•¸ë“¤ëŸ¬ ë² ì´ìŠ¤ í´ë˜ìŠ¤
import streamlit as st  # Streamlit ì›¹ ì•± í”„ë ˆì„ì›Œí¬

# Streamlit í˜ì´ì§€ ì„¤ì • - ë¸Œë¼ìš°ì € íƒ­ì— í‘œì‹œë  ì œëª©ê³¼ ì•„ì´ì½˜ ì„¤ì •
st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)


# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ êµ¬í˜„í•˜ê¸° ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬
# LLMì´ ìƒì„±í•˜ëŠ” ê° í† í°ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™”ë©´ì— í‘œì‹œ
class ChatCallbackHandler(BaseCallbackHandler):
    message = ""  # ëˆ„ì ë  ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ëŠ” ë³€ìˆ˜

    def on_llm_start(self, *args, **kwargs):
        # LLMì´ ì‘ë‹µ ìƒì„±ì„ ì‹œì‘í•  ë•Œ í˜¸ì¶œ
        # st.empty()ë¡œ ë¹ˆ ì»¨í…Œì´ë„ˆë¥¼ ìƒì„±í•˜ì—¬ ë‚˜ì¤‘ì— ë‚´ìš©ì„ ì—…ë°ì´íŠ¸
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        # LLMì´ ì‘ë‹µ ìƒì„±ì„ ì™„ë£Œí–ˆì„ ë•Œ í˜¸ì¶œ
        # ì™„ì„±ëœ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        # ìƒˆë¡œìš´ í† í°(ê¸€ì)ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ í˜¸ì¶œ
        self.message += token  # ê¸°ì¡´ ë©”ì‹œì§€ì— ìƒˆ í† í° ì¶”ê°€
        self.message_box.markdown(self.message)  # í™”ë©´ì˜ ë©”ì‹œì§€ ë°•ìŠ¤ ì—…ë°ì´íŠ¸


# ChatOpenAI ëª¨ë¸ ì´ˆê¸°í™”
# temperature: 0.1ë¡œ ì„¤ì •í•˜ì—¬ ë” ì¼ê´€ëœ ì‘ë‹µ ìƒì„± (0=ê²°ì •ì , 1=ì°½ì˜ì )
# streaming: Trueë¡œ ì„¤ì •í•˜ì—¬ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”
# callbacks: ìŠ¤íŠ¸ë¦¬ë°ì„ ì²˜ë¦¬í•  ì»¤ìŠ¤í…€ ì½œë°± í•¸ë“¤ëŸ¬ ë“±ë¡
llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)


# @st.cache_data ë°ì½”ë ˆì´í„°: ë¹„ìš©ì´ ë§ì´ ë“œëŠ” ì—°ì‚°ì„ ìºì‹±
# ë™ì¼í•œ íŒŒì¼ì— ëŒ€í•´ì„œëŠ” í•¨ìˆ˜ë¥¼ ì¬ì‹¤í–‰í•˜ì§€ ì•Šê³  ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
# show_spinner: ì²˜ë¦¬ ì¤‘ í‘œì‹œí•  ë©”ì‹œì§€
@st.cache_data(show_spinner="Embedding file...")
def embed_file(file):
    # ì—…ë¡œë“œëœ íŒŒì¼ì„ ë¡œì»¬ì— ì €ì¥í•˜ëŠ” ê³¼ì •
    file_content = file.read()  # íŒŒì¼ ë‚´ìš©ì„ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
    file_path = f"./.cache/files/{file.name}"  # ì €ì¥í•  ê²½ë¡œ ì„¤ì •
    # íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ ì“°ê¸° ëª¨ë“œë¡œ ì—´ì–´ ì €ì¥
    with open(file_path, "wb") as f:
        f.write(file_content)
    # ì„ë² ë”© ìºì‹œë¥¼ ìœ„í•œ ë¡œì»¬ íŒŒì¼ ì €ì¥ì†Œ ì„¤ì •
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    
    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • - tiktoken ì¸ì½”ë” ì‚¬ìš©
    # separator: ì¤„ë°”ê¿ˆ ë¬¸ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    # chunk_size: ê° ì²­í¬ì˜ ìµœëŒ€ í† í° ìˆ˜ (600)
    # chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ í† í° ìˆ˜ (100) - ì»¨í…ìŠ¤íŠ¸ ì—°ì†ì„± ìœ ì§€
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    # UnstructuredFileLoader: PDF, TXT, DOCX ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
    loader = UnstructuredFileLoader(file_path)
    # ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  ì„¤ì •í•œ splitterë¡œ ë¶„í• 
    docs = loader.load_and_split(text_splitter=splitter)
    
    # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings = OpenAIEmbeddings()
    # ìºì‹œ ê¸°ëŠ¥ì´ ì¶”ê°€ëœ ì„ë² ë”© ìƒì„± - ë™ì¼í•œ í…ìŠ¤íŠ¸ëŠ” ì¬ê³„ì‚°í•˜ì§€ ì•ŠìŒ
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    
    # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± - ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„° DB
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    # ê²€ìƒ‰ê¸°(retriever) ìƒì„± - ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ëŠ” ì—­í• 
    retriever = vectorstore.as_retriever()
    return retriever


# ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
# Streamlitì€ ì¬ì‹¤í–‰ ì‹œ ëª¨ë“  ë³€ìˆ˜ê°€ ì´ˆê¸°í™”ë˜ë¯€ë¡œ st.session_state ì‚¬ìš©
def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})


# ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ê³  ì„ íƒì ìœ¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def send_message(message, role, save=True):
    # st.chat_messageë¡œ ì—­í• (human/ai)ì— ë§ëŠ” ì±„íŒ… UI ìƒì„±
    with st.chat_message(role):
        st.markdown(message)  # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ í‘œì‹œ
    # save=Trueì¼ ë•Œë§Œ ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
    if save:
        save_message(message, role)


# ì €ì¥ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™”ë©´ì— ë‹¤ì‹œ ê·¸ë¦¬ëŠ” í•¨ìˆ˜
# Streamlitì´ ì¬ì‹¤í–‰ë  ë•Œë§ˆë‹¤ ì´ì „ ëŒ€í™”ë¥¼ ë³µì›
def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,  # ì´ë¯¸ ì €ì¥ëœ ë©”ì‹œì§€ì´ë¯€ë¡œ ë‹¤ì‹œ ì €ì¥í•˜ì§€ ì•ŠìŒ
        )


# ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
# ê° ë¬¸ì„œ ë‚´ìš©ì„ ë‘ ì¤„ ë„ì›€(\n\n)ìœ¼ë¡œ êµ¬ë¶„
def format_docs(docs):
    st.write(docs)
    st.write(docs[0].page_content)
    return "\n\n".join(document.page_content for document in docs)


# RAG(Retrieval Augmented Generation)ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# system: AIì˜ í–‰ë™ ì§€ì¹¨ ì„¤ì •
# human: ì‚¬ìš©ìì˜ ì§ˆë¬¸
# {context}ì™€ {question}ì€ ëŸ°íƒ€ì„ì— ì‹¤ì œ ê°’ìœ¼ë¡œ ì¹˜í™˜ë¨
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.
            
            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)


# í˜ì´ì§€ ì œëª© ì„¤ì •
st.title("DocumentGPT")

# í™˜ì˜ ë©”ì‹œì§€ì™€ ì‚¬ìš© ì•ˆë‚´
st.markdown(
    """
Welcome!
            
Use this chatbot to ask questions to an AI about your files!

Upload your files on the sidebar.
"""
)

# ì‚¬ì´ë“œë°”ì— íŒŒì¼ ì—…ë¡œë” ìœ„ì ¯ ìƒì„±
# with êµ¬ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ì´ë“œë°” ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ìœ„ì ¯ ìƒì„±
with st.sidebar:
    file = st.file_uploader(
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],  # í—ˆìš©ëœ íŒŒì¼ í˜•ì‹ ì§€ì •
    )

# íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš°ì˜ ë©”ì¸ ë¡œì§
if file:
    # íŒŒì¼ì„ ì„ë² ë”©í•˜ê³  ê²€ìƒ‰ê¸° ìƒì„± (ìºì‹œë¨)
    retriever = embed_file(file)
    # AI ì¤€ë¹„ ì™„ë£Œ ë©”ì‹œì§€ (ì €ì¥í•˜ì§€ ì•ŠìŒ)
    send_message("I'm ready! Ask away!", "ai", save=False)
    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    paint_history()

    # ì±„íŒ… ì…ë ¥ ìœ„ì ¯ ìƒì„±
    message = st.chat_input("Ask anything about your file...")
    
    if message:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ ë° ì €ì¥
        send_message(message, "human")
        
        # LCEL(LangChain Expression Language) ì²´ì¸ êµ¬ì„±
        # 1. retrieverê°€ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        # 2. format_docsê°€ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·
        # 3. RunnablePassthroughê°€ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
        # 4. promptê°€ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        # 5. llmì´ ìµœì¢… ì‘ë‹µ ìƒì„±
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        # AI ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì²´ì¸ ì‹¤í–‰
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì´ ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ í†µí•´ ì‹¤ì‹œê°„ í‘œì‹œë¨
        with st.chat_message("ai"):
            chain.invoke(message)

# íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
else:
    st.session_state["messages"] = []