{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nStuff Documents 체인을 사용하여 완전한 RAG 파이프라인을 구현하세요.\n체인을 수동으로 구현해야 합니다.\n체인에 ConversationBufferMemory를 부여합니다.\n이 문서를 사용하여 RAG를 수행하세요: https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223\n체인에 다음 질문을 합니다:\n    Aaronson 은 유죄인가요?\n    그가 테이블에 어떤 메시지를 썼나요?\n    Julia 는 누구인가요?\n\"\"\"\n\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.document_loaders import UnstructuredFileLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.storage import LocalFileStore\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.memory import ConversationBufferMemory\n\nllm = ChatOpenAI(\n    temperature=0.1,\n)\n\ncache_dir = LocalFileStore(\"./.cache/\")\n\nsplitter = CharacterTextSplitter.from_tiktoken_encoder(\n    separator=\"\\n\",\n    chunk_size=600,\n    chunk_overlap=100,\n)\nloader = UnstructuredFileLoader(\"../files/document.txt\")\n\ndocs = loader.load_and_split(text_splitter=splitter)\n\nembeddings = OpenAIEmbeddings()\n\ncached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n\nvectorstore = FAISS.from_documents(docs, cached_embeddings)\n\nretriver = vectorstore.as_retriever()\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"\n            You are a helpful assistant. Answer questions using only the following context.\n            If you don't know the answer just say you don't know, don't make it up:\n            {context}\n            \"\"\",\n        ),\n        (\"human\", \"{message}\"),\n    ]\n)\n\nmemory = ConversationBufferMemory(return_messages=True)\n\n\ndef load_memory(_):\n    x = memory.load_memory_variables({})\n    return {\"history\": x[\"history\"]}\n\nchain = (\n    {\n        \"context\": retriver,\n        \"message\": RunnablePassthrough(chat_history=load_memory),\n    }\n    | prompt\n    | llm\n)\n\na1 = chain.invoke(\"Aaronson 은 유죄인가요?\")\nprint(a1)\na2 = chain.invoke(\"그가 테이블에 어떤 메시지를 썼나요?\")\nprint(a2)\na3 = chain.invoke(\"Julia 는 누구인가요?\")\nprint(a3)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}